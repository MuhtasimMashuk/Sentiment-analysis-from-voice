{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('C:/voice/Actor_01/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x25f61dd7cc8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 131.85807013511658 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "path = 'C:/voice/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1440, 40), (1440,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = 'C:/another_model/'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('C:/another_model/X.joblib')\n",
    "y = joblib.load('C:/another_model/y.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1152 samples, validate on 288 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1152/1152 [==============================] - 1s 1ms/step - loss: 2.0654 - acc: 0.1641 - val_loss: 2.0008 - val_acc: 0.1910\n",
      "Epoch 2/1000\n",
      "1152/1152 [==============================] - 0s 393us/step - loss: 2.0090 - acc: 0.1840 - val_loss: 1.9593 - val_acc: 0.1944\n",
      "Epoch 3/1000\n",
      "1152/1152 [==============================] - 1s 550us/step - loss: 1.9701 - acc: 0.2283 - val_loss: 1.9341 - val_acc: 0.2049\n",
      "Epoch 4/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.9394 - acc: 0.2300 - val_loss: 1.9317 - val_acc: 0.2396\n",
      "Epoch 5/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 1.9252 - acc: 0.2361 - val_loss: 1.8924 - val_acc: 0.2569\n",
      "Epoch 6/1000\n",
      "1152/1152 [==============================] - 0s 401us/step - loss: 1.8976 - acc: 0.2630 - val_loss: 1.8718 - val_acc: 0.2778\n",
      "Epoch 7/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 1.8826 - acc: 0.2830 - val_loss: 1.8865 - val_acc: 0.2396\n",
      "Epoch 8/1000\n",
      "1152/1152 [==============================] - 0s 411us/step - loss: 1.8617 - acc: 0.2726 - val_loss: 1.8411 - val_acc: 0.3160\n",
      "Epoch 9/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 1.8424 - acc: 0.2995 - val_loss: 1.8448 - val_acc: 0.2708\n",
      "Epoch 10/1000\n",
      "1152/1152 [==============================] - 1s 522us/step - loss: 1.8268 - acc: 0.2908 - val_loss: 1.8447 - val_acc: 0.2674\n",
      "Epoch 11/1000\n",
      "1152/1152 [==============================] - 1s 514us/step - loss: 1.8197 - acc: 0.3047 - val_loss: 1.8407 - val_acc: 0.2847\n",
      "Epoch 12/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.8010 - acc: 0.3090 - val_loss: 1.8112 - val_acc: 0.3125\n",
      "Epoch 13/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 1.7984 - acc: 0.3090 - val_loss: 1.8109 - val_acc: 0.3021\n",
      "Epoch 14/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 1.7912 - acc: 0.3220 - val_loss: 1.8009 - val_acc: 0.3194\n",
      "Epoch 15/1000\n",
      "1152/1152 [==============================] - 0s 303us/step - loss: 1.7737 - acc: 0.3464 - val_loss: 1.7890 - val_acc: 0.3403\n",
      "Epoch 16/1000\n",
      "1152/1152 [==============================] - 1s 460us/step - loss: 1.7658 - acc: 0.3420 - val_loss: 1.7868 - val_acc: 0.3194\n",
      "Epoch 17/1000\n",
      "1152/1152 [==============================] - 1s 469us/step - loss: 1.7730 - acc: 0.3333 - val_loss: 1.7936 - val_acc: 0.3021\n",
      "Epoch 18/1000\n",
      "1152/1152 [==============================] - 0s 415us/step - loss: 1.7534 - acc: 0.3429 - val_loss: 1.7699 - val_acc: 0.3333\n",
      "Epoch 19/1000\n",
      "1152/1152 [==============================] - 1s 489us/step - loss: 1.7496 - acc: 0.3516 - val_loss: 1.7705 - val_acc: 0.3299\n",
      "Epoch 20/1000\n",
      "1152/1152 [==============================] - 1s 465us/step - loss: 1.7350 - acc: 0.3559 - val_loss: 1.7663 - val_acc: 0.3194\n",
      "Epoch 21/1000\n",
      "1152/1152 [==============================] - 1s 450us/step - loss: 1.7324 - acc: 0.3542 - val_loss: 1.7890 - val_acc: 0.2951\n",
      "Epoch 22/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 1.7260 - acc: 0.3455 - val_loss: 1.7610 - val_acc: 0.3507\n",
      "Epoch 23/1000\n",
      "1152/1152 [==============================] - 0s 430us/step - loss: 1.7161 - acc: 0.3628 - val_loss: 1.7633 - val_acc: 0.3229\n",
      "Epoch 24/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.7109 - acc: 0.3490 - val_loss: 1.7508 - val_acc: 0.3542\n",
      "Epoch 25/1000\n",
      "1152/1152 [==============================] - 1s 441us/step - loss: 1.6923 - acc: 0.3767 - val_loss: 1.7417 - val_acc: 0.3507\n",
      "Epoch 26/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 1.6781 - acc: 0.3698 - val_loss: 1.7468 - val_acc: 0.3438\n",
      "Epoch 27/1000\n",
      "1152/1152 [==============================] - 1s 553us/step - loss: 1.6769 - acc: 0.3924 - val_loss: 1.7515 - val_acc: 0.3299\n",
      "Epoch 28/1000\n",
      "1152/1152 [==============================] - 1s 576us/step - loss: 1.6715 - acc: 0.3889 - val_loss: 1.7403 - val_acc: 0.3333\n",
      "Epoch 29/1000\n",
      "1152/1152 [==============================] - 1s 498us/step - loss: 1.6713 - acc: 0.3880 - val_loss: 1.7227 - val_acc: 0.3611\n",
      "Epoch 30/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.6574 - acc: 0.391 - 1s 521us/step - loss: 1.6605 - acc: 0.3872 - val_loss: 1.7088 - val_acc: 0.3785\n",
      "Epoch 31/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 1.6524 - acc: 0.3915 - val_loss: 1.7134 - val_acc: 0.3646\n",
      "Epoch 32/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 1.6514 - acc: 0.3976 - val_loss: 1.7093 - val_acc: 0.3819\n",
      "Epoch 33/1000\n",
      "1152/1152 [==============================] - 1s 486us/step - loss: 1.6353 - acc: 0.4045 - val_loss: 1.7048 - val_acc: 0.3611\n",
      "Epoch 34/1000\n",
      "1152/1152 [==============================] - 0s 405us/step - loss: 1.6382 - acc: 0.4054 - val_loss: 1.7430 - val_acc: 0.3472\n",
      "Epoch 35/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 1.6255 - acc: 0.4080 - val_loss: 1.7167 - val_acc: 0.3542\n",
      "Epoch 36/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 1.6182 - acc: 0.4062 - val_loss: 1.7111 - val_acc: 0.3507\n",
      "Epoch 37/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 1.6236 - acc: 0.4167 - val_loss: 1.6963 - val_acc: 0.3750\n",
      "Epoch 38/1000\n",
      "1152/1152 [==============================] - 0s 420us/step - loss: 1.6186 - acc: 0.4071 - val_loss: 1.7023 - val_acc: 0.3611\n",
      "Epoch 39/1000\n",
      "1152/1152 [==============================] - 0s 405us/step - loss: 1.6146 - acc: 0.4054 - val_loss: 1.6994 - val_acc: 0.3715\n",
      "Epoch 40/1000\n",
      "1152/1152 [==============================] - 1s 489us/step - loss: 1.6095 - acc: 0.4141 - val_loss: 1.6867 - val_acc: 0.3889\n",
      "Epoch 41/1000\n",
      "1152/1152 [==============================] - 1s 518us/step - loss: 1.5915 - acc: 0.4236 - val_loss: 1.6935 - val_acc: 0.3646\n",
      "Epoch 42/1000\n",
      "1152/1152 [==============================] - 1s 452us/step - loss: 1.5862 - acc: 0.4314 - val_loss: 1.6972 - val_acc: 0.3715\n",
      "Epoch 43/1000\n",
      "1152/1152 [==============================] - 0s 432us/step - loss: 1.5759 - acc: 0.4410 - val_loss: 1.6962 - val_acc: 0.3646\n",
      "Epoch 44/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 1.5769 - acc: 0.4392 - val_loss: 1.6821 - val_acc: 0.3785\n",
      "Epoch 45/1000\n",
      "1152/1152 [==============================] - 1s 536us/step - loss: 1.5848 - acc: 0.4470 - val_loss: 1.6761 - val_acc: 0.3750\n",
      "Epoch 46/1000\n",
      "1152/1152 [==============================] - 1s 499us/step - loss: 1.5734 - acc: 0.4427 - val_loss: 1.6888 - val_acc: 0.3576\n",
      "Epoch 47/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.5709 - acc: 0.426 - 1s 489us/step - loss: 1.5727 - acc: 0.4297 - val_loss: 1.6808 - val_acc: 0.3715\n",
      "Epoch 48/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 1.5606 - acc: 0.4609 - val_loss: 1.6934 - val_acc: 0.3646\n",
      "Epoch 49/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 1.5464 - acc: 0.4540 - val_loss: 1.6601 - val_acc: 0.3750\n",
      "Epoch 50/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 1.5565 - acc: 0.4366 - val_loss: 1.6706 - val_acc: 0.3889\n",
      "Epoch 51/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 1.5351 - acc: 0.4401 - val_loss: 1.6534 - val_acc: 0.3889\n",
      "Epoch 52/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 1.5464 - acc: 0.4505 - val_loss: 1.6647 - val_acc: 0.3993\n",
      "Epoch 53/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 1.5416 - acc: 0.4462 - val_loss: 1.6545 - val_acc: 0.3889\n",
      "Epoch 54/1000\n",
      "1152/1152 [==============================] - 1s 464us/step - loss: 1.5344 - acc: 0.4410 - val_loss: 1.6491 - val_acc: 0.3854\n",
      "Epoch 55/1000\n",
      "1152/1152 [==============================] - 1s 531us/step - loss: 1.5360 - acc: 0.4470 - val_loss: 1.6502 - val_acc: 0.3993\n",
      "Epoch 56/1000\n",
      "1152/1152 [==============================] - 1s 503us/step - loss: 1.5095 - acc: 0.4566 - val_loss: 1.6384 - val_acc: 0.3993\n",
      "Epoch 57/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 1.5220 - acc: 0.4653 - val_loss: 1.6519 - val_acc: 0.4028\n",
      "Epoch 58/1000\n",
      "1152/1152 [==============================] - 1s 471us/step - loss: 1.5016 - acc: 0.4609 - val_loss: 1.6367 - val_acc: 0.3889\n",
      "Epoch 59/1000\n",
      "1152/1152 [==============================] - 1s 576us/step - loss: 1.5156 - acc: 0.4479 - val_loss: 1.6535 - val_acc: 0.4062\n",
      "Epoch 60/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.5062 - acc: 0.458 - 1s 454us/step - loss: 1.5079 - acc: 0.4557 - val_loss: 1.6441 - val_acc: 0.3889\n",
      "Epoch 61/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 1.4986 - acc: 0.4783 - val_loss: 1.6417 - val_acc: 0.4097\n",
      "Epoch 62/1000\n",
      "1152/1152 [==============================] - 0s 408us/step - loss: 1.5018 - acc: 0.4601 - val_loss: 1.6461 - val_acc: 0.3819\n",
      "Epoch 63/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 1.4897 - acc: 0.4653 - val_loss: 1.6218 - val_acc: 0.4201\n",
      "Epoch 64/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 1.4868 - acc: 0.4748 - val_loss: 1.6690 - val_acc: 0.3646\n",
      "Epoch 65/1000\n",
      "1152/1152 [==============================] - 0s 418us/step - loss: 1.4835 - acc: 0.4705 - val_loss: 1.6210 - val_acc: 0.4132\n",
      "Epoch 66/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 1.4824 - acc: 0.4688 - val_loss: 1.6252 - val_acc: 0.3958\n",
      "Epoch 67/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 1.4865 - acc: 0.4696 - val_loss: 1.6172 - val_acc: 0.4201\n",
      "Epoch 68/1000\n",
      "1152/1152 [==============================] - 1s 487us/step - loss: 1.4655 - acc: 0.4844 - val_loss: 1.6384 - val_acc: 0.3785\n",
      "Epoch 69/1000\n",
      "1152/1152 [==============================] - 1s 441us/step - loss: 1.4603 - acc: 0.4748 - val_loss: 1.6412 - val_acc: 0.3958\n",
      "Epoch 70/1000\n",
      "1152/1152 [==============================] - 1s 440us/step - loss: 1.4572 - acc: 0.4965 - val_loss: 1.6119 - val_acc: 0.3854\n",
      "Epoch 71/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 1.4662 - acc: 0.4774 - val_loss: 1.6103 - val_acc: 0.4167\n",
      "Epoch 72/1000\n",
      "1152/1152 [==============================] - 1s 515us/step - loss: 1.4468 - acc: 0.4939 - val_loss: 1.6313 - val_acc: 0.3889\n",
      "Epoch 73/1000\n",
      "1152/1152 [==============================] - 0s 394us/step - loss: 1.4482 - acc: 0.4861 - val_loss: 1.6110 - val_acc: 0.3889\n",
      "Epoch 74/1000\n",
      "1152/1152 [==============================] - 0s 416us/step - loss: 1.4354 - acc: 0.4826 - val_loss: 1.6274 - val_acc: 0.3889\n",
      "Epoch 75/1000\n",
      "1152/1152 [==============================] - 0s 391us/step - loss: 1.4399 - acc: 0.4835 - val_loss: 1.6286 - val_acc: 0.3993\n",
      "Epoch 76/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 1.4296 - acc: 0.4957 - val_loss: 1.5893 - val_acc: 0.3889\n",
      "Epoch 77/1000\n",
      "1152/1152 [==============================] - 1s 510us/step - loss: 1.4308 - acc: 0.4939 - val_loss: 1.6051 - val_acc: 0.4097\n",
      "Epoch 78/1000\n",
      "1152/1152 [==============================] - 0s 409us/step - loss: 1.4385 - acc: 0.4948 - val_loss: 1.6203 - val_acc: 0.3854\n",
      "Epoch 79/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 1.4209 - acc: 0.4939 - val_loss: 1.6045 - val_acc: 0.4271\n",
      "Epoch 80/1000\n",
      "1152/1152 [==============================] - 0s 407us/step - loss: 1.4211 - acc: 0.4922 - val_loss: 1.5877 - val_acc: 0.4236\n",
      "Epoch 81/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 1.4078 - acc: 0.4974 - val_loss: 1.6306 - val_acc: 0.3889\n",
      "Epoch 82/1000\n",
      "1152/1152 [==============================] - 0s 399us/step - loss: 1.4217 - acc: 0.4922 - val_loss: 1.6189 - val_acc: 0.4062\n",
      "Epoch 83/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 1.4086 - acc: 0.5000 - val_loss: 1.5629 - val_acc: 0.4375\n",
      "Epoch 84/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 1.4029 - acc: 0.5078 - val_loss: 1.5822 - val_acc: 0.4167\n",
      "Epoch 85/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 1.4010 - acc: 0.5191 - val_loss: 1.5889 - val_acc: 0.4167\n",
      "Epoch 86/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.511 - 1s 446us/step - loss: 1.3944 - acc: 0.5130 - val_loss: 1.6269 - val_acc: 0.4028\n",
      "Epoch 87/1000\n",
      "1152/1152 [==============================] - 1s 436us/step - loss: 1.3876 - acc: 0.5139 - val_loss: 1.6159 - val_acc: 0.3993\n",
      "Epoch 88/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 1.3837 - acc: 0.5130 - val_loss: 1.5829 - val_acc: 0.4201\n",
      "Epoch 89/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 1.3871 - acc: 0.5104 - val_loss: 1.5727 - val_acc: 0.4097\n",
      "Epoch 90/1000\n",
      "1152/1152 [==============================] - 1s 506us/step - loss: 1.3784 - acc: 0.5234 - val_loss: 1.6044 - val_acc: 0.3993\n",
      "Epoch 91/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 1.3666 - acc: 0.5148 - val_loss: 1.6008 - val_acc: 0.4132\n",
      "Epoch 92/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 1.3741 - acc: 0.5286 - val_loss: 1.5702 - val_acc: 0.4236\n",
      "Epoch 93/1000\n",
      "1152/1152 [==============================] - 0s 383us/step - loss: 1.3731 - acc: 0.5182 - val_loss: 1.5746 - val_acc: 0.4236\n",
      "Epoch 94/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 1.3656 - acc: 0.5252 - val_loss: 1.5968 - val_acc: 0.4201\n",
      "Epoch 95/1000\n",
      "1152/1152 [==============================] - 1s 539us/step - loss: 1.3623 - acc: 0.5321 - val_loss: 1.5644 - val_acc: 0.4201\n",
      "Epoch 96/1000\n",
      "1152/1152 [==============================] - 0s 433us/step - loss: 1.3493 - acc: 0.5278 - val_loss: 1.5723 - val_acc: 0.4167\n",
      "Epoch 97/1000\n",
      "1152/1152 [==============================] - 0s 417us/step - loss: 1.3566 - acc: 0.5312 - val_loss: 1.5588 - val_acc: 0.4167\n",
      "Epoch 98/1000\n",
      "1152/1152 [==============================] - 1s 524us/step - loss: 1.3410 - acc: 0.5243 - val_loss: 1.5698 - val_acc: 0.4306\n",
      "Epoch 99/1000\n",
      "1152/1152 [==============================] - 0s 393us/step - loss: 1.3456 - acc: 0.5391 - val_loss: 1.5574 - val_acc: 0.4236\n",
      "Epoch 100/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 1.3558 - acc: 0.5226 - val_loss: 1.5628 - val_acc: 0.4201\n",
      "Epoch 101/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 1.3499 - acc: 0.5174 - val_loss: 1.5731 - val_acc: 0.4132\n",
      "Epoch 102/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 1.3296 - acc: 0.5312 - val_loss: 1.5820 - val_acc: 0.4062\n",
      "Epoch 103/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 1.3401 - acc: 0.5321 - val_loss: 1.5948 - val_acc: 0.4097\n",
      "Epoch 104/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 1.3306 - acc: 0.5373 - val_loss: 1.5661 - val_acc: 0.4167\n",
      "Epoch 105/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 1.3263 - acc: 0.5382 - val_loss: 1.5980 - val_acc: 0.4132\n",
      "Epoch 106/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 1.3301 - acc: 0.5486 - val_loss: 1.5921 - val_acc: 0.4167\n",
      "Epoch 107/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 1.3185 - acc: 0.5347 - val_loss: 1.5618 - val_acc: 0.4167\n",
      "Epoch 108/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 1.3035 - acc: 0.5547 - val_loss: 1.5369 - val_acc: 0.4062\n",
      "Epoch 109/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 1.3032 - acc: 0.5425 - val_loss: 1.5530 - val_acc: 0.4271\n",
      "Epoch 110/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 1.3170 - acc: 0.5321 - val_loss: 1.5737 - val_acc: 0.4271\n",
      "Epoch 111/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 1.3076 - acc: 0.5399 - val_loss: 1.5664 - val_acc: 0.4340\n",
      "Epoch 112/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 1.3145 - acc: 0.5408 - val_loss: 1.5342 - val_acc: 0.4236\n",
      "Epoch 113/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 1.2924 - acc: 0.5451 - val_loss: 1.5349 - val_acc: 0.4097\n",
      "Epoch 114/1000\n",
      "1152/1152 [==============================] - 0s 409us/step - loss: 1.2855 - acc: 0.5642 - val_loss: 1.5512 - val_acc: 0.4201\n",
      "Epoch 115/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 1.2874 - acc: 0.5547 - val_loss: 1.5611 - val_acc: 0.4097\n",
      "Epoch 116/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 1.2870 - acc: 0.5608 - val_loss: 1.5539 - val_acc: 0.4271\n",
      "Epoch 117/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 1.2955 - acc: 0.5469 - val_loss: 1.5321 - val_acc: 0.4271\n",
      "Epoch 118/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 1.2807 - acc: 0.5616 - val_loss: 1.5370 - val_acc: 0.4132\n",
      "Epoch 119/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 1.2959 - acc: 0.5486 - val_loss: 1.5510 - val_acc: 0.4062\n",
      "Epoch 120/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 1.2807 - acc: 0.5564 - val_loss: 1.5383 - val_acc: 0.4375\n",
      "Epoch 121/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 1.2708 - acc: 0.5634 - val_loss: 1.5409 - val_acc: 0.4201\n",
      "Epoch 122/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 1.2673 - acc: 0.5677 - val_loss: 1.5353 - val_acc: 0.4236\n",
      "Epoch 123/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.2596 - acc: 0.5634 - val_loss: 1.5264 - val_acc: 0.4097\n",
      "Epoch 124/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.2665 - acc: 0.5686 - val_loss: 1.5224 - val_acc: 0.4306\n",
      "Epoch 125/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 1.2694 - acc: 0.5512 - val_loss: 1.5241 - val_acc: 0.4306\n",
      "Epoch 126/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 1.2660 - acc: 0.5590 - val_loss: 1.5294 - val_acc: 0.4271\n",
      "Epoch 127/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 1.2388 - acc: 0.5738 - val_loss: 1.5722 - val_acc: 0.4132\n",
      "Epoch 128/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 1.2546 - acc: 0.5530 - val_loss: 1.5376 - val_acc: 0.4201\n",
      "Epoch 129/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 1.2588 - acc: 0.5668 - val_loss: 1.5510 - val_acc: 0.4306\n",
      "Epoch 130/1000\n",
      "1152/1152 [==============================] - 1s 464us/step - loss: 1.2409 - acc: 0.5764 - val_loss: 1.5230 - val_acc: 0.4271\n",
      "Epoch 131/1000\n",
      "1152/1152 [==============================] - 0s 411us/step - loss: 1.2505 - acc: 0.5781 - val_loss: 1.5274 - val_acc: 0.4340\n",
      "Epoch 132/1000\n",
      "1152/1152 [==============================] - 1s 444us/step - loss: 1.2235 - acc: 0.5729 - val_loss: 1.5074 - val_acc: 0.4306\n",
      "Epoch 133/1000\n",
      "1152/1152 [==============================] - 1s 480us/step - loss: 1.2431 - acc: 0.5703 - val_loss: 1.5568 - val_acc: 0.4201\n",
      "Epoch 134/1000\n",
      "1152/1152 [==============================] - 1s 566us/step - loss: 1.2228 - acc: 0.5781 - val_loss: 1.5415 - val_acc: 0.4201\n",
      "Epoch 135/1000\n",
      "1152/1152 [==============================] - 1s 638us/step - loss: 1.2338 - acc: 0.5816 - val_loss: 1.5457 - val_acc: 0.4201\n",
      "Epoch 136/1000\n",
      "1152/1152 [==============================] - 1s 502us/step - loss: 1.2338 - acc: 0.5807 - val_loss: 1.5429 - val_acc: 0.4167\n",
      "Epoch 137/1000\n",
      "1152/1152 [==============================] - 1s 560us/step - loss: 1.2222 - acc: 0.5842 - val_loss: 1.5383 - val_acc: 0.4306\n",
      "Epoch 138/1000\n",
      "1152/1152 [==============================] - 1s 444us/step - loss: 1.2175 - acc: 0.5799 - val_loss: 1.5037 - val_acc: 0.4479\n",
      "Epoch 139/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.2234 - acc: 0.5712 - val_loss: 1.5122 - val_acc: 0.4271\n",
      "Epoch 140/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.2382 - acc: 0.5590 - val_loss: 1.5263 - val_acc: 0.4306\n",
      "Epoch 141/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 1.2147 - acc: 0.5825 - val_loss: 1.5345 - val_acc: 0.4236\n",
      "Epoch 142/1000\n",
      "1152/1152 [==============================] - 0s 395us/step - loss: 1.2097 - acc: 0.5807 - val_loss: 1.4977 - val_acc: 0.4479\n",
      "Epoch 143/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 1.2092 - acc: 0.5799 - val_loss: 1.5441 - val_acc: 0.4236\n",
      "Epoch 144/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.2031 - acc: 0.5842 - val_loss: 1.5992 - val_acc: 0.4097\n",
      "Epoch 145/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 1.2140 - acc: 0.5773 - val_loss: 1.5053 - val_acc: 0.4444\n",
      "Epoch 146/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 1.1994 - acc: 0.5859 - val_loss: 1.5233 - val_acc: 0.4236\n",
      "Epoch 147/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 1.2051 - acc: 0.5790 - val_loss: 1.5026 - val_acc: 0.4375\n",
      "Epoch 148/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 1.1974 - acc: 0.5833 - val_loss: 1.5463 - val_acc: 0.4201\n",
      "Epoch 149/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.1968 - acc: 0.5799 - val_loss: 1.5012 - val_acc: 0.4479\n",
      "Epoch 150/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 1.1980 - acc: 0.5894 - val_loss: 1.5085 - val_acc: 0.4410\n",
      "Epoch 151/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 1.2010 - acc: 0.5859 - val_loss: 1.5441 - val_acc: 0.4201\n",
      "Epoch 152/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 1.1906 - acc: 0.5894 - val_loss: 1.4798 - val_acc: 0.4514\n",
      "Epoch 153/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.1847 - acc: 0.5929 - val_loss: 1.5084 - val_acc: 0.4340\n",
      "Epoch 154/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.1950 - acc: 0.5929 - val_loss: 1.4928 - val_acc: 0.4306\n",
      "Epoch 155/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.1936 - acc: 0.5894 - val_loss: 1.5105 - val_acc: 0.4444\n",
      "Epoch 156/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 1.1750 - acc: 0.5799 - val_loss: 1.5247 - val_acc: 0.4340\n",
      "Epoch 157/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 1.1753 - acc: 0.5842 - val_loss: 1.4724 - val_acc: 0.4653\n",
      "Epoch 158/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 1.1868 - acc: 0.5842 - val_loss: 1.4852 - val_acc: 0.4444\n",
      "Epoch 159/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.1730 - acc: 0.5911 - val_loss: 1.5040 - val_acc: 0.4410\n",
      "Epoch 160/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.1705 - acc: 0.5981 - val_loss: 1.4861 - val_acc: 0.4514\n",
      "Epoch 161/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.1765 - acc: 0.5964 - val_loss: 1.4959 - val_acc: 0.4410\n",
      "Epoch 162/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 1.1699 - acc: 0.5885 - val_loss: 1.5011 - val_acc: 0.4444\n",
      "Epoch 163/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.1555 - acc: 0.5990 - val_loss: 1.4816 - val_acc: 0.4583\n",
      "Epoch 164/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 1.1668 - acc: 0.5911 - val_loss: 1.4805 - val_acc: 0.4444\n",
      "Epoch 165/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.1570 - acc: 0.6128 - val_loss: 1.4967 - val_acc: 0.4444\n",
      "Epoch 166/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 1.1592 - acc: 0.5946 - val_loss: 1.5055 - val_acc: 0.4340\n",
      "Epoch 167/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 1.1576 - acc: 0.6120 - val_loss: 1.4830 - val_acc: 0.4410\n",
      "Epoch 168/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.1531 - acc: 0.5964 - val_loss: 1.5184 - val_acc: 0.4410\n",
      "Epoch 169/1000\n",
      "1152/1152 [==============================] - 0s 299us/step - loss: 1.1464 - acc: 0.5998 - val_loss: 1.4864 - val_acc: 0.4549\n",
      "Epoch 170/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 1.1424 - acc: 0.5990 - val_loss: 1.4999 - val_acc: 0.4479\n",
      "Epoch 171/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 1.1436 - acc: 0.6076 - val_loss: 1.4892 - val_acc: 0.4410\n",
      "Epoch 172/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.1529 - acc: 0.5851 - val_loss: 1.4833 - val_acc: 0.4375\n",
      "Epoch 173/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.1415 - acc: 0.6111 - val_loss: 1.4697 - val_acc: 0.4583\n",
      "Epoch 174/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 1.1384 - acc: 0.6155 - val_loss: 1.4931 - val_acc: 0.4444\n",
      "Epoch 175/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 1.1311 - acc: 0.6198 - val_loss: 1.4788 - val_acc: 0.4479\n",
      "Epoch 176/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 1.1396 - acc: 0.6024 - val_loss: 1.5196 - val_acc: 0.4271\n",
      "Epoch 177/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 1.1333 - acc: 0.6215 - val_loss: 1.4972 - val_acc: 0.4549\n",
      "Epoch 178/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 1.1262 - acc: 0.6207 - val_loss: 1.4982 - val_acc: 0.4375\n",
      "Epoch 179/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.1122 - acc: 0.6137 - val_loss: 1.4953 - val_acc: 0.4375\n",
      "Epoch 180/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.1142 - acc: 0.6155 - val_loss: 1.4600 - val_acc: 0.4653\n",
      "Epoch 181/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 1.1280 - acc: 0.6120 - val_loss: 1.4666 - val_acc: 0.4583\n",
      "Epoch 182/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 1.1149 - acc: 0.6198 - val_loss: 1.4945 - val_acc: 0.4236\n",
      "Epoch 183/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.1121 - acc: 0.6111 - val_loss: 1.4747 - val_acc: 0.4653\n",
      "Epoch 184/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.0947 - acc: 0.6276 - val_loss: 1.4611 - val_acc: 0.4410\n",
      "Epoch 185/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.1109 - acc: 0.6189 - val_loss: 1.4760 - val_acc: 0.4410\n",
      "Epoch 186/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 1.1204 - acc: 0.5972 - val_loss: 1.4850 - val_acc: 0.4514\n",
      "Epoch 187/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.1066 - acc: 0.6094 - val_loss: 1.5074 - val_acc: 0.4375\n",
      "Epoch 188/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 1.1131 - acc: 0.6207 - val_loss: 1.5072 - val_acc: 0.4132\n",
      "Epoch 189/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.1051 - acc: 0.6163 - val_loss: 1.4840 - val_acc: 0.4479\n",
      "Epoch 190/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 1.0923 - acc: 0.6293 - val_loss: 1.4704 - val_acc: 0.4583\n",
      "Epoch 191/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.1000 - acc: 0.6102 - val_loss: 1.4572 - val_acc: 0.4583\n",
      "Epoch 192/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.1014 - acc: 0.6250 - val_loss: 1.5088 - val_acc: 0.4306\n",
      "Epoch 193/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 1.1017 - acc: 0.6215 - val_loss: 1.4779 - val_acc: 0.4410\n",
      "Epoch 194/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.1098 - acc: 0.6215 - val_loss: 1.4568 - val_acc: 0.4757\n",
      "Epoch 195/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.0999 - acc: 0.6233 - val_loss: 1.4609 - val_acc: 0.4479\n",
      "Epoch 196/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.0931 - acc: 0.6207 - val_loss: 1.4566 - val_acc: 0.4583\n",
      "Epoch 197/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 1.0953 - acc: 0.6146 - val_loss: 1.4799 - val_acc: 0.4479\n",
      "Epoch 198/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.0882 - acc: 0.6259 - val_loss: 1.5117 - val_acc: 0.4340\n",
      "Epoch 199/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 1.0895 - acc: 0.6276 - val_loss: 1.4514 - val_acc: 0.4514\n",
      "Epoch 200/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.0822 - acc: 0.6406 - val_loss: 1.4939 - val_acc: 0.4306\n",
      "Epoch 201/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.0865 - acc: 0.6137 - val_loss: 1.4821 - val_acc: 0.4479\n",
      "Epoch 202/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 1.0749 - acc: 0.6380 - val_loss: 1.4778 - val_acc: 0.4375\n",
      "Epoch 203/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0754 - acc: 0.6267 - val_loss: 1.4884 - val_acc: 0.4514\n",
      "Epoch 204/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 1.0713 - acc: 0.6398 - val_loss: 1.4309 - val_acc: 0.4722\n",
      "Epoch 205/1000\n",
      "1152/1152 [==============================] - 0s 302us/step - loss: 1.0786 - acc: 0.6302 - val_loss: 1.4598 - val_acc: 0.4583\n",
      "Epoch 206/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.0827 - acc: 0.6337 - val_loss: 1.4678 - val_acc: 0.4618\n",
      "Epoch 207/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 1.0652 - acc: 0.6285 - val_loss: 1.4591 - val_acc: 0.4722\n",
      "Epoch 208/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.0379 - acc: 0.6484 - val_loss: 1.4513 - val_acc: 0.4583\n",
      "Epoch 209/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.0606 - acc: 0.6354 - val_loss: 1.4414 - val_acc: 0.4479\n",
      "Epoch 210/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 1.0760 - acc: 0.6354 - val_loss: 1.4992 - val_acc: 0.4375\n",
      "Epoch 211/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.0609 - acc: 0.6337 - val_loss: 1.4713 - val_acc: 0.4549\n",
      "Epoch 212/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 1.0573 - acc: 0.6207 - val_loss: 1.4785 - val_acc: 0.4340\n",
      "Epoch 213/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 1.0448 - acc: 0.6476 - val_loss: 1.4383 - val_acc: 0.4722\n",
      "Epoch 214/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.0529 - acc: 0.6493 - val_loss: 1.4712 - val_acc: 0.4549\n",
      "Epoch 215/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0450 - acc: 0.6372 - val_loss: 1.4513 - val_acc: 0.4618\n",
      "Epoch 216/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 1.0564 - acc: 0.6337 - val_loss: 1.4784 - val_acc: 0.4444\n",
      "Epoch 217/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.0526 - acc: 0.6372 - val_loss: 1.4370 - val_acc: 0.4653\n",
      "Epoch 218/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.0667 - acc: 0.6215 - val_loss: 1.4493 - val_acc: 0.4583\n",
      "Epoch 219/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.0372 - acc: 0.6554 - val_loss: 1.4805 - val_acc: 0.4410\n",
      "Epoch 220/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 1.0489 - acc: 0.6424 - val_loss: 1.4337 - val_acc: 0.4757\n",
      "Epoch 221/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.0373 - acc: 0.6406 - val_loss: 1.4508 - val_acc: 0.4583\n",
      "Epoch 222/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.0490 - acc: 0.6458 - val_loss: 1.4582 - val_acc: 0.4653\n",
      "Epoch 223/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.0387 - acc: 0.6536 - val_loss: 1.4395 - val_acc: 0.4688\n",
      "Epoch 224/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0537 - acc: 0.6432 - val_loss: 1.4446 - val_acc: 0.4688\n",
      "Epoch 225/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.0348 - acc: 0.6528 - val_loss: 1.4581 - val_acc: 0.4271\n",
      "Epoch 226/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.0537 - acc: 0.6267 - val_loss: 1.4540 - val_acc: 0.4444\n",
      "Epoch 227/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 1.0315 - acc: 0.6545 - val_loss: 1.4775 - val_acc: 0.4410\n",
      "Epoch 228/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.0237 - acc: 0.6345 - val_loss: 1.4359 - val_acc: 0.4549\n",
      "Epoch 229/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 1.0413 - acc: 0.6267 - val_loss: 1.4548 - val_acc: 0.4653\n",
      "Epoch 230/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.0331 - acc: 0.6510 - val_loss: 1.4527 - val_acc: 0.4549\n",
      "Epoch 231/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.0047 - acc: 0.6545 - val_loss: 1.4449 - val_acc: 0.4549\n",
      "Epoch 232/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.0287 - acc: 0.6311 - val_loss: 1.4268 - val_acc: 0.4722\n",
      "Epoch 233/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.0084 - acc: 0.6510 - val_loss: 1.4595 - val_acc: 0.4653\n",
      "Epoch 234/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 1.0034 - acc: 0.6623 - val_loss: 1.4491 - val_acc: 0.4444\n",
      "Epoch 235/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 1.0100 - acc: 0.6450 - val_loss: 1.4687 - val_acc: 0.4375\n",
      "Epoch 236/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.0163 - acc: 0.6493 - val_loss: 1.4180 - val_acc: 0.4757\n",
      "Epoch 237/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.0203 - acc: 0.6372 - val_loss: 1.4662 - val_acc: 0.4514\n",
      "Epoch 238/1000\n",
      "1152/1152 [==============================] - 0s 304us/step - loss: 1.0014 - acc: 0.6510 - val_loss: 1.4537 - val_acc: 0.4514\n",
      "Epoch 239/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.0132 - acc: 0.6641 - val_loss: 1.4908 - val_acc: 0.4375\n",
      "Epoch 240/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.0023 - acc: 0.6589 - val_loss: 1.4624 - val_acc: 0.4479\n",
      "Epoch 241/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.0045 - acc: 0.6580 - val_loss: 1.4550 - val_acc: 0.4514\n",
      "Epoch 242/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 0.9917 - acc: 0.6580 - val_loss: 1.4176 - val_acc: 0.4826\n",
      "Epoch 243/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.0044 - acc: 0.6519 - val_loss: 1.4434 - val_acc: 0.4549\n",
      "Epoch 244/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.0010 - acc: 0.6667 - val_loss: 1.4534 - val_acc: 0.4583\n",
      "Epoch 245/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.0024 - acc: 0.6615 - val_loss: 1.4314 - val_acc: 0.4722\n",
      "Epoch 246/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.9960 - acc: 0.6562 - val_loss: 1.4381 - val_acc: 0.4618\n",
      "Epoch 247/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.9943 - acc: 0.6615 - val_loss: 1.4580 - val_acc: 0.4583\n",
      "Epoch 248/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.9997 - acc: 0.6571 - val_loss: 1.4297 - val_acc: 0.4618\n",
      "Epoch 249/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.9724 - acc: 0.6762 - val_loss: 1.4436 - val_acc: 0.4410\n",
      "Epoch 250/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.9981 - acc: 0.6727 - val_loss: 1.4396 - val_acc: 0.4479\n",
      "Epoch 251/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.9915 - acc: 0.6684 - val_loss: 1.4478 - val_acc: 0.4410\n",
      "Epoch 252/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.9901 - acc: 0.6484 - val_loss: 1.4506 - val_acc: 0.4688\n",
      "Epoch 253/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 0.9781 - acc: 0.6667 - val_loss: 1.4500 - val_acc: 0.4653\n",
      "Epoch 254/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 0.9741 - acc: 0.6675 - val_loss: 1.4416 - val_acc: 0.4444\n",
      "Epoch 255/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.9827 - acc: 0.6632 - val_loss: 1.4181 - val_acc: 0.4722\n",
      "Epoch 256/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.9945 - acc: 0.6606 - val_loss: 1.4780 - val_acc: 0.4618\n",
      "Epoch 257/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.9841 - acc: 0.6606 - val_loss: 1.4339 - val_acc: 0.4618\n",
      "Epoch 258/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.9845 - acc: 0.6363 - val_loss: 1.4364 - val_acc: 0.4618\n",
      "Epoch 259/1000\n",
      "1152/1152 [==============================] - 0s 303us/step - loss: 0.9713 - acc: 0.6589 - val_loss: 1.4236 - val_acc: 0.4583\n",
      "Epoch 260/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.9914 - acc: 0.6415 - val_loss: 1.4145 - val_acc: 0.4618\n",
      "Epoch 261/1000\n",
      "1152/1152 [==============================] - 0s 307us/step - loss: 0.9840 - acc: 0.6545 - val_loss: 1.4208 - val_acc: 0.4688\n",
      "Epoch 262/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.9779 - acc: 0.6632 - val_loss: 1.4154 - val_acc: 0.4896\n",
      "Epoch 263/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.9723 - acc: 0.6797 - val_loss: 1.4357 - val_acc: 0.4583\n",
      "Epoch 264/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.9692 - acc: 0.6615 - val_loss: 1.4544 - val_acc: 0.4792\n",
      "Epoch 265/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.9742 - acc: 0.6797 - val_loss: 1.4798 - val_acc: 0.4514\n",
      "Epoch 266/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.9696 - acc: 0.6701 - val_loss: 1.4525 - val_acc: 0.4618\n",
      "Epoch 267/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.9716 - acc: 0.6536 - val_loss: 1.4583 - val_acc: 0.4514\n",
      "Epoch 268/1000\n",
      "1152/1152 [==============================] - 1s 473us/step - loss: 0.9738 - acc: 0.6554 - val_loss: 1.4335 - val_acc: 0.4618\n",
      "Epoch 269/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.9613 - acc: 0.6727 - val_loss: 1.4416 - val_acc: 0.4826\n",
      "Epoch 270/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.9618 - acc: 0.6667 - val_loss: 1.4571 - val_acc: 0.4410\n",
      "Epoch 271/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 0.9760 - acc: 0.6606 - val_loss: 1.4445 - val_acc: 0.4583\n",
      "Epoch 272/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 0.9639 - acc: 0.6727 - val_loss: 1.4026 - val_acc: 0.4757\n",
      "Epoch 273/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 0.9759 - acc: 0.6562 - val_loss: 1.4208 - val_acc: 0.4653\n",
      "Epoch 274/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.9637 - acc: 0.6780 - val_loss: 1.4630 - val_acc: 0.4514\n",
      "Epoch 275/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 0.9651 - acc: 0.6693 - val_loss: 1.4087 - val_acc: 0.4792\n",
      "Epoch 276/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 0.9553 - acc: 0.6901 - val_loss: 1.4343 - val_acc: 0.4583\n",
      "Epoch 277/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.9696 - acc: 0.6615 - val_loss: 1.4294 - val_acc: 0.4514\n",
      "Epoch 278/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.9484 - acc: 0.6667 - val_loss: 1.4552 - val_acc: 0.4583\n",
      "Epoch 279/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.9444 - acc: 0.6814 - val_loss: 1.4428 - val_acc: 0.4618\n",
      "Epoch 280/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.9398 - acc: 0.6684 - val_loss: 1.4368 - val_acc: 0.4618\n",
      "Epoch 281/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.9319 - acc: 0.6753 - val_loss: 1.4412 - val_acc: 0.4375\n",
      "Epoch 282/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.9323 - acc: 0.6806 - val_loss: 1.4330 - val_acc: 0.4514\n",
      "Epoch 283/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.9444 - acc: 0.6753 - val_loss: 1.4125 - val_acc: 0.4514\n",
      "Epoch 284/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.9379 - acc: 0.6797 - val_loss: 1.4433 - val_acc: 0.4514\n",
      "Epoch 285/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 0.9295 - acc: 0.6762 - val_loss: 1.4334 - val_acc: 0.4653\n",
      "Epoch 286/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 0.9323 - acc: 0.6927 - val_loss: 1.4463 - val_acc: 0.4444\n",
      "Epoch 287/1000\n",
      "1152/1152 [==============================] - 0s 307us/step - loss: 0.9467 - acc: 0.6745 - val_loss: 1.4363 - val_acc: 0.4757\n",
      "Epoch 288/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.9317 - acc: 0.6910 - val_loss: 1.4060 - val_acc: 0.4792\n",
      "Epoch 289/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.9499 - acc: 0.6615 - val_loss: 1.4511 - val_acc: 0.4653\n",
      "Epoch 290/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.9352 - acc: 0.6797 - val_loss: 1.4248 - val_acc: 0.4549\n",
      "Epoch 291/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.9137 - acc: 0.6884 - val_loss: 1.4282 - val_acc: 0.4722\n",
      "Epoch 292/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.9239 - acc: 0.6788 - val_loss: 1.4130 - val_acc: 0.4688\n",
      "Epoch 293/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.9279 - acc: 0.6797 - val_loss: 1.4522 - val_acc: 0.4549\n",
      "Epoch 294/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 0.9394 - acc: 0.6806 - val_loss: 1.4747 - val_acc: 0.4688\n",
      "Epoch 295/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 0.9373 - acc: 0.6658 - val_loss: 1.4328 - val_acc: 0.4688\n",
      "Epoch 296/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.9356 - acc: 0.6727 - val_loss: 1.4326 - val_acc: 0.4583\n",
      "Epoch 297/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.9279 - acc: 0.6884 - val_loss: 1.4367 - val_acc: 0.4722\n",
      "Epoch 298/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.9127 - acc: 0.6910 - val_loss: 1.4388 - val_acc: 0.4583\n",
      "Epoch 299/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.9146 - acc: 0.6753 - val_loss: 1.4755 - val_acc: 0.4410\n",
      "Epoch 300/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.9181 - acc: 0.6858 - val_loss: 1.4738 - val_acc: 0.4514\n",
      "Epoch 301/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.9086 - acc: 0.6849 - val_loss: 1.4368 - val_acc: 0.4618\n",
      "Epoch 302/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 0.9381 - acc: 0.6658 - val_loss: 1.4371 - val_acc: 0.4479\n",
      "Epoch 303/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.9204 - acc: 0.6936 - val_loss: 1.4350 - val_acc: 0.4688\n",
      "Epoch 304/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 0.9058 - acc: 0.6892 - val_loss: 1.4334 - val_acc: 0.4688\n",
      "Epoch 305/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.9064 - acc: 0.6953 - val_loss: 1.4250 - val_acc: 0.4583\n",
      "Epoch 306/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.9114 - acc: 0.6814 - val_loss: 1.4239 - val_acc: 0.4826\n",
      "Epoch 307/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.9140 - acc: 0.6858 - val_loss: 1.4297 - val_acc: 0.4618\n",
      "Epoch 308/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.9186 - acc: 0.7005 - val_loss: 1.4412 - val_acc: 0.4583\n",
      "Epoch 309/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.9223 - acc: 0.6797 - val_loss: 1.4439 - val_acc: 0.4688\n",
      "Epoch 310/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 0.8898 - acc: 0.7023 - val_loss: 1.4266 - val_acc: 0.4618\n",
      "Epoch 311/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.9123 - acc: 0.6693 - val_loss: 1.4024 - val_acc: 0.4757\n",
      "Epoch 312/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.9102 - acc: 0.6858 - val_loss: 1.4341 - val_acc: 0.4618\n",
      "Epoch 313/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.8987 - acc: 0.6901 - val_loss: 1.4171 - val_acc: 0.4757\n",
      "Epoch 314/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.8967 - acc: 0.6979 - val_loss: 1.4203 - val_acc: 0.4688\n",
      "Epoch 315/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 0.8948 - acc: 0.6970 - val_loss: 1.4349 - val_acc: 0.4688\n",
      "Epoch 316/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 0.9097 - acc: 0.6953 - val_loss: 1.4272 - val_acc: 0.4583\n",
      "Epoch 317/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.8917 - acc: 0.6962 - val_loss: 1.4373 - val_acc: 0.4549\n",
      "Epoch 318/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.8997 - acc: 0.6962 - val_loss: 1.4121 - val_acc: 0.4861\n",
      "Epoch 319/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.8813 - acc: 0.6988 - val_loss: 1.4098 - val_acc: 0.4479\n",
      "Epoch 320/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.8976 - acc: 0.6962 - val_loss: 1.4169 - val_acc: 0.4618\n",
      "Epoch 321/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.8839 - acc: 0.6884 - val_loss: 1.4685 - val_acc: 0.4444\n",
      "Epoch 322/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.8952 - acc: 0.6936 - val_loss: 1.4077 - val_acc: 0.4792\n",
      "Epoch 323/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.9176 - acc: 0.6797 - val_loss: 1.4376 - val_acc: 0.4688\n",
      "Epoch 324/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.8899 - acc: 0.6979 - val_loss: 1.4071 - val_acc: 0.4653\n",
      "Epoch 325/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.8892 - acc: 0.7092 - val_loss: 1.4252 - val_acc: 0.4688\n",
      "Epoch 326/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.8877 - acc: 0.6866 - val_loss: 1.4280 - val_acc: 0.4757\n",
      "Epoch 327/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.8847 - acc: 0.6962 - val_loss: 1.4129 - val_acc: 0.4722\n",
      "Epoch 328/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.9029 - acc: 0.6753 - val_loss: 1.4536 - val_acc: 0.4549\n",
      "Epoch 329/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.8972 - acc: 0.6970 - val_loss: 1.4195 - val_acc: 0.4757\n",
      "Epoch 330/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.9013 - acc: 0.6814 - val_loss: 1.4306 - val_acc: 0.4549\n",
      "Epoch 331/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.8678 - acc: 0.7049 - val_loss: 1.4290 - val_acc: 0.4722\n",
      "Epoch 332/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.8917 - acc: 0.6970 - val_loss: 1.4346 - val_acc: 0.4792\n",
      "Epoch 333/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8780 - acc: 0.6962 - val_loss: 1.4159 - val_acc: 0.4757\n",
      "Epoch 334/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.8701 - acc: 0.7109 - val_loss: 1.4301 - val_acc: 0.4653\n",
      "Epoch 335/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.8598 - acc: 0.7031 - val_loss: 1.4099 - val_acc: 0.4792\n",
      "Epoch 336/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.8733 - acc: 0.6875 - val_loss: 1.4753 - val_acc: 0.4722\n",
      "Epoch 337/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.8746 - acc: 0.6953 - val_loss: 1.4203 - val_acc: 0.4549\n",
      "Epoch 338/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.8838 - acc: 0.6970 - val_loss: 1.4531 - val_acc: 0.4618\n",
      "Epoch 339/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.8868 - acc: 0.6962 - val_loss: 1.4465 - val_acc: 0.4583\n",
      "Epoch 340/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.8738 - acc: 0.6997 - val_loss: 1.4431 - val_acc: 0.4583\n",
      "Epoch 341/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.8699 - acc: 0.7031 - val_loss: 1.4457 - val_acc: 0.4618\n",
      "Epoch 342/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.8682 - acc: 0.7066 - val_loss: 1.4057 - val_acc: 0.4792\n",
      "Epoch 343/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 0.8649 - acc: 0.7005 - val_loss: 1.4619 - val_acc: 0.4826\n",
      "Epoch 344/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 0.8763 - acc: 0.6979 - val_loss: 1.4620 - val_acc: 0.4653\n",
      "Epoch 345/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.8408 - acc: 0.7274 - val_loss: 1.4169 - val_acc: 0.4583\n",
      "Epoch 346/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.8864 - acc: 0.6858 - val_loss: 1.4661 - val_acc: 0.4514\n",
      "Epoch 347/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 0.8662 - acc: 0.7135 - val_loss: 1.4268 - val_acc: 0.4549\n",
      "Epoch 348/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 0.8603 - acc: 0.7179 - val_loss: 1.4401 - val_acc: 0.4549\n",
      "Epoch 349/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.8589 - acc: 0.7014 - val_loss: 1.4236 - val_acc: 0.4583\n",
      "Epoch 350/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.8708 - acc: 0.7083 - val_loss: 1.3933 - val_acc: 0.4757\n",
      "Epoch 351/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.8657 - acc: 0.6936 - val_loss: 1.4226 - val_acc: 0.4653\n",
      "Epoch 352/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8437 - acc: 0.7118 - val_loss: 1.4393 - val_acc: 0.4653\n",
      "Epoch 353/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.8614 - acc: 0.7240 - val_loss: 1.4030 - val_acc: 0.4861\n",
      "Epoch 354/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.8480 - acc: 0.7222 - val_loss: 1.4111 - val_acc: 0.4792\n",
      "Epoch 355/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.8687 - acc: 0.7049 - val_loss: 1.4319 - val_acc: 0.4896\n",
      "Epoch 356/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.8650 - acc: 0.7057 - val_loss: 1.4151 - val_acc: 0.4792\n",
      "Epoch 357/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.8543 - acc: 0.7075 - val_loss: 1.4221 - val_acc: 0.4722\n",
      "Epoch 358/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.8451 - acc: 0.7135 - val_loss: 1.4255 - val_acc: 0.4653\n",
      "Epoch 359/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.8573 - acc: 0.7135 - val_loss: 1.4071 - val_acc: 0.4826\n",
      "Epoch 360/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8672 - acc: 0.7066 - val_loss: 1.4178 - val_acc: 0.4757\n",
      "Epoch 361/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.8302 - acc: 0.7248 - val_loss: 1.4042 - val_acc: 0.4965\n",
      "Epoch 362/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.8447 - acc: 0.7266 - val_loss: 1.4151 - val_acc: 0.4722\n",
      "Epoch 363/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.8739 - acc: 0.7005 - val_loss: 1.4097 - val_acc: 0.4688\n",
      "Epoch 364/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.8554 - acc: 0.7118 - val_loss: 1.4534 - val_acc: 0.4514\n",
      "Epoch 365/1000\n",
      "1152/1152 [==============================] - 1s 449us/step - loss: 0.8365 - acc: 0.7231 - val_loss: 1.4269 - val_acc: 0.4688\n",
      "Epoch 366/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 0.8586 - acc: 0.7023 - val_loss: 1.4022 - val_acc: 0.4653\n",
      "Epoch 367/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.8562 - acc: 0.7049 - val_loss: 1.4392 - val_acc: 0.4618\n",
      "Epoch 368/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.8433 - acc: 0.7144 - val_loss: 1.4388 - val_acc: 0.4722\n",
      "Epoch 369/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.8547 - acc: 0.7057 - val_loss: 1.4376 - val_acc: 0.4653\n",
      "Epoch 370/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.8542 - acc: 0.7144 - val_loss: 1.4161 - val_acc: 0.4826\n",
      "Epoch 371/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.8605 - acc: 0.7075 - val_loss: 1.4294 - val_acc: 0.4688\n",
      "Epoch 372/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.8271 - acc: 0.7161 - val_loss: 1.4343 - val_acc: 0.4722\n",
      "Epoch 373/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.8247 - acc: 0.7257 - val_loss: 1.4059 - val_acc: 0.4896\n",
      "Epoch 374/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.8300 - acc: 0.7257 - val_loss: 1.4169 - val_acc: 0.4792\n",
      "Epoch 375/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.8324 - acc: 0.7153 - val_loss: 1.4274 - val_acc: 0.4757\n",
      "Epoch 376/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.8266 - acc: 0.7257 - val_loss: 1.4085 - val_acc: 0.4688\n",
      "Epoch 377/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.8442 - acc: 0.7144 - val_loss: 1.4116 - val_acc: 0.4618\n",
      "Epoch 378/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.8319 - acc: 0.7214 - val_loss: 1.3952 - val_acc: 0.5000\n",
      "Epoch 379/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.8205 - acc: 0.7283 - val_loss: 1.4119 - val_acc: 0.4757\n",
      "Epoch 380/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.8290 - acc: 0.7257 - val_loss: 1.4129 - val_acc: 0.4688\n",
      "Epoch 381/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 0.8169 - acc: 0.7222 - val_loss: 1.4209 - val_acc: 0.4653\n",
      "Epoch 382/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.8281 - acc: 0.7118 - val_loss: 1.3943 - val_acc: 0.4931\n",
      "Epoch 383/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.8182 - acc: 0.7370 - val_loss: 1.4080 - val_acc: 0.4722\n",
      "Epoch 384/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.8152 - acc: 0.7335 - val_loss: 1.4197 - val_acc: 0.4653\n",
      "Epoch 385/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8300 - acc: 0.7231 - val_loss: 1.4261 - val_acc: 0.4757\n",
      "Epoch 386/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8431 - acc: 0.7109 - val_loss: 1.4097 - val_acc: 0.4757\n",
      "Epoch 387/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.8261 - acc: 0.7188 - val_loss: 1.4566 - val_acc: 0.4479\n",
      "Epoch 388/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.8365 - acc: 0.7075 - val_loss: 1.4203 - val_acc: 0.4826\n",
      "Epoch 389/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.8320 - acc: 0.7014 - val_loss: 1.4174 - val_acc: 0.4653\n",
      "Epoch 390/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.8031 - acc: 0.7179 - val_loss: 1.4184 - val_acc: 0.4722\n",
      "Epoch 391/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.8218 - acc: 0.7205 - val_loss: 1.4154 - val_acc: 0.4792\n",
      "Epoch 392/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.7967 - acc: 0.7266 - val_loss: 1.4329 - val_acc: 0.4757\n",
      "Epoch 393/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8099 - acc: 0.7248 - val_loss: 1.4355 - val_acc: 0.4688\n",
      "Epoch 394/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.8095 - acc: 0.7326 - val_loss: 1.4087 - val_acc: 0.4931\n",
      "Epoch 395/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.8331 - acc: 0.7161 - val_loss: 1.4129 - val_acc: 0.4965\n",
      "Epoch 396/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.8266 - acc: 0.7161 - val_loss: 1.4188 - val_acc: 0.4792\n",
      "Epoch 397/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.8353 - acc: 0.6962 - val_loss: 1.4242 - val_acc: 0.4826\n",
      "Epoch 398/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.8155 - acc: 0.7196 - val_loss: 1.3972 - val_acc: 0.5000\n",
      "Epoch 399/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.8093 - acc: 0.7335 - val_loss: 1.4368 - val_acc: 0.4549\n",
      "Epoch 400/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.8151 - acc: 0.7205 - val_loss: 1.4342 - val_acc: 0.4861\n",
      "Epoch 401/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.8064 - acc: 0.7300 - val_loss: 1.4148 - val_acc: 0.4722\n",
      "Epoch 402/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.8120 - acc: 0.7109 - val_loss: 1.4197 - val_acc: 0.4896\n",
      "Epoch 403/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.8397 - acc: 0.7179 - val_loss: 1.4641 - val_acc: 0.4826\n",
      "Epoch 404/1000\n",
      "1152/1152 [==============================] - 1s 469us/step - loss: 0.8371 - acc: 0.7222 - val_loss: 1.4033 - val_acc: 0.4861\n",
      "Epoch 405/1000\n",
      "1152/1152 [==============================] - 1s 468us/step - loss: 0.8000 - acc: 0.7300 - val_loss: 1.4222 - val_acc: 0.4688\n",
      "Epoch 406/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.7981 - acc: 0.7170 - val_loss: 1.4305 - val_acc: 0.4792\n",
      "Epoch 407/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.7986 - acc: 0.7335 - val_loss: 1.4141 - val_acc: 0.4896\n",
      "Epoch 408/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 0.8124 - acc: 0.7196 - val_loss: 1.4256 - val_acc: 0.4757\n",
      "Epoch 409/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 0.8093 - acc: 0.7266 - val_loss: 1.4335 - val_acc: 0.4826\n",
      "Epoch 410/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 0.8014 - acc: 0.7170 - val_loss: 1.4346 - val_acc: 0.4757\n",
      "Epoch 411/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.7909 - acc: 0.7257 - val_loss: 1.4080 - val_acc: 0.4931\n",
      "Epoch 412/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.8105 - acc: 0.7231 - val_loss: 1.4265 - val_acc: 0.4826\n",
      "Epoch 413/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 0.7826 - acc: 0.7292 - val_loss: 1.4233 - val_acc: 0.4792\n",
      "Epoch 414/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 0.8212 - acc: 0.734 - 0s 350us/step - loss: 0.8188 - acc: 0.7326 - val_loss: 1.4121 - val_acc: 0.4722\n",
      "Epoch 415/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.7884 - acc: 0.7300 - val_loss: 1.4295 - val_acc: 0.4792\n",
      "Epoch 416/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.7928 - acc: 0.7378 - val_loss: 1.3978 - val_acc: 0.4931\n",
      "Epoch 417/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.7757 - acc: 0.7396 - val_loss: 1.4191 - val_acc: 0.4896\n",
      "Epoch 418/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.8029 - acc: 0.7188 - val_loss: 1.4439 - val_acc: 0.4653\n",
      "Epoch 419/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.7850 - acc: 0.7405 - val_loss: 1.4157 - val_acc: 0.4722\n",
      "Epoch 420/1000\n",
      "1152/1152 [==============================] - 1s 455us/step - loss: 0.7991 - acc: 0.7205 - val_loss: 1.4081 - val_acc: 0.4896\n",
      "Epoch 421/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.7754 - acc: 0.7526 - val_loss: 1.4292 - val_acc: 0.4792\n",
      "Epoch 422/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.7760 - acc: 0.7405 - val_loss: 1.4083 - val_acc: 0.5104\n",
      "Epoch 423/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.8028 - acc: 0.7240 - val_loss: 1.4171 - val_acc: 0.4861\n",
      "Epoch 424/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.8061 - acc: 0.7179 - val_loss: 1.4295 - val_acc: 0.4931\n",
      "Epoch 425/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7776 - acc: 0.7283 - val_loss: 1.4386 - val_acc: 0.4688\n",
      "Epoch 426/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.7857 - acc: 0.7483 - val_loss: 1.4795 - val_acc: 0.4618\n",
      "Epoch 427/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.7681 - acc: 0.7387 - val_loss: 1.4726 - val_acc: 0.4618\n",
      "Epoch 428/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7883 - acc: 0.7266 - val_loss: 1.4305 - val_acc: 0.4688\n",
      "Epoch 429/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.7847 - acc: 0.7248 - val_loss: 1.4154 - val_acc: 0.4965\n",
      "Epoch 430/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.7819 - acc: 0.7457 - val_loss: 1.4024 - val_acc: 0.4826\n",
      "Epoch 431/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.7584 - acc: 0.7439 - val_loss: 1.4220 - val_acc: 0.4792\n",
      "Epoch 432/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.8013 - acc: 0.7127 - val_loss: 1.3954 - val_acc: 0.4931\n",
      "Epoch 433/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.7919 - acc: 0.7352 - val_loss: 1.4184 - val_acc: 0.4861\n",
      "Epoch 434/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.7898 - acc: 0.7326 - val_loss: 1.4522 - val_acc: 0.4757\n",
      "Epoch 435/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.7906 - acc: 0.7274 - val_loss: 1.4458 - val_acc: 0.4688\n",
      "Epoch 436/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.7752 - acc: 0.7439 - val_loss: 1.4126 - val_acc: 0.4861\n",
      "Epoch 437/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.7607 - acc: 0.7509 - val_loss: 1.4457 - val_acc: 0.4722\n",
      "Epoch 438/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.7767 - acc: 0.7300 - val_loss: 1.4102 - val_acc: 0.4931\n",
      "Epoch 439/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.7792 - acc: 0.7309 - val_loss: 1.4153 - val_acc: 0.5035\n",
      "Epoch 440/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.7789 - acc: 0.7396 - val_loss: 1.4146 - val_acc: 0.4792\n",
      "Epoch 441/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.7507 - acc: 0.7517 - val_loss: 1.4183 - val_acc: 0.4965\n",
      "Epoch 442/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.7812 - acc: 0.7222 - val_loss: 1.4180 - val_acc: 0.4931\n",
      "Epoch 443/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.7620 - acc: 0.7422 - val_loss: 1.4210 - val_acc: 0.4861\n",
      "Epoch 444/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.7638 - acc: 0.7413 - val_loss: 1.4236 - val_acc: 0.4792\n",
      "Epoch 445/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 0.7611 - acc: 0.7344 - val_loss: 1.4295 - val_acc: 0.4826\n",
      "Epoch 446/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.7801 - acc: 0.7292 - val_loss: 1.4118 - val_acc: 0.4826\n",
      "Epoch 447/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.7738 - acc: 0.7405 - val_loss: 1.4667 - val_acc: 0.4861\n",
      "Epoch 448/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.7568 - acc: 0.7378 - val_loss: 1.4339 - val_acc: 0.4896\n",
      "Epoch 449/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.7666 - acc: 0.7474 - val_loss: 1.4310 - val_acc: 0.4757\n",
      "Epoch 450/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 0.7575 - acc: 0.7431 - val_loss: 1.4101 - val_acc: 0.5000\n",
      "Epoch 451/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.7676 - acc: 0.7517 - val_loss: 1.4070 - val_acc: 0.4931\n",
      "Epoch 452/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.7580 - acc: 0.7439 - val_loss: 1.4202 - val_acc: 0.5000\n",
      "Epoch 453/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.7685 - acc: 0.7370 - val_loss: 1.4325 - val_acc: 0.4826\n",
      "Epoch 454/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.7597 - acc: 0.7405 - val_loss: 1.4321 - val_acc: 0.4826\n",
      "Epoch 455/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.7694 - acc: 0.7283 - val_loss: 1.4152 - val_acc: 0.5000\n",
      "Epoch 456/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.7513 - acc: 0.7465 - val_loss: 1.4240 - val_acc: 0.4826\n",
      "Epoch 457/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.7819 - acc: 0.7448 - val_loss: 1.4322 - val_acc: 0.4792\n",
      "Epoch 458/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.7550 - acc: 0.7457 - val_loss: 1.4070 - val_acc: 0.5000\n",
      "Epoch 459/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7444 - acc: 0.7535 - val_loss: 1.4575 - val_acc: 0.4861\n",
      "Epoch 460/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.7483 - acc: 0.7387 - val_loss: 1.4175 - val_acc: 0.4931\n",
      "Epoch 461/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.7506 - acc: 0.7465 - val_loss: 1.4107 - val_acc: 0.4896\n",
      "Epoch 462/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7408 - acc: 0.7526 - val_loss: 1.4466 - val_acc: 0.4549\n",
      "Epoch 463/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.7505 - acc: 0.7474 - val_loss: 1.4317 - val_acc: 0.4722\n",
      "Epoch 464/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.7637 - acc: 0.7335 - val_loss: 1.4187 - val_acc: 0.4861\n",
      "Epoch 465/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 0.7610 - acc: 0.7448 - val_loss: 1.4026 - val_acc: 0.4861\n",
      "Epoch 466/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.7439 - acc: 0.7431 - val_loss: 1.4191 - val_acc: 0.4896\n",
      "Epoch 467/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.7655 - acc: 0.7292 - val_loss: 1.4555 - val_acc: 0.4688\n",
      "Epoch 468/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.7313 - acc: 0.7648 - val_loss: 1.4214 - val_acc: 0.4931\n",
      "Epoch 469/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 0.7727 - acc: 0.7387 - val_loss: 1.4393 - val_acc: 0.4792\n",
      "Epoch 470/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 0.7606 - acc: 0.7439 - val_loss: 1.4148 - val_acc: 0.5104\n",
      "Epoch 471/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 0.7522 - acc: 0.7448 - val_loss: 1.4162 - val_acc: 0.5000\n",
      "Epoch 472/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.7413 - acc: 0.7370 - val_loss: 1.4237 - val_acc: 0.4861\n",
      "Epoch 473/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.7399 - acc: 0.7448 - val_loss: 1.4077 - val_acc: 0.5069\n",
      "Epoch 474/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.7422 - acc: 0.7500 - val_loss: 1.4436 - val_acc: 0.4792\n",
      "Epoch 475/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.7556 - acc: 0.7517 - val_loss: 1.4104 - val_acc: 0.5139\n",
      "Epoch 476/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 0.7389 - acc: 0.7474 - val_loss: 1.4122 - val_acc: 0.4896\n",
      "Epoch 477/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.7307 - acc: 0.7509 - val_loss: 1.4222 - val_acc: 0.4931\n",
      "Epoch 478/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7351 - acc: 0.7569 - val_loss: 1.4082 - val_acc: 0.4896\n",
      "Epoch 479/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.7427 - acc: 0.7439 - val_loss: 1.4681 - val_acc: 0.4931\n",
      "Epoch 480/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.7497 - acc: 0.7405 - val_loss: 1.4315 - val_acc: 0.5035\n",
      "Epoch 481/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.7436 - acc: 0.7370 - val_loss: 1.4643 - val_acc: 0.4792\n",
      "Epoch 482/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 0.7517 - acc: 0.7413 - val_loss: 1.4150 - val_acc: 0.5174\n",
      "Epoch 483/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.7202 - acc: 0.7491 - val_loss: 1.4632 - val_acc: 0.4618\n",
      "Epoch 484/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.7752 - acc: 0.7370 - val_loss: 1.4405 - val_acc: 0.4931\n",
      "Epoch 485/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7360 - acc: 0.7491 - val_loss: 1.3958 - val_acc: 0.5139\n",
      "Epoch 486/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.7378 - acc: 0.7491 - val_loss: 1.3893 - val_acc: 0.5035\n",
      "Epoch 487/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 0.7399 - acc: 0.7526 - val_loss: 1.4265 - val_acc: 0.4931\n",
      "Epoch 488/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.7320 - acc: 0.7552 - val_loss: 1.4051 - val_acc: 0.5035\n",
      "Epoch 489/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7493 - acc: 0.7517 - val_loss: 1.3965 - val_acc: 0.5069\n",
      "Epoch 490/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.7294 - acc: 0.7578 - val_loss: 1.3981 - val_acc: 0.4965\n",
      "Epoch 491/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 0.7204 - acc: 0.7622 - val_loss: 1.4266 - val_acc: 0.4896\n",
      "Epoch 492/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.7211 - acc: 0.7682 - val_loss: 1.4079 - val_acc: 0.4931\n",
      "Epoch 493/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.7204 - acc: 0.7517 - val_loss: 1.4033 - val_acc: 0.5035\n",
      "Epoch 494/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.7364 - acc: 0.7604 - val_loss: 1.4145 - val_acc: 0.5000\n",
      "Epoch 495/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.7446 - acc: 0.7361 - val_loss: 1.4038 - val_acc: 0.5139\n",
      "Epoch 496/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.7337 - acc: 0.7561 - val_loss: 1.4410 - val_acc: 0.5069\n",
      "Epoch 497/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.7175 - acc: 0.7595 - val_loss: 1.4067 - val_acc: 0.5174\n",
      "Epoch 498/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7421 - acc: 0.7491 - val_loss: 1.4168 - val_acc: 0.5139\n",
      "Epoch 499/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.7108 - acc: 0.7639 - val_loss: 1.4104 - val_acc: 0.4896\n",
      "Epoch 500/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.7287 - acc: 0.7465 - val_loss: 1.4172 - val_acc: 0.5104\n",
      "Epoch 501/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7041 - acc: 0.7682 - val_loss: 1.4388 - val_acc: 0.5000\n",
      "Epoch 502/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.7383 - acc: 0.7405 - val_loss: 1.4410 - val_acc: 0.5035\n",
      "Epoch 503/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.7390 - acc: 0.7431 - val_loss: 1.4285 - val_acc: 0.5104\n",
      "Epoch 504/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.7246 - acc: 0.7543 - val_loss: 1.4190 - val_acc: 0.4931\n",
      "Epoch 505/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.7158 - acc: 0.7569 - val_loss: 1.4210 - val_acc: 0.5069\n",
      "Epoch 506/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.7194 - acc: 0.7552 - val_loss: 1.4193 - val_acc: 0.5035\n",
      "Epoch 507/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.7192 - acc: 0.7491 - val_loss: 1.4067 - val_acc: 0.4861\n",
      "Epoch 508/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.7413 - acc: 0.7552 - val_loss: 1.4300 - val_acc: 0.5069\n",
      "Epoch 509/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 0.7342 - acc: 0.7483 - val_loss: 1.4309 - val_acc: 0.4931\n",
      "Epoch 510/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.7145 - acc: 0.7517 - val_loss: 1.4079 - val_acc: 0.5104\n",
      "Epoch 511/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.7234 - acc: 0.7569 - val_loss: 1.4134 - val_acc: 0.4965\n",
      "Epoch 512/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.7169 - acc: 0.7587 - val_loss: 1.4308 - val_acc: 0.5139\n",
      "Epoch 513/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.7097 - acc: 0.7483 - val_loss: 1.3987 - val_acc: 0.4965\n",
      "Epoch 514/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.7149 - acc: 0.7595 - val_loss: 1.4380 - val_acc: 0.5000\n",
      "Epoch 515/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.7215 - acc: 0.7517 - val_loss: 1.4454 - val_acc: 0.4931\n",
      "Epoch 516/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.7076 - acc: 0.7717 - val_loss: 1.4252 - val_acc: 0.5000\n",
      "Epoch 517/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.6896 - acc: 0.7682 - val_loss: 1.4126 - val_acc: 0.5208\n",
      "Epoch 518/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7155 - acc: 0.7595 - val_loss: 1.4198 - val_acc: 0.5035\n",
      "Epoch 519/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6985 - acc: 0.7604 - val_loss: 1.4111 - val_acc: 0.4896\n",
      "Epoch 520/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.7106 - acc: 0.7561 - val_loss: 1.4203 - val_acc: 0.5174\n",
      "Epoch 521/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.7073 - acc: 0.7569 - val_loss: 1.4218 - val_acc: 0.4965\n",
      "Epoch 522/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7194 - acc: 0.7491 - val_loss: 1.4063 - val_acc: 0.5174\n",
      "Epoch 523/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.7104 - acc: 0.7578 - val_loss: 1.4233 - val_acc: 0.4826\n",
      "Epoch 524/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.7074 - acc: 0.7691 - val_loss: 1.4307 - val_acc: 0.4861\n",
      "Epoch 525/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7090 - acc: 0.7648 - val_loss: 1.4154 - val_acc: 0.4861\n",
      "Epoch 526/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.6958 - acc: 0.7665 - val_loss: 1.4084 - val_acc: 0.5278\n",
      "Epoch 527/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.7207 - acc: 0.7648 - val_loss: 1.4322 - val_acc: 0.5104\n",
      "Epoch 528/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.7064 - acc: 0.7639 - val_loss: 1.4390 - val_acc: 0.4722\n",
      "Epoch 529/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.6989 - acc: 0.7578 - val_loss: 1.4168 - val_acc: 0.4931\n",
      "Epoch 530/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.7209 - acc: 0.7561 - val_loss: 1.4154 - val_acc: 0.4826\n",
      "Epoch 531/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 0.6988 - acc: 0.7587 - val_loss: 1.3945 - val_acc: 0.5035\n",
      "Epoch 532/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.7016 - acc: 0.7656 - val_loss: 1.4266 - val_acc: 0.5139\n",
      "Epoch 533/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 0.7164 - acc: 0.7465 - val_loss: 1.3987 - val_acc: 0.5069\n",
      "Epoch 534/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.7285 - acc: 0.7422 - val_loss: 1.4109 - val_acc: 0.5035\n",
      "Epoch 535/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7188 - acc: 0.7613 - val_loss: 1.4197 - val_acc: 0.4861\n",
      "Epoch 536/1000\n",
      "1152/1152 [==============================] - 0s 395us/step - loss: 0.7051 - acc: 0.7517 - val_loss: 1.4070 - val_acc: 0.5104\n",
      "Epoch 537/1000\n",
      "1152/1152 [==============================] - 1s 438us/step - loss: 0.7144 - acc: 0.7448 - val_loss: 1.3992 - val_acc: 0.4931\n",
      "Epoch 538/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.7084 - acc: 0.7639 - val_loss: 1.4169 - val_acc: 0.4861\n",
      "Epoch 539/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.7015 - acc: 0.7543 - val_loss: 1.4153 - val_acc: 0.4931\n",
      "Epoch 540/1000\n",
      "1152/1152 [==============================] - 1s 436us/step - loss: 0.7203 - acc: 0.7509 - val_loss: 1.4442 - val_acc: 0.5104\n",
      "Epoch 541/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.7062 - acc: 0.7708 - val_loss: 1.4104 - val_acc: 0.5035\n",
      "Epoch 542/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6995 - acc: 0.7474 - val_loss: 1.4420 - val_acc: 0.4757\n",
      "Epoch 543/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.6824 - acc: 0.7648 - val_loss: 1.4138 - val_acc: 0.5174\n",
      "Epoch 544/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.6880 - acc: 0.7578 - val_loss: 1.4076 - val_acc: 0.5000\n",
      "Epoch 545/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.7197 - acc: 0.7526 - val_loss: 1.4261 - val_acc: 0.4826\n",
      "Epoch 546/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.6781 - acc: 0.7665 - val_loss: 1.4181 - val_acc: 0.4931\n",
      "Epoch 547/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.7018 - acc: 0.7691 - val_loss: 1.4246 - val_acc: 0.5139\n",
      "Epoch 548/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.6975 - acc: 0.7535 - val_loss: 1.4459 - val_acc: 0.4826\n",
      "Epoch 549/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.6955 - acc: 0.7552 - val_loss: 1.4398 - val_acc: 0.4861\n",
      "Epoch 550/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.7187 - acc: 0.7552 - val_loss: 1.4227 - val_acc: 0.4965\n",
      "Epoch 551/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.6889 - acc: 0.7578 - val_loss: 1.4233 - val_acc: 0.5174\n",
      "Epoch 552/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.7034 - acc: 0.7578 - val_loss: 1.4384 - val_acc: 0.5174\n",
      "Epoch 553/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.6946 - acc: 0.7569 - val_loss: 1.4651 - val_acc: 0.4722\n",
      "Epoch 554/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.6781 - acc: 0.7691 - val_loss: 1.4285 - val_acc: 0.4931\n",
      "Epoch 555/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.7002 - acc: 0.7604 - val_loss: 1.4329 - val_acc: 0.4931\n",
      "Epoch 556/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.6909 - acc: 0.7656 - val_loss: 1.4332 - val_acc: 0.5139\n",
      "Epoch 557/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.6837 - acc: 0.7630 - val_loss: 1.4447 - val_acc: 0.5174\n",
      "Epoch 558/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.6961 - acc: 0.7491 - val_loss: 1.4389 - val_acc: 0.4757\n",
      "Epoch 559/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.6733 - acc: 0.7674 - val_loss: 1.4443 - val_acc: 0.4896\n",
      "Epoch 560/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 0.6975 - acc: 0.7760 - val_loss: 1.4498 - val_acc: 0.4965\n",
      "Epoch 561/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6712 - acc: 0.7778 - val_loss: 1.4079 - val_acc: 0.5069\n",
      "Epoch 562/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.6741 - acc: 0.7595 - val_loss: 1.4202 - val_acc: 0.5104\n",
      "Epoch 563/1000\n",
      "1152/1152 [==============================] - 0s 304us/step - loss: 0.7010 - acc: 0.7674 - val_loss: 1.4332 - val_acc: 0.4931\n",
      "Epoch 564/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.6773 - acc: 0.7769 - val_loss: 1.4178 - val_acc: 0.5069\n",
      "Epoch 565/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.7025 - acc: 0.7691 - val_loss: 1.4104 - val_acc: 0.5208\n",
      "Epoch 566/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.6909 - acc: 0.7648 - val_loss: 1.4583 - val_acc: 0.5035\n",
      "Epoch 567/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.6707 - acc: 0.7630 - val_loss: 1.4376 - val_acc: 0.4757\n",
      "Epoch 568/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.6866 - acc: 0.7639 - val_loss: 1.4280 - val_acc: 0.5069\n",
      "Epoch 569/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.6706 - acc: 0.7622 - val_loss: 1.4215 - val_acc: 0.5035\n",
      "Epoch 570/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6892 - acc: 0.7656 - val_loss: 1.4076 - val_acc: 0.5069\n",
      "Epoch 571/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6746 - acc: 0.7726 - val_loss: 1.4196 - val_acc: 0.5069\n",
      "Epoch 572/1000\n",
      "1152/1152 [==============================] - 1s 460us/step - loss: 0.6590 - acc: 0.7856 - val_loss: 1.4575 - val_acc: 0.4826\n",
      "Epoch 573/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.6818 - acc: 0.7752 - val_loss: 1.4193 - val_acc: 0.5069\n",
      "Epoch 574/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.6571 - acc: 0.7804 - val_loss: 1.4494 - val_acc: 0.5069\n",
      "Epoch 575/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 0.6650 - acc: 0.7674 - val_loss: 1.4443 - val_acc: 0.4965\n",
      "Epoch 576/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.6931 - acc: 0.7561 - val_loss: 1.4100 - val_acc: 0.5104\n",
      "Epoch 577/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.6948 - acc: 0.7587 - val_loss: 1.4419 - val_acc: 0.4896\n",
      "Epoch 578/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.6700 - acc: 0.7786 - val_loss: 1.4325 - val_acc: 0.5139\n",
      "Epoch 579/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.6833 - acc: 0.7830 - val_loss: 1.4155 - val_acc: 0.5139\n",
      "Epoch 580/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.6892 - acc: 0.7526 - val_loss: 1.4203 - val_acc: 0.5035\n",
      "Epoch 581/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.6668 - acc: 0.7691 - val_loss: 1.4332 - val_acc: 0.5069\n",
      "Epoch 582/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6984 - acc: 0.7708 - val_loss: 1.4264 - val_acc: 0.5035\n",
      "Epoch 583/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.6663 - acc: 0.7708 - val_loss: 1.4229 - val_acc: 0.5069\n",
      "Epoch 584/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.6760 - acc: 0.7769 - val_loss: 1.4405 - val_acc: 0.5069\n",
      "Epoch 585/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.6713 - acc: 0.7708 - val_loss: 1.4589 - val_acc: 0.4861\n",
      "Epoch 586/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6502 - acc: 0.7726 - val_loss: 1.4237 - val_acc: 0.5000\n",
      "Epoch 587/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.6605 - acc: 0.7743 - val_loss: 1.4763 - val_acc: 0.4722\n",
      "Epoch 588/1000\n",
      "1152/1152 [==============================] - 1s 443us/step - loss: 0.6981 - acc: 0.7604 - val_loss: 1.4395 - val_acc: 0.5000\n",
      "Epoch 589/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.6599 - acc: 0.7839 - val_loss: 1.4427 - val_acc: 0.5069\n",
      "Epoch 590/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.6652 - acc: 0.7839 - val_loss: 1.4454 - val_acc: 0.4826\n",
      "Epoch 591/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6821 - acc: 0.7595 - val_loss: 1.4556 - val_acc: 0.4688\n",
      "Epoch 592/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.6599 - acc: 0.7717 - val_loss: 1.4355 - val_acc: 0.4965\n",
      "Epoch 593/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.6496 - acc: 0.7786 - val_loss: 1.4268 - val_acc: 0.4965\n",
      "Epoch 594/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.6858 - acc: 0.7734 - val_loss: 1.4344 - val_acc: 0.4757\n",
      "Epoch 595/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.6445 - acc: 0.7682 - val_loss: 1.4286 - val_acc: 0.5035\n",
      "Epoch 596/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.6669 - acc: 0.7604 - val_loss: 1.4336 - val_acc: 0.4931\n",
      "Epoch 597/1000\n",
      "1152/1152 [==============================] - 0s 422us/step - loss: 0.6628 - acc: 0.7726 - val_loss: 1.4411 - val_acc: 0.4931\n",
      "Epoch 598/1000\n",
      "1152/1152 [==============================] - 1s 442us/step - loss: 0.6715 - acc: 0.7708 - val_loss: 1.4278 - val_acc: 0.4931\n",
      "Epoch 599/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.6517 - acc: 0.7778 - val_loss: 1.4276 - val_acc: 0.5069\n",
      "Epoch 600/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.6712 - acc: 0.7752 - val_loss: 1.4274 - val_acc: 0.5035\n",
      "Epoch 601/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6683 - acc: 0.7717 - val_loss: 1.4558 - val_acc: 0.5069\n",
      "Epoch 602/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.6468 - acc: 0.7656 - val_loss: 1.4352 - val_acc: 0.5035\n",
      "Epoch 603/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.6673 - acc: 0.7648 - val_loss: 1.4392 - val_acc: 0.5069\n",
      "Epoch 604/1000\n",
      "1152/1152 [==============================] - 1s 526us/step - loss: 0.6559 - acc: 0.7726 - val_loss: 1.4474 - val_acc: 0.4931\n",
      "Epoch 605/1000\n",
      "1152/1152 [==============================] - 0s 401us/step - loss: 0.6435 - acc: 0.7795 - val_loss: 1.4754 - val_acc: 0.4826\n",
      "Epoch 606/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.6507 - acc: 0.7752 - val_loss: 1.4785 - val_acc: 0.4826\n",
      "Epoch 607/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.6793 - acc: 0.7717 - val_loss: 1.4395 - val_acc: 0.4965\n",
      "Epoch 608/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.6440 - acc: 0.7778 - val_loss: 1.4225 - val_acc: 0.5139\n",
      "Epoch 609/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.6514 - acc: 0.7700 - val_loss: 1.4242 - val_acc: 0.4896\n",
      "Epoch 610/1000\n",
      "1152/1152 [==============================] - 1s 476us/step - loss: 0.6570 - acc: 0.7717 - val_loss: 1.4290 - val_acc: 0.5069\n",
      "Epoch 611/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.6507 - acc: 0.7891 - val_loss: 1.4124 - val_acc: 0.5243\n",
      "Epoch 612/1000\n",
      "1152/1152 [==============================] - 0s 391us/step - loss: 0.6614 - acc: 0.7743 - val_loss: 1.4304 - val_acc: 0.5069\n",
      "Epoch 613/1000\n",
      "1152/1152 [==============================] - 0s 395us/step - loss: 0.6371 - acc: 0.7960 - val_loss: 1.4367 - val_acc: 0.5312\n",
      "Epoch 614/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.6397 - acc: 0.7934 - val_loss: 1.4291 - val_acc: 0.5243\n",
      "Epoch 615/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 0.6790 - acc: 0.7639 - val_loss: 1.4581 - val_acc: 0.5208\n",
      "Epoch 616/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.6648 - acc: 0.7648 - val_loss: 1.4489 - val_acc: 0.4826\n",
      "Epoch 617/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 0.6496 - acc: 0.7717 - val_loss: 1.4231 - val_acc: 0.5174\n",
      "Epoch 618/1000\n",
      "1152/1152 [==============================] - 1s 482us/step - loss: 0.6525 - acc: 0.7700 - val_loss: 1.4419 - val_acc: 0.5000\n",
      "Epoch 619/1000\n",
      "1152/1152 [==============================] - 1s 454us/step - loss: 0.6433 - acc: 0.7804 - val_loss: 1.4182 - val_acc: 0.5208\n",
      "Epoch 620/1000\n",
      "1152/1152 [==============================] - 1s 447us/step - loss: 0.6315 - acc: 0.7752 - val_loss: 1.4422 - val_acc: 0.4931\n",
      "Epoch 621/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.6418 - acc: 0.7830 - val_loss: 1.4290 - val_acc: 0.5174\n",
      "Epoch 622/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.6534 - acc: 0.7595 - val_loss: 1.4154 - val_acc: 0.5069\n",
      "Epoch 623/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.6526 - acc: 0.7839 - val_loss: 1.4171 - val_acc: 0.5139\n",
      "Epoch 624/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.6741 - acc: 0.7656 - val_loss: 1.4339 - val_acc: 0.5104\n",
      "Epoch 625/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.6718 - acc: 0.7622 - val_loss: 1.4485 - val_acc: 0.4931\n",
      "Epoch 626/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.6416 - acc: 0.7804 - val_loss: 1.4223 - val_acc: 0.5174\n",
      "Epoch 627/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.6671 - acc: 0.7804 - val_loss: 1.4121 - val_acc: 0.5104\n",
      "Epoch 628/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.6353 - acc: 0.7865 - val_loss: 1.4410 - val_acc: 0.5139\n",
      "Epoch 629/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.6469 - acc: 0.7726 - val_loss: 1.4386 - val_acc: 0.5000\n",
      "Epoch 630/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.6596 - acc: 0.7674 - val_loss: 1.4338 - val_acc: 0.5035\n",
      "Epoch 631/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.6430 - acc: 0.7691 - val_loss: 1.4275 - val_acc: 0.5174\n",
      "Epoch 632/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 0.6558 - acc: 0.7648 - val_loss: 1.4192 - val_acc: 0.5208\n",
      "Epoch 633/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 0.6431 - acc: 0.7743 - val_loss: 1.4538 - val_acc: 0.5174\n",
      "Epoch 634/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.6496 - acc: 0.7682 - val_loss: 1.4474 - val_acc: 0.5174\n",
      "Epoch 635/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.6439 - acc: 0.7734 - val_loss: 1.4401 - val_acc: 0.4861\n",
      "Epoch 636/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 0.6362 - acc: 0.7691 - val_loss: 1.4442 - val_acc: 0.4861\n",
      "Epoch 637/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.6619 - acc: 0.7769 - val_loss: 1.4295 - val_acc: 0.5069\n",
      "Epoch 638/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.6307 - acc: 0.7812 - val_loss: 1.4352 - val_acc: 0.4965\n",
      "Epoch 639/1000\n",
      "1152/1152 [==============================] - 1s 448us/step - loss: 0.6503 - acc: 0.7812 - val_loss: 1.4270 - val_acc: 0.5208\n",
      "Epoch 640/1000\n",
      "1152/1152 [==============================] - 0s 432us/step - loss: 0.6097 - acc: 0.7960 - val_loss: 1.4142 - val_acc: 0.5174\n",
      "Epoch 641/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.6585 - acc: 0.7760 - val_loss: 1.4301 - val_acc: 0.5035\n",
      "Epoch 642/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.6327 - acc: 0.7899 - val_loss: 1.4363 - val_acc: 0.5104\n",
      "Epoch 643/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.6399 - acc: 0.7795 - val_loss: 1.4539 - val_acc: 0.5000\n",
      "Epoch 644/1000\n",
      "1152/1152 [==============================] - 1s 554us/step - loss: 0.6348 - acc: 0.7891 - val_loss: 1.4410 - val_acc: 0.4965\n",
      "Epoch 645/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 0.6513 - acc: 0.7830 - val_loss: 1.4335 - val_acc: 0.5069\n",
      "Epoch 646/1000\n",
      "1152/1152 [==============================] - 0s 434us/step - loss: 0.6324 - acc: 0.7969 - val_loss: 1.4219 - val_acc: 0.5069\n",
      "Epoch 647/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.6265 - acc: 0.7873 - val_loss: 1.4126 - val_acc: 0.5208\n",
      "Epoch 648/1000\n",
      "1152/1152 [==============================] - 1s 498us/step - loss: 0.6435 - acc: 0.7648 - val_loss: 1.4449 - val_acc: 0.5104\n",
      "Epoch 649/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6457 - acc: 0.7795 - val_loss: 1.4272 - val_acc: 0.5069\n",
      "Epoch 650/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.6460 - acc: 0.7769 - val_loss: 1.4626 - val_acc: 0.4861\n",
      "Epoch 651/1000\n",
      "1152/1152 [==============================] - 1s 457us/step - loss: 0.6232 - acc: 0.7960 - val_loss: 1.4188 - val_acc: 0.5312\n",
      "Epoch 652/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.6358 - acc: 0.7891 - val_loss: 1.4723 - val_acc: 0.4861\n",
      "Epoch 653/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.6523 - acc: 0.7700 - val_loss: 1.4459 - val_acc: 0.5208\n",
      "Epoch 654/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.6125 - acc: 0.8038 - val_loss: 1.4452 - val_acc: 0.5208\n",
      "Epoch 655/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.6420 - acc: 0.7760 - val_loss: 1.4337 - val_acc: 0.5139\n",
      "Epoch 656/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.6423 - acc: 0.7674 - val_loss: 1.4300 - val_acc: 0.5174\n",
      "Epoch 657/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6220 - acc: 0.7891 - val_loss: 1.4293 - val_acc: 0.5069\n",
      "Epoch 658/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.6350 - acc: 0.7977 - val_loss: 1.4464 - val_acc: 0.5139\n",
      "Epoch 659/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.6016 - acc: 0.7934 - val_loss: 1.4197 - val_acc: 0.5278\n",
      "Epoch 660/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.6070 - acc: 0.7934 - val_loss: 1.4164 - val_acc: 0.4965\n",
      "Epoch 661/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.6316 - acc: 0.7778 - val_loss: 1.4566 - val_acc: 0.4826\n",
      "Epoch 662/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.6070 - acc: 0.8108 - val_loss: 1.4137 - val_acc: 0.4965\n",
      "Epoch 663/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.6154 - acc: 0.7995 - val_loss: 1.4201 - val_acc: 0.5139\n",
      "Epoch 664/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 0.6174 - acc: 0.7925 - val_loss: 1.4382 - val_acc: 0.5139\n",
      "Epoch 665/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.6166 - acc: 0.7934 - val_loss: 1.4543 - val_acc: 0.5069\n",
      "Epoch 666/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.6378 - acc: 0.7726 - val_loss: 1.4456 - val_acc: 0.4931\n",
      "Epoch 667/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.6298 - acc: 0.7795 - val_loss: 1.4224 - val_acc: 0.5174\n",
      "Epoch 668/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.6150 - acc: 0.7882 - val_loss: 1.4197 - val_acc: 0.5278\n",
      "Epoch 669/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.6413 - acc: 0.7769 - val_loss: 1.4564 - val_acc: 0.5000\n",
      "Epoch 670/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.6280 - acc: 0.7865 - val_loss: 1.4505 - val_acc: 0.5069\n",
      "Epoch 671/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.6298 - acc: 0.7865 - val_loss: 1.4402 - val_acc: 0.5069\n",
      "Epoch 672/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.6152 - acc: 0.7847 - val_loss: 1.4176 - val_acc: 0.4965\n",
      "Epoch 673/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.6415 - acc: 0.7691 - val_loss: 1.4294 - val_acc: 0.5174\n",
      "Epoch 674/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.6126 - acc: 0.7908 - val_loss: 1.4532 - val_acc: 0.5208\n",
      "Epoch 675/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.6054 - acc: 0.8073 - val_loss: 1.4461 - val_acc: 0.5208\n",
      "Epoch 676/1000\n",
      "1152/1152 [==============================] - 0s 434us/step - loss: 0.6182 - acc: 0.7839 - val_loss: 1.4599 - val_acc: 0.4861\n",
      "Epoch 677/1000\n",
      "1152/1152 [==============================] - 1s 488us/step - loss: 0.6237 - acc: 0.7865 - val_loss: 1.4537 - val_acc: 0.5000\n",
      "Epoch 678/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.6231 - acc: 0.7778 - val_loss: 1.4688 - val_acc: 0.5000\n",
      "Epoch 679/1000\n",
      "1152/1152 [==============================] - 1s 449us/step - loss: 0.6114 - acc: 0.7891 - val_loss: 1.4298 - val_acc: 0.5069\n",
      "Epoch 680/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.6016 - acc: 0.7995 - val_loss: 1.4677 - val_acc: 0.5035\n",
      "Epoch 681/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 0.6340 - acc: 0.7743 - val_loss: 1.4296 - val_acc: 0.5208\n",
      "Epoch 682/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.6144 - acc: 0.8003 - val_loss: 1.4706 - val_acc: 0.4861\n",
      "Epoch 683/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.6269 - acc: 0.7708 - val_loss: 1.4328 - val_acc: 0.5208\n",
      "Epoch 684/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.6039 - acc: 0.7882 - val_loss: 1.4245 - val_acc: 0.5174\n",
      "Epoch 685/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.6054 - acc: 0.7960 - val_loss: 1.4297 - val_acc: 0.5174\n",
      "Epoch 686/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.6134 - acc: 0.7769 - val_loss: 1.4543 - val_acc: 0.5035\n",
      "Epoch 687/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.6263 - acc: 0.7977 - val_loss: 1.4210 - val_acc: 0.5000\n",
      "Epoch 688/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.6108 - acc: 0.7951 - val_loss: 1.4316 - val_acc: 0.5174\n",
      "Epoch 689/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.6312 - acc: 0.7743 - val_loss: 1.4362 - val_acc: 0.4931\n",
      "Epoch 690/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6035 - acc: 0.7925 - val_loss: 1.4408 - val_acc: 0.5104\n",
      "Epoch 691/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.6103 - acc: 0.7986 - val_loss: 1.4182 - val_acc: 0.5243\n",
      "Epoch 692/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.6213 - acc: 0.7969 - val_loss: 1.4468 - val_acc: 0.5208\n",
      "Epoch 693/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.5989 - acc: 0.8047 - val_loss: 1.4482 - val_acc: 0.4965\n",
      "Epoch 694/1000\n",
      "1152/1152 [==============================] - 1s 551us/step - loss: 0.6097 - acc: 0.7969 - val_loss: 1.4232 - val_acc: 0.5104\n",
      "Epoch 695/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.5917 - acc: 0.7977 - val_loss: 1.4138 - val_acc: 0.5382\n",
      "Epoch 696/1000\n",
      "1152/1152 [==============================] - 1s 451us/step - loss: 0.6097 - acc: 0.7882 - val_loss: 1.4442 - val_acc: 0.5208\n",
      "Epoch 697/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 0.5969 - acc: 0.7951 - val_loss: 1.4371 - val_acc: 0.5035\n",
      "Epoch 698/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 0.6159 - acc: 0.7778 - val_loss: 1.4313 - val_acc: 0.5000\n",
      "Epoch 699/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5897 - acc: 0.8116 - val_loss: 1.4587 - val_acc: 0.5174\n",
      "Epoch 700/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.6059 - acc: 0.8030 - val_loss: 1.4197 - val_acc: 0.5139\n",
      "Epoch 701/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.5943 - acc: 0.7977 - val_loss: 1.4398 - val_acc: 0.5174\n",
      "Epoch 702/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 0.6101 - acc: 0.784 - 0s 361us/step - loss: 0.6077 - acc: 0.7856 - val_loss: 1.4619 - val_acc: 0.5104\n",
      "Epoch 703/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.6203 - acc: 0.7847 - val_loss: 1.4275 - val_acc: 0.5069\n",
      "Epoch 704/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.6026 - acc: 0.7969 - val_loss: 1.4466 - val_acc: 0.5139\n",
      "Epoch 705/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.5645 - acc: 0.8108 - val_loss: 1.4305 - val_acc: 0.5035\n",
      "Epoch 706/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.6100 - acc: 0.7821 - val_loss: 1.4445 - val_acc: 0.5104\n",
      "Epoch 707/1000\n",
      "1152/1152 [==============================] - 1s 480us/step - loss: 0.6048 - acc: 0.7925 - val_loss: 1.4809 - val_acc: 0.4965\n",
      "Epoch 708/1000\n",
      "1152/1152 [==============================] - 1s 551us/step - loss: 0.6167 - acc: 0.7899 - val_loss: 1.4494 - val_acc: 0.5104\n",
      "Epoch 709/1000\n",
      "1152/1152 [==============================] - 1s 545us/step - loss: 0.6127 - acc: 0.7908 - val_loss: 1.4433 - val_acc: 0.5069\n",
      "Epoch 710/1000\n",
      "1152/1152 [==============================] - 1s 526us/step - loss: 0.6299 - acc: 0.7812 - val_loss: 1.4318 - val_acc: 0.5243\n",
      "Epoch 711/1000\n",
      "1152/1152 [==============================] - 1s 526us/step - loss: 0.6040 - acc: 0.8003 - val_loss: 1.4024 - val_acc: 0.5243\n",
      "Epoch 712/1000\n",
      "1152/1152 [==============================] - 1s 481us/step - loss: 0.5861 - acc: 0.8047 - val_loss: 1.4247 - val_acc: 0.5174\n",
      "Epoch 713/1000\n",
      "1152/1152 [==============================] - 1s 525us/step - loss: 0.5767 - acc: 0.8238 - val_loss: 1.4107 - val_acc: 0.5174\n",
      "Epoch 714/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.6085 - acc: 0.7917 - val_loss: 1.4241 - val_acc: 0.5208\n",
      "Epoch 715/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.6000 - acc: 0.7986 - val_loss: 1.4185 - val_acc: 0.5278\n",
      "Epoch 716/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.5948 - acc: 0.7882 - val_loss: 1.4354 - val_acc: 0.5069\n",
      "Epoch 717/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.5828 - acc: 0.7917 - val_loss: 1.4365 - val_acc: 0.5312\n",
      "Epoch 718/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.6039 - acc: 0.7708 - val_loss: 1.4219 - val_acc: 0.5243\n",
      "Epoch 719/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5899 - acc: 0.8021 - val_loss: 1.4097 - val_acc: 0.5208\n",
      "Epoch 720/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5961 - acc: 0.7934 - val_loss: 1.4180 - val_acc: 0.5104\n",
      "Epoch 721/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.6027 - acc: 0.7960 - val_loss: 1.4482 - val_acc: 0.5069\n",
      "Epoch 722/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5815 - acc: 0.7917 - val_loss: 1.4608 - val_acc: 0.5278\n",
      "Epoch 723/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.5932 - acc: 0.8090 - val_loss: 1.4530 - val_acc: 0.5347\n",
      "Epoch 724/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.5965 - acc: 0.7943 - val_loss: 1.4346 - val_acc: 0.5139\n",
      "Epoch 725/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.5999 - acc: 0.7821 - val_loss: 1.4341 - val_acc: 0.4965\n",
      "Epoch 726/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.6007 - acc: 0.7995 - val_loss: 1.4548 - val_acc: 0.5174\n",
      "Epoch 727/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5985 - acc: 0.8021 - val_loss: 1.4482 - val_acc: 0.4965\n",
      "Epoch 728/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.5911 - acc: 0.8134 - val_loss: 1.4288 - val_acc: 0.5243\n",
      "Epoch 729/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.5996 - acc: 0.7969 - val_loss: 1.4345 - val_acc: 0.5139\n",
      "Epoch 730/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.6013 - acc: 0.7847 - val_loss: 1.4391 - val_acc: 0.4896\n",
      "Epoch 731/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5922 - acc: 0.8082 - val_loss: 1.4556 - val_acc: 0.5243\n",
      "Epoch 732/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.5874 - acc: 0.7865 - val_loss: 1.4641 - val_acc: 0.5208\n",
      "Epoch 733/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.5948 - acc: 0.8090 - val_loss: 1.4777 - val_acc: 0.4896\n",
      "Epoch 734/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5926 - acc: 0.7995 - val_loss: 1.4419 - val_acc: 0.5278\n",
      "Epoch 735/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.5833 - acc: 0.7986 - val_loss: 1.4472 - val_acc: 0.5000\n",
      "Epoch 736/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5809 - acc: 0.8047 - val_loss: 1.4286 - val_acc: 0.5069\n",
      "Epoch 737/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.5830 - acc: 0.8038 - val_loss: 1.4012 - val_acc: 0.5347\n",
      "Epoch 738/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.5884 - acc: 0.8030 - val_loss: 1.4383 - val_acc: 0.5139\n",
      "Epoch 739/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.5684 - acc: 0.8021 - val_loss: 1.4402 - val_acc: 0.5069\n",
      "Epoch 740/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5932 - acc: 0.7934 - val_loss: 1.4361 - val_acc: 0.5069\n",
      "Epoch 741/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.5955 - acc: 0.8021 - val_loss: 1.4202 - val_acc: 0.5312\n",
      "Epoch 742/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5871 - acc: 0.7891 - val_loss: 1.4236 - val_acc: 0.5035\n",
      "Epoch 743/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.5751 - acc: 0.7943 - val_loss: 1.4239 - val_acc: 0.5208\n",
      "Epoch 744/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5701 - acc: 0.8064 - val_loss: 1.4275 - val_acc: 0.5104\n",
      "Epoch 745/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.5862 - acc: 0.7943 - val_loss: 1.4130 - val_acc: 0.5243\n",
      "Epoch 746/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.5689 - acc: 0.8030 - val_loss: 1.4276 - val_acc: 0.5312\n",
      "Epoch 747/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.5874 - acc: 0.7995 - val_loss: 1.4416 - val_acc: 0.5312\n",
      "Epoch 748/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.5732 - acc: 0.8082 - val_loss: 1.4152 - val_acc: 0.5069\n",
      "Epoch 749/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5798 - acc: 0.8030 - val_loss: 1.4425 - val_acc: 0.5139\n",
      "Epoch 750/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.6042 - acc: 0.7839 - val_loss: 1.4249 - val_acc: 0.5243\n",
      "Epoch 751/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.5749 - acc: 0.8021 - val_loss: 1.4278 - val_acc: 0.5139\n",
      "Epoch 752/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5862 - acc: 0.7934 - val_loss: 1.4308 - val_acc: 0.5069\n",
      "Epoch 753/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5746 - acc: 0.8030 - val_loss: 1.4371 - val_acc: 0.5035\n",
      "Epoch 754/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.5828 - acc: 0.7925 - val_loss: 1.4161 - val_acc: 0.5243\n",
      "Epoch 755/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5719 - acc: 0.8108 - val_loss: 1.4139 - val_acc: 0.5278\n",
      "Epoch 756/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.5657 - acc: 0.8177 - val_loss: 1.4198 - val_acc: 0.5278\n",
      "Epoch 757/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.5553 - acc: 0.8038 - val_loss: 1.4414 - val_acc: 0.5174\n",
      "Epoch 758/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.5918 - acc: 0.7786 - val_loss: 1.4337 - val_acc: 0.5104\n",
      "Epoch 759/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5616 - acc: 0.8160 - val_loss: 1.4473 - val_acc: 0.5069\n",
      "Epoch 760/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.5743 - acc: 0.7908 - val_loss: 1.4340 - val_acc: 0.5347\n",
      "Epoch 761/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.5785 - acc: 0.8090 - val_loss: 1.4748 - val_acc: 0.5104\n",
      "Epoch 762/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.5815 - acc: 0.7977 - val_loss: 1.4481 - val_acc: 0.5243\n",
      "Epoch 763/1000\n",
      "1152/1152 [==============================] - 0s 404us/step - loss: 0.5590 - acc: 0.8064 - val_loss: 1.4449 - val_acc: 0.5174\n",
      "Epoch 764/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.5721 - acc: 0.7917 - val_loss: 1.4681 - val_acc: 0.5104\n",
      "Epoch 765/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 0.5488 - acc: 0.8125 - val_loss: 1.4580 - val_acc: 0.5243\n",
      "Epoch 766/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 0.5519 - acc: 0.8160 - val_loss: 1.4349 - val_acc: 0.5347\n",
      "Epoch 767/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.5726 - acc: 0.8090 - val_loss: 1.4657 - val_acc: 0.5104\n",
      "Epoch 768/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.5678 - acc: 0.8030 - val_loss: 1.4452 - val_acc: 0.5000\n",
      "Epoch 769/1000\n",
      "1152/1152 [==============================] - 0s 409us/step - loss: 0.5630 - acc: 0.7995 - val_loss: 1.4714 - val_acc: 0.5000\n",
      "Epoch 770/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.5640 - acc: 0.8030 - val_loss: 1.4357 - val_acc: 0.5243\n",
      "Epoch 771/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.5629 - acc: 0.8220 - val_loss: 1.4265 - val_acc: 0.5174\n",
      "Epoch 772/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.5758 - acc: 0.7969 - val_loss: 1.4733 - val_acc: 0.5208\n",
      "Epoch 773/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5527 - acc: 0.8090 - val_loss: 1.4322 - val_acc: 0.5243\n",
      "Epoch 774/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5607 - acc: 0.8116 - val_loss: 1.4759 - val_acc: 0.5069\n",
      "Epoch 775/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.5578 - acc: 0.8108 - val_loss: 1.4805 - val_acc: 0.5000\n",
      "Epoch 776/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.5774 - acc: 0.8021 - val_loss: 1.4283 - val_acc: 0.5069\n",
      "Epoch 777/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5656 - acc: 0.8212 - val_loss: 1.4362 - val_acc: 0.5243\n",
      "Epoch 778/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5793 - acc: 0.7995 - val_loss: 1.5029 - val_acc: 0.4618\n",
      "Epoch 779/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.5750 - acc: 0.8160 - val_loss: 1.4438 - val_acc: 0.5069\n",
      "Epoch 780/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.5599 - acc: 0.8116 - val_loss: 1.4596 - val_acc: 0.5174\n",
      "Epoch 781/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5872 - acc: 0.8021 - val_loss: 1.4373 - val_acc: 0.5278\n",
      "Epoch 782/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5425 - acc: 0.8125 - val_loss: 1.4854 - val_acc: 0.4792\n",
      "Epoch 783/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5631 - acc: 0.8108 - val_loss: 1.4289 - val_acc: 0.5278\n",
      "Epoch 784/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.5662 - acc: 0.8030 - val_loss: 1.4410 - val_acc: 0.5035\n",
      "Epoch 785/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.5839 - acc: 0.7995 - val_loss: 1.4332 - val_acc: 0.5069\n",
      "Epoch 786/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5476 - acc: 0.8073 - val_loss: 1.4464 - val_acc: 0.5208\n",
      "Epoch 787/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.5535 - acc: 0.8116 - val_loss: 1.4533 - val_acc: 0.4896\n",
      "Epoch 788/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5687 - acc: 0.8047 - val_loss: 1.4440 - val_acc: 0.5104\n",
      "Epoch 789/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5602 - acc: 0.8030 - val_loss: 1.4355 - val_acc: 0.5347\n",
      "Epoch 790/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5498 - acc: 0.8064 - val_loss: 1.4761 - val_acc: 0.5139\n",
      "Epoch 791/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5489 - acc: 0.8056 - val_loss: 1.4601 - val_acc: 0.5000\n",
      "Epoch 792/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.5667 - acc: 0.8003 - val_loss: 1.4334 - val_acc: 0.5174\n",
      "Epoch 793/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.5623 - acc: 0.8003 - val_loss: 1.4613 - val_acc: 0.5035\n",
      "Epoch 794/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5433 - acc: 0.8134 - val_loss: 1.4333 - val_acc: 0.5069\n",
      "Epoch 795/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5653 - acc: 0.7995 - val_loss: 1.4687 - val_acc: 0.5174\n",
      "Epoch 796/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5564 - acc: 0.8264 - val_loss: 1.4712 - val_acc: 0.4931\n",
      "Epoch 797/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5697 - acc: 0.8003 - val_loss: 1.4498 - val_acc: 0.5174\n",
      "Epoch 798/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.5592 - acc: 0.8125 - val_loss: 1.4469 - val_acc: 0.5104\n",
      "Epoch 799/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5586 - acc: 0.8090 - val_loss: 1.4612 - val_acc: 0.5139\n",
      "Epoch 800/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 0.5670 - acc: 0.7943 - val_loss: 1.4831 - val_acc: 0.5069\n",
      "Epoch 801/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 0.5409 - acc: 0.8134 - val_loss: 1.4632 - val_acc: 0.5139\n",
      "Epoch 802/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.5536 - acc: 0.8064 - val_loss: 1.4453 - val_acc: 0.5104\n",
      "Epoch 803/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.5404 - acc: 0.8056 - val_loss: 1.4437 - val_acc: 0.5243\n",
      "Epoch 804/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.5369 - acc: 0.8134 - val_loss: 1.4480 - val_acc: 0.5035\n",
      "Epoch 805/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.5534 - acc: 0.8160 - val_loss: 1.4293 - val_acc: 0.5174\n",
      "Epoch 806/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.5228 - acc: 0.8290 - val_loss: 1.4422 - val_acc: 0.5139\n",
      "Epoch 807/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5583 - acc: 0.7986 - val_loss: 1.4392 - val_acc: 0.5104\n",
      "Epoch 808/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.5390 - acc: 0.8212 - val_loss: 1.4460 - val_acc: 0.5139\n",
      "Epoch 809/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.5759 - acc: 0.8021 - val_loss: 1.4506 - val_acc: 0.5139\n",
      "Epoch 810/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5456 - acc: 0.8142 - val_loss: 1.4508 - val_acc: 0.5243\n",
      "Epoch 811/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5465 - acc: 0.8168 - val_loss: 1.4506 - val_acc: 0.5000\n",
      "Epoch 812/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.5496 - acc: 0.8108 - val_loss: 1.4479 - val_acc: 0.5104\n",
      "Epoch 813/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5521 - acc: 0.8030 - val_loss: 1.4236 - val_acc: 0.5278\n",
      "Epoch 814/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5479 - acc: 0.8108 - val_loss: 1.4444 - val_acc: 0.5104\n",
      "Epoch 815/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.5484 - acc: 0.8151 - val_loss: 1.4378 - val_acc: 0.5174\n",
      "Epoch 816/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5391 - acc: 0.8134 - val_loss: 1.4336 - val_acc: 0.5278\n",
      "Epoch 817/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5467 - acc: 0.8134 - val_loss: 1.4457 - val_acc: 0.5243\n",
      "Epoch 818/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5593 - acc: 0.8151 - val_loss: 1.4394 - val_acc: 0.5312\n",
      "Epoch 819/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5290 - acc: 0.8194 - val_loss: 1.4394 - val_acc: 0.5104\n",
      "Epoch 820/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5240 - acc: 0.8160 - val_loss: 1.4358 - val_acc: 0.5174\n",
      "Epoch 821/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.5185 - acc: 0.8229 - val_loss: 1.4744 - val_acc: 0.5035\n",
      "Epoch 822/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5412 - acc: 0.8281 - val_loss: 1.4850 - val_acc: 0.5000\n",
      "Epoch 823/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 0.5306 - acc: 0.8186 - val_loss: 1.4694 - val_acc: 0.5104\n",
      "Epoch 824/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.5128 - acc: 0.8351 - val_loss: 1.4413 - val_acc: 0.5243\n",
      "Epoch 825/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5560 - acc: 0.8160 - val_loss: 1.4372 - val_acc: 0.5174\n",
      "Epoch 826/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.5489 - acc: 0.8082 - val_loss: 1.4579 - val_acc: 0.5174\n",
      "Epoch 827/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5295 - acc: 0.8273 - val_loss: 1.4366 - val_acc: 0.5139\n",
      "Epoch 828/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.5560 - acc: 0.8142 - val_loss: 1.4371 - val_acc: 0.5417\n",
      "Epoch 829/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.5488 - acc: 0.8003 - val_loss: 1.4493 - val_acc: 0.5243\n",
      "Epoch 830/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.5149 - acc: 0.8220 - val_loss: 1.4243 - val_acc: 0.5278\n",
      "Epoch 831/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5433 - acc: 0.8142 - val_loss: 1.4680 - val_acc: 0.5417\n",
      "Epoch 832/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5194 - acc: 0.8186 - val_loss: 1.5146 - val_acc: 0.4965\n",
      "Epoch 833/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5520 - acc: 0.8186 - val_loss: 1.4329 - val_acc: 0.5278\n",
      "Epoch 834/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5301 - acc: 0.8125 - val_loss: 1.4432 - val_acc: 0.5208\n",
      "Epoch 835/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.5287 - acc: 0.8290 - val_loss: 1.4495 - val_acc: 0.5139\n",
      "Epoch 836/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5240 - acc: 0.8108 - val_loss: 1.4415 - val_acc: 0.5243\n",
      "Epoch 837/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.5143 - acc: 0.8220 - val_loss: 1.4445 - val_acc: 0.5278\n",
      "Epoch 838/1000\n",
      "1152/1152 [==============================] - 0s 401us/step - loss: 0.5360 - acc: 0.8273 - val_loss: 1.4613 - val_acc: 0.5382\n",
      "Epoch 839/1000\n",
      "1152/1152 [==============================] - 0s 394us/step - loss: 0.5170 - acc: 0.8203 - val_loss: 1.4313 - val_acc: 0.5312\n",
      "Epoch 840/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5316 - acc: 0.8203 - val_loss: 1.4264 - val_acc: 0.5278\n",
      "Epoch 841/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.5259 - acc: 0.8168 - val_loss: 1.4742 - val_acc: 0.5104\n",
      "Epoch 842/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5285 - acc: 0.8281 - val_loss: 1.4588 - val_acc: 0.5174\n",
      "Epoch 843/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5102 - acc: 0.8325 - val_loss: 1.4661 - val_acc: 0.5278\n",
      "Epoch 844/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5174 - acc: 0.8229 - val_loss: 1.4734 - val_acc: 0.4965\n",
      "Epoch 845/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.5379 - acc: 0.8082 - val_loss: 1.4800 - val_acc: 0.4757\n",
      "Epoch 846/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.5415 - acc: 0.8177 - val_loss: 1.4392 - val_acc: 0.5174\n",
      "Epoch 847/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.5311 - acc: 0.8177 - val_loss: 1.4314 - val_acc: 0.5208\n",
      "Epoch 848/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.5234 - acc: 0.8273 - val_loss: 1.4631 - val_acc: 0.5069\n",
      "Epoch 849/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5185 - acc: 0.8212 - val_loss: 1.4606 - val_acc: 0.4931\n",
      "Epoch 850/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5378 - acc: 0.8142 - val_loss: 1.4572 - val_acc: 0.5208\n",
      "Epoch 851/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5259 - acc: 0.8220 - val_loss: 1.4899 - val_acc: 0.5035\n",
      "Epoch 852/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.5226 - acc: 0.8229 - val_loss: 1.4638 - val_acc: 0.5139\n",
      "Epoch 853/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5400 - acc: 0.8238 - val_loss: 1.4379 - val_acc: 0.5312\n",
      "Epoch 854/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5324 - acc: 0.8247 - val_loss: 1.4730 - val_acc: 0.5069\n",
      "Epoch 855/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5267 - acc: 0.8264 - val_loss: 1.4593 - val_acc: 0.5035\n",
      "Epoch 856/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.5093 - acc: 0.8281 - val_loss: 1.4393 - val_acc: 0.5278\n",
      "Epoch 857/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.5221 - acc: 0.8247 - val_loss: 1.4362 - val_acc: 0.5208\n",
      "Epoch 858/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.5251 - acc: 0.8160 - val_loss: 1.4398 - val_acc: 0.5347\n",
      "Epoch 859/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5151 - acc: 0.8325 - val_loss: 1.4589 - val_acc: 0.5208\n",
      "Epoch 860/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5138 - acc: 0.8307 - val_loss: 1.4892 - val_acc: 0.4896\n",
      "Epoch 861/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5410 - acc: 0.8203 - val_loss: 1.4710 - val_acc: 0.5035\n",
      "Epoch 862/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5234 - acc: 0.8264 - val_loss: 1.4519 - val_acc: 0.5417\n",
      "Epoch 863/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5239 - acc: 0.8212 - val_loss: 1.4740 - val_acc: 0.5208\n",
      "Epoch 864/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5145 - acc: 0.8247 - val_loss: 1.4787 - val_acc: 0.5278\n",
      "Epoch 865/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5201 - acc: 0.8090 - val_loss: 1.4365 - val_acc: 0.5347\n",
      "Epoch 866/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.5149 - acc: 0.8255 - val_loss: 1.5041 - val_acc: 0.4861\n",
      "Epoch 867/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.5391 - acc: 0.8099 - val_loss: 1.4608 - val_acc: 0.5208\n",
      "Epoch 868/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.5297 - acc: 0.8099 - val_loss: 1.4480 - val_acc: 0.5278\n",
      "Epoch 869/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.5587 - acc: 0.7917 - val_loss: 1.4615 - val_acc: 0.5069\n",
      "Epoch 870/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5062 - acc: 0.8247 - val_loss: 1.4473 - val_acc: 0.5278\n",
      "Epoch 871/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.4993 - acc: 0.8385 - val_loss: 1.4676 - val_acc: 0.5347\n",
      "Epoch 872/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.5191 - acc: 0.8255 - val_loss: 1.4497 - val_acc: 0.5139\n",
      "Epoch 873/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5202 - acc: 0.8212 - val_loss: 1.4331 - val_acc: 0.5243\n",
      "Epoch 874/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5094 - acc: 0.8247 - val_loss: 1.4525 - val_acc: 0.5069\n",
      "Epoch 875/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5234 - acc: 0.8125 - val_loss: 1.4788 - val_acc: 0.5243\n",
      "Epoch 876/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 0.5291 - acc: 0.8194 - val_loss: 1.4547 - val_acc: 0.5104\n",
      "Epoch 877/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.5173 - acc: 0.8307 - val_loss: 1.4399 - val_acc: 0.5312\n",
      "Epoch 878/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.5098 - acc: 0.8368 - val_loss: 1.4585 - val_acc: 0.5139\n",
      "Epoch 879/1000\n",
      "1152/1152 [==============================] - 0s 404us/step - loss: 0.5191 - acc: 0.8203 - val_loss: 1.4450 - val_acc: 0.5312\n",
      "Epoch 880/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.5337 - acc: 0.8099 - val_loss: 1.4956 - val_acc: 0.5000\n",
      "Epoch 881/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.5072 - acc: 0.8194 - val_loss: 1.4694 - val_acc: 0.5104\n",
      "Epoch 882/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5067 - acc: 0.8290 - val_loss: 1.4516 - val_acc: 0.5174\n",
      "Epoch 883/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.5352 - acc: 0.8203 - val_loss: 1.4309 - val_acc: 0.5278\n",
      "Epoch 884/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5362 - acc: 0.8168 - val_loss: 1.4701 - val_acc: 0.5243\n",
      "Epoch 885/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.5061 - acc: 0.8186 - val_loss: 1.4829 - val_acc: 0.5312\n",
      "Epoch 886/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5130 - acc: 0.8168 - val_loss: 1.4606 - val_acc: 0.5174\n",
      "Epoch 887/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5233 - acc: 0.8186 - val_loss: 1.4506 - val_acc: 0.5104\n",
      "Epoch 888/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5106 - acc: 0.8212 - val_loss: 1.4501 - val_acc: 0.5208\n",
      "Epoch 889/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5144 - acc: 0.8220 - val_loss: 1.5029 - val_acc: 0.5069\n",
      "Epoch 890/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.5035 - acc: 0.8299 - val_loss: 1.4374 - val_acc: 0.5312\n",
      "Epoch 891/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.4891 - acc: 0.8307 - val_loss: 1.5102 - val_acc: 0.5069\n",
      "Epoch 892/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5231 - acc: 0.8116 - val_loss: 1.4898 - val_acc: 0.5069\n",
      "Epoch 893/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5038 - acc: 0.8359 - val_loss: 1.4475 - val_acc: 0.5278\n",
      "Epoch 894/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5103 - acc: 0.8299 - val_loss: 1.4565 - val_acc: 0.5278\n",
      "Epoch 895/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.5219 - acc: 0.8220 - val_loss: 1.4729 - val_acc: 0.5208\n",
      "Epoch 896/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5276 - acc: 0.8125 - val_loss: 1.4582 - val_acc: 0.5035\n",
      "Epoch 897/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5168 - acc: 0.8160 - val_loss: 1.4314 - val_acc: 0.5104\n",
      "Epoch 898/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5022 - acc: 0.8238 - val_loss: 1.4548 - val_acc: 0.5347\n",
      "Epoch 899/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.5030 - acc: 0.8307 - val_loss: 1.4647 - val_acc: 0.5312\n",
      "Epoch 900/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5232 - acc: 0.8151 - val_loss: 1.4723 - val_acc: 0.5486\n",
      "Epoch 901/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.4982 - acc: 0.8290 - val_loss: 1.4515 - val_acc: 0.5278\n",
      "Epoch 902/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5084 - acc: 0.8212 - val_loss: 1.5104 - val_acc: 0.5000\n",
      "Epoch 903/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.5145 - acc: 0.8238 - val_loss: 1.4558 - val_acc: 0.5243\n",
      "Epoch 904/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.5111 - acc: 0.8238 - val_loss: 1.4541 - val_acc: 0.5174\n",
      "Epoch 905/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.4908 - acc: 0.8333 - val_loss: 1.4290 - val_acc: 0.5278\n",
      "Epoch 906/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5256 - acc: 0.8125 - val_loss: 1.4681 - val_acc: 0.5243\n",
      "Epoch 907/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.4950 - acc: 0.8229 - val_loss: 1.4633 - val_acc: 0.5278\n",
      "Epoch 908/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.4993 - acc: 0.8247 - val_loss: 1.4696 - val_acc: 0.5347\n",
      "Epoch 909/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.5124 - acc: 0.8168 - val_loss: 1.4575 - val_acc: 0.5278\n",
      "Epoch 910/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.5136 - acc: 0.8212 - val_loss: 1.4438 - val_acc: 0.5312\n",
      "Epoch 911/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.4677 - acc: 0.8438 - val_loss: 1.4598 - val_acc: 0.5243\n",
      "Epoch 912/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4918 - acc: 0.8281 - val_loss: 1.4406 - val_acc: 0.5208\n",
      "Epoch 913/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.5044 - acc: 0.8229 - val_loss: 1.4539 - val_acc: 0.5347\n",
      "Epoch 914/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.4911 - acc: 0.8316 - val_loss: 1.4672 - val_acc: 0.5139\n",
      "Epoch 915/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.5052 - acc: 0.8125 - val_loss: 1.4509 - val_acc: 0.5243\n",
      "Epoch 916/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.5036 - acc: 0.8351 - val_loss: 1.4698 - val_acc: 0.5174\n",
      "Epoch 917/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 0.4708 - acc: 0.8307 - val_loss: 1.4370 - val_acc: 0.5347\n",
      "Epoch 918/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.4837 - acc: 0.8325 - val_loss: 1.4540 - val_acc: 0.5243\n",
      "Epoch 919/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.4929 - acc: 0.8351 - val_loss: 1.4526 - val_acc: 0.5104\n",
      "Epoch 920/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.5133 - acc: 0.8299 - val_loss: 1.4609 - val_acc: 0.5208\n",
      "Epoch 921/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.4988 - acc: 0.8325 - val_loss: 1.4491 - val_acc: 0.5069\n",
      "Epoch 922/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.4834 - acc: 0.8394 - val_loss: 1.4663 - val_acc: 0.5243\n",
      "Epoch 923/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.4859 - acc: 0.8299 - val_loss: 1.4457 - val_acc: 0.5347\n",
      "Epoch 924/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5080 - acc: 0.8220 - val_loss: 1.4553 - val_acc: 0.5243\n",
      "Epoch 925/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.4795 - acc: 0.8455 - val_loss: 1.4470 - val_acc: 0.5347\n",
      "Epoch 926/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.5202 - acc: 0.8160 - val_loss: 1.4708 - val_acc: 0.5451\n",
      "Epoch 927/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.4691 - acc: 0.8385 - val_loss: 1.4497 - val_acc: 0.5312\n",
      "Epoch 928/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.5051 - acc: 0.8307 - val_loss: 1.4681 - val_acc: 0.5278\n",
      "Epoch 929/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.4867 - acc: 0.8411 - val_loss: 1.4714 - val_acc: 0.5035\n",
      "Epoch 930/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 0.5016 - acc: 0.8325 - val_loss: 1.4739 - val_acc: 0.5139\n",
      "Epoch 931/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.4840 - acc: 0.8368 - val_loss: 1.4988 - val_acc: 0.5139\n",
      "Epoch 932/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.5140 - acc: 0.8247 - val_loss: 1.4672 - val_acc: 0.4931\n",
      "Epoch 933/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.4802 - acc: 0.8455 - val_loss: 1.4868 - val_acc: 0.5104\n",
      "Epoch 934/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.4642 - acc: 0.8446 - val_loss: 1.4926 - val_acc: 0.5000\n",
      "Epoch 935/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.4840 - acc: 0.8342 - val_loss: 1.4785 - val_acc: 0.5104\n",
      "Epoch 936/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.4951 - acc: 0.8325 - val_loss: 1.4470 - val_acc: 0.5312\n",
      "Epoch 937/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.4687 - acc: 0.8446 - val_loss: 1.4469 - val_acc: 0.5312\n",
      "Epoch 938/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 0.4680 - acc: 0.8498 - val_loss: 1.4815 - val_acc: 0.5035\n",
      "Epoch 939/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.4865 - acc: 0.8351 - val_loss: 1.4666 - val_acc: 0.5035\n",
      "Epoch 940/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.4944 - acc: 0.8229 - val_loss: 1.4654 - val_acc: 0.5382\n",
      "Epoch 941/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5037 - acc: 0.8160 - val_loss: 1.4623 - val_acc: 0.5347\n",
      "Epoch 942/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.4763 - acc: 0.8333 - val_loss: 1.4844 - val_acc: 0.5243\n",
      "Epoch 943/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.4817 - acc: 0.8333 - val_loss: 1.4452 - val_acc: 0.5451\n",
      "Epoch 944/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.4989 - acc: 0.8264 - val_loss: 1.4763 - val_acc: 0.5208\n",
      "Epoch 945/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.5054 - acc: 0.8299 - val_loss: 1.5103 - val_acc: 0.5243\n",
      "Epoch 946/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.4934 - acc: 0.8342 - val_loss: 1.4728 - val_acc: 0.5417\n",
      "Epoch 947/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5216 - acc: 0.8151 - val_loss: 1.4819 - val_acc: 0.5139\n",
      "Epoch 948/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.4793 - acc: 0.8377 - val_loss: 1.4855 - val_acc: 0.5347\n",
      "Epoch 949/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5048 - acc: 0.8203 - val_loss: 1.4623 - val_acc: 0.5243\n",
      "Epoch 950/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.4900 - acc: 0.8264 - val_loss: 1.4453 - val_acc: 0.5139\n",
      "Epoch 951/1000\n",
      "1152/1152 [==============================] - 0s 409us/step - loss: 0.4811 - acc: 0.8307 - val_loss: 1.4644 - val_acc: 0.5278\n",
      "Epoch 952/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 0.4681 - acc: 0.8472 - val_loss: 1.4822 - val_acc: 0.5208\n",
      "Epoch 953/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.4873 - acc: 0.8325 - val_loss: 1.4605 - val_acc: 0.5312\n",
      "Epoch 954/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.5120 - acc: 0.8238 - val_loss: 1.4820 - val_acc: 0.5174\n",
      "Epoch 955/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.4746 - acc: 0.8403 - val_loss: 1.4615 - val_acc: 0.5035\n",
      "Epoch 956/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.4783 - acc: 0.8394 - val_loss: 1.4395 - val_acc: 0.5312\n",
      "Epoch 957/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.4924 - acc: 0.8411 - val_loss: 1.4791 - val_acc: 0.5104\n",
      "Epoch 958/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4869 - acc: 0.8385 - val_loss: 1.4462 - val_acc: 0.5174\n",
      "Epoch 959/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.4636 - acc: 0.8385 - val_loss: 1.4463 - val_acc: 0.5174\n",
      "Epoch 960/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.4710 - acc: 0.8498 - val_loss: 1.4526 - val_acc: 0.5139\n",
      "Epoch 961/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.4601 - acc: 0.8455 - val_loss: 1.4431 - val_acc: 0.5139\n",
      "Epoch 962/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.4843 - acc: 0.8377 - val_loss: 1.4870 - val_acc: 0.5139\n",
      "Epoch 963/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.4556 - acc: 0.8490 - val_loss: 1.4526 - val_acc: 0.5174\n",
      "Epoch 964/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.4915 - acc: 0.8229 - val_loss: 1.4674 - val_acc: 0.5417\n",
      "Epoch 965/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.4822 - acc: 0.8359 - val_loss: 1.4689 - val_acc: 0.5174\n",
      "Epoch 966/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.4920 - acc: 0.8264 - val_loss: 1.4659 - val_acc: 0.5208\n",
      "Epoch 967/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.4855 - acc: 0.8359 - val_loss: 1.4536 - val_acc: 0.5208\n",
      "Epoch 968/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.5092 - acc: 0.8299 - val_loss: 1.4558 - val_acc: 0.5312\n",
      "Epoch 969/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.4904 - acc: 0.8333 - val_loss: 1.4884 - val_acc: 0.5208\n",
      "Epoch 970/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.4698 - acc: 0.8368 - val_loss: 1.4886 - val_acc: 0.5347\n",
      "Epoch 971/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.4974 - acc: 0.8264 - val_loss: 1.4819 - val_acc: 0.5174\n",
      "Epoch 972/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.4937 - acc: 0.8394 - val_loss: 1.4849 - val_acc: 0.5139\n",
      "Epoch 973/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.4811 - acc: 0.8420 - val_loss: 1.4601 - val_acc: 0.5347\n",
      "Epoch 974/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.4789 - acc: 0.8429 - val_loss: 1.4442 - val_acc: 0.5312\n",
      "Epoch 975/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4902 - acc: 0.8385 - val_loss: 1.4519 - val_acc: 0.5312\n",
      "Epoch 976/1000\n",
      "1152/1152 [==============================] - 0s 385us/step - loss: 0.4664 - acc: 0.8446 - val_loss: 1.4608 - val_acc: 0.5278\n",
      "Epoch 977/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.4612 - acc: 0.8411 - val_loss: 1.4505 - val_acc: 0.5278\n",
      "Epoch 978/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.4696 - acc: 0.8333 - val_loss: 1.4403 - val_acc: 0.5382\n",
      "Epoch 979/1000\n",
      "1152/1152 [==============================] - 0s 383us/step - loss: 0.4937 - acc: 0.8351 - val_loss: 1.4535 - val_acc: 0.5278\n",
      "Epoch 980/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.4391 - acc: 0.8507 - val_loss: 1.4810 - val_acc: 0.5278\n",
      "Epoch 981/1000\n",
      "1152/1152 [==============================] - 0s 393us/step - loss: 0.4919 - acc: 0.8342 - val_loss: 1.4555 - val_acc: 0.5243\n",
      "Epoch 982/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.4681 - acc: 0.8455 - val_loss: 1.4626 - val_acc: 0.5278\n",
      "Epoch 983/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 0.4788 - acc: 0.8325 - val_loss: 1.4539 - val_acc: 0.5312\n",
      "Epoch 984/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.4598 - acc: 0.8307 - val_loss: 1.4728 - val_acc: 0.5104\n",
      "Epoch 985/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 0.4597 - acc: 0.8472 - val_loss: 1.4749 - val_acc: 0.5069\n",
      "Epoch 986/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 0.4939 - acc: 0.8368 - val_loss: 1.4706 - val_acc: 0.5139\n",
      "Epoch 987/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.4591 - acc: 0.8411 - val_loss: 1.4503 - val_acc: 0.5278\n",
      "Epoch 988/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.4607 - acc: 0.8299 - val_loss: 1.4635 - val_acc: 0.5208\n",
      "Epoch 989/1000\n",
      "1152/1152 [==============================] - 0s 422us/step - loss: 0.4375 - acc: 0.8576 - val_loss: 1.4998 - val_acc: 0.5417\n",
      "Epoch 990/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.4719 - acc: 0.8325 - val_loss: 1.4401 - val_acc: 0.5417\n",
      "Epoch 991/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.4703 - acc: 0.8333 - val_loss: 1.4692 - val_acc: 0.5382\n",
      "Epoch 992/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.4608 - acc: 0.8429 - val_loss: 1.4539 - val_acc: 0.5312\n",
      "Epoch 993/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.4634 - acc: 0.8411 - val_loss: 1.4689 - val_acc: 0.5347\n",
      "Epoch 994/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.4746 - acc: 0.8290 - val_loss: 1.4577 - val_acc: 0.5382\n",
      "Epoch 995/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.4720 - acc: 0.8290 - val_loss: 1.4573 - val_acc: 0.5451\n",
      "Epoch 996/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.4650 - acc: 0.8394 - val_loss: 1.4696 - val_acc: 0.5278\n",
      "Epoch 997/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.4527 - acc: 0.8490 - val_loss: 1.4628 - val_acc: 0.5451\n",
      "Epoch 998/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4716 - acc: 0.8377 - val_loss: 1.4693 - val_acc: 0.5382\n",
      "Epoch 999/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.4642 - acc: 0.8342 - val_loss: 1.4670 - val_acc: 0.5208\n",
      "Epoch 1000/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4410 - acc: 0.8333 - val_loss: 1.4546 - val_acc: 0.5312\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 5, 1, 6, 1, 1, 2, 6, 1, 2, 7, 3, 3, 6, 4, 7, 7, 1, 2, 4, 3,\n",
       "       0, 1, 3, 6, 4, 7, 3, 3, 2, 4, 5, 1, 1, 1, 7, 4, 5, 7, 3, 1, 1, 4,\n",
       "       3, 1, 0, 3, 5, 4, 2, 5, 7, 4, 0, 3, 7, 7, 6, 3, 7, 5, 7, 4, 5, 4,\n",
       "       3, 6, 7, 3, 7, 7, 1, 2, 6, 1, 4, 7, 3, 3, 2, 2, 2, 1, 2, 4, 5, 1,\n",
       "       4, 1, 0, 1, 4, 6, 6, 6, 6, 4, 7, 7, 6, 5, 4, 7, 0, 5, 1, 6, 4, 1,\n",
       "       7, 4, 7, 1, 6, 4, 5, 1, 2, 5, 4, 1, 7, 2, 3, 4, 6, 0, 2, 5, 2, 2,\n",
       "       1, 5, 4, 7, 3, 3, 4, 6, 4, 6, 2, 7, 4, 6, 4, 1, 6, 3, 5, 3, 1, 5,\n",
       "       5, 1, 7, 1, 7, 5, 1, 6, 4, 1, 1, 0, 2, 6, 3, 7, 1, 4, 4, 6, 4, 1,\n",
       "       7, 7, 2, 6, 2, 6, 2, 1, 0, 3, 7, 4, 2, 3, 3, 5, 3, 6, 3, 3, 7, 7,\n",
       "       6, 6, 2, 3, 7, 3, 1, 4, 6, 1, 2, 7, 7, 2, 3, 5, 1, 5, 4, 4, 4, 7,\n",
       "       2, 2, 6, 3, 3, 6, 3, 6, 1, 0, 3, 7, 3, 7, 7, 6, 7, 6, 4, 0, 7, 1,\n",
       "       7, 3, 2, 7, 0, 1, 1, 3, 7, 3, 2, 1, 4, 6, 3, 7, 6, 1, 1, 3, 1, 7,\n",
       "       6, 6, 4, 7, 7, 1, 7, 0, 3, 6, 6, 4, 6, 7, 4, 4, 4, 2, 3, 5, 3, 4,\n",
       "       5, 3], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 1, 1, 4, 1, 1, 5, 7, 1, 6, 7, 3, 5, 3, 6, 4, 5, 1, 6, 7, 0,\n",
       "       0, 3, 5, 3, 7, 7, 6, 4, 7, 4, 2, 5, 7, 1, 6, 4, 2, 4, 1, 0, 1, 4,\n",
       "       7, 1, 1, 0, 5, 3, 3, 7, 7, 4, 0, 1, 1, 7, 2, 0, 7, 5, 7, 7, 5, 2,\n",
       "       1, 6, 6, 3, 2, 7, 1, 2, 7, 1, 4, 7, 3, 6, 2, 2, 7, 1, 2, 4, 5, 1,\n",
       "       2, 1, 0, 1, 4, 4, 0, 6, 7, 2, 2, 7, 6, 5, 4, 5, 3, 7, 2, 6, 5, 0,\n",
       "       7, 4, 7, 1, 6, 4, 5, 6, 6, 5, 6, 1, 7, 5, 3, 4, 7, 2, 5, 5, 2, 2,\n",
       "       3, 5, 2, 3, 1, 1, 6, 6, 4, 4, 2, 2, 4, 2, 4, 1, 7, 5, 5, 3, 1, 5,\n",
       "       2, 0, 5, 3, 7, 5, 3, 4, 4, 3, 2, 0, 4, 1, 3, 7, 1, 5, 4, 6, 5, 1,\n",
       "       4, 7, 2, 3, 2, 5, 2, 3, 0, 0, 7, 4, 4, 3, 6, 5, 3, 1, 3, 5, 7, 7,\n",
       "       6, 1, 5, 3, 7, 3, 1, 4, 6, 3, 2, 7, 2, 6, 7, 5, 0, 5, 4, 4, 7, 7,\n",
       "       3, 7, 6, 5, 3, 6, 1, 3, 3, 0, 3, 7, 3, 6, 7, 4, 4, 6, 4, 0, 6, 1,\n",
       "       7, 0, 2, 2, 0, 1, 1, 3, 4, 3, 4, 1, 4, 6, 3, 7, 1, 1, 4, 1, 1, 1,\n",
       "       4, 0, 6, 7, 7, 1, 2, 0, 3, 7, 3, 4, 4, 2, 4, 2, 4, 2, 3, 6, 3, 6,\n",
       "       2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 1, 1, 4, 1, 1, 5, 7, 1, 6, 7, 3, 5, 3, 6, 4, 5, 1, 6, 7, 0,\n",
       "       0, 3, 5, 3, 7, 7, 6, 4, 7, 4, 2, 5, 7, 1, 6, 4, 2, 4, 1, 0, 1, 4,\n",
       "       7, 1, 1, 0, 5, 3, 3, 7, 7, 4, 0, 1, 1, 7, 2, 0, 7, 5, 7, 7, 5, 2,\n",
       "       1, 6, 6, 3, 2, 7, 1, 2, 7, 1, 4, 7, 3, 6, 2, 2, 7, 1, 2, 4, 5, 1,\n",
       "       2, 1, 0, 1, 4, 4, 0, 6, 7, 2, 2, 7, 6, 5, 4, 5, 3, 7, 2, 6, 5, 0,\n",
       "       7, 4, 7, 1, 6, 4, 5, 6, 6, 5, 6, 1, 7, 5, 3, 4, 7, 2, 5, 5, 2, 2,\n",
       "       3, 5, 2, 3, 1, 1, 6, 6, 4, 4, 2, 2, 4, 2, 4, 1, 7, 5, 5, 3, 1, 5,\n",
       "       2, 0, 5, 3, 7, 5, 3, 4, 4, 3, 2, 0, 4, 1, 3, 7, 1, 5, 4, 6, 5, 1,\n",
       "       4, 7, 2, 3, 2, 5, 2, 3, 0, 0, 7, 4, 4, 3, 6, 5, 3, 1, 3, 5, 7, 7,\n",
       "       6, 1, 5, 3, 7, 3, 1, 4, 6, 3, 2, 7, 2, 6, 7, 5, 0, 5, 4, 4, 7, 7,\n",
       "       3, 7, 6, 5, 3, 6, 1, 3, 3, 0, 3, 7, 3, 6, 7, 4, 4, 6, 4, 0, 6, 1,\n",
       "       7, 0, 2, 2, 0, 1, 1, 3, 4, 3, 4, 1, 4, 6, 3, 7, 1, 1, 4, 1, 1, 1,\n",
       "       4, 0, 6, 7, 7, 1, 2, 0, 3, 7, 3, 4, 4, 2, 4, 2, 4, 2, 3, 6, 3, 6,\n",
       "       2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.45      0.56        20\n",
      "           1       0.62      0.66      0.64        44\n",
      "           2       0.45      0.38      0.41        34\n",
      "           3       0.48      0.54      0.51        39\n",
      "           4       0.58      0.60      0.59        42\n",
      "           5       0.65      0.47      0.55        32\n",
      "           6       0.34      0.44      0.38        32\n",
      "           7       0.55      0.60      0.57        45\n",
      "\n",
      "    accuracy                           0.53       288\n",
      "   macro avg       0.55      0.52      0.53       288\n",
      "weighted avg       0.55      0.53      0.53       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  4  0  5  0  0  2  0]\n",
      " [ 1 29  0  7  0  1  4  2]\n",
      " [ 1  2 13  0  5  4  2  7]\n",
      " [ 1  8  2 21  1  0  5  1]\n",
      " [ 0  1  3  1 25  0  7  5]\n",
      " [ 0  1  4  5  3 15  1  3]\n",
      " [ 0  1  4  3  5  1 14  4]\n",
      " [ 0  1  3  2  4  2  6 27]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:/another_model/Emotion_Voice_model_tanh.h5 \n"
     ]
    }
   ],
   "source": [
    "#Save the model---/\n",
    "model_name = 'Emotion_Voice_model_tanh.h5'\n",
    "save_dir = 'C:/another_model/'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('C:/another_model/Emotion_Voice_model_tanh.h5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 0s 519us/step\n",
      "Restored model, accuracy: 53.12%\n"
     ]
    }
   ],
   "source": [
    "oss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
