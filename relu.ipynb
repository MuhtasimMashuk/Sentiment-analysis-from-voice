{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('C:/voice/Actor_01/03-01-01-01-01-01-01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1ed7218e348>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded. Loading time: 85.84619164466858 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "path = 'C:/voice/'\n",
    "lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "  for file in files:\n",
    "      try:\n",
    "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
    "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
    "        file = int(file[7:8]) - 1 \n",
    "        arr = mfccs, file\n",
    "        lst.append(arr)\n",
    "      # If the file is not valid, skip it\n",
    "      except ValueError:\n",
    "        continue\n",
    "\n",
    "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1440, 40), (1440,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = 'C:/another_model/'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('C:/another_model/X.joblib')\n",
    "y = joblib.load('C:/another_model/y.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1152 samples, validate on 288 samples\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1152/1152 [==============================] - 1s 953us/step - loss: 9.3874 - acc: 0.1502 - val_loss: 7.2822 - val_acc: 0.1736\n",
      "Epoch 2/1000\n",
      "1152/1152 [==============================] - 1s 435us/step - loss: 9.0942 - acc: 0.1424 - val_loss: 7.3296 - val_acc: 0.1493\n",
      "Epoch 3/1000\n",
      "1152/1152 [==============================] - 0s 405us/step - loss: 8.3651 - acc: 0.1450 - val_loss: 5.5413 - val_acc: 0.1319\n",
      "Epoch 4/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 7.7226 - acc: 0.1493 - val_loss: 5.1241 - val_acc: 0.2049\n",
      "Epoch 5/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 7.2191 - acc: 0.1432 - val_loss: 3.2071 - val_acc: 0.1944\n",
      "Epoch 6/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 6.0415 - acc: 0.1441 - val_loss: 3.0445 - val_acc: 0.1944\n",
      "Epoch 7/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 5.5537 - acc: 0.1701 - val_loss: 2.5294 - val_acc: 0.2014\n",
      "Epoch 8/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 4.8522 - acc: 0.1458 - val_loss: 2.5070 - val_acc: 0.1667\n",
      "Epoch 9/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 4.5430 - acc: 0.1493 - val_loss: 2.3616 - val_acc: 0.1736\n",
      "Epoch 10/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 4.1273 - acc: 0.1658 - val_loss: 2.2309 - val_acc: 0.2500\n",
      "Epoch 11/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 3.9061 - acc: 0.1615 - val_loss: 2.0525 - val_acc: 0.2674\n",
      "Epoch 12/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 3.4671 - acc: 0.1736 - val_loss: 1.9990 - val_acc: 0.2604\n",
      "Epoch 13/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 3.0947 - acc: 0.1962 - val_loss: 1.9526 - val_acc: 0.2708\n",
      "Epoch 14/1000\n",
      "1152/1152 [==============================] - 0s 408us/step - loss: 2.9362 - acc: 0.2049 - val_loss: 1.8857 - val_acc: 0.2917\n",
      "Epoch 15/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 2.7563 - acc: 0.1858 - val_loss: 1.9097 - val_acc: 0.2882\n",
      "Epoch 16/1000\n",
      "1152/1152 [==============================] - 0s 400us/step - loss: 2.5539 - acc: 0.1953 - val_loss: 1.9416 - val_acc: 0.2326\n",
      "Epoch 17/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 2.3875 - acc: 0.2127 - val_loss: 1.9011 - val_acc: 0.2986\n",
      "Epoch 18/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 2.3857 - acc: 0.1936 - val_loss: 1.9085 - val_acc: 0.2014\n",
      "Epoch 19/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 2.2783 - acc: 0.2153 - val_loss: 1.9357 - val_acc: 0.2049\n",
      "Epoch 20/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 2.0925 - acc: 0.2240 - val_loss: 1.8936 - val_acc: 0.2917\n",
      "Epoch 21/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 2.0659 - acc: 0.2344 - val_loss: 1.8743 - val_acc: 0.3021\n",
      "Epoch 22/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 2.0498 - acc: 0.2214 - val_loss: 1.8690 - val_acc: 0.3194\n",
      "Epoch 23/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 2.0592 - acc: 0.2144 - val_loss: 1.8976 - val_acc: 0.2188\n",
      "Epoch 24/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 2.0107 - acc: 0.2318 - val_loss: 1.8743 - val_acc: 0.3090\n",
      "Epoch 25/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 1.9746 - acc: 0.2474 - val_loss: 1.8573 - val_acc: 0.3090\n",
      "Epoch 26/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 1.9659 - acc: 0.2413 - val_loss: 1.8783 - val_acc: 0.2708\n",
      "Epoch 27/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 1.9831 - acc: 0.2387 - val_loss: 1.8791 - val_acc: 0.2917\n",
      "Epoch 28/1000\n",
      "1152/1152 [==============================] - 0s 426us/step - loss: 1.9475 - acc: 0.2526 - val_loss: 1.8857 - val_acc: 0.2708\n",
      "Epoch 29/1000\n",
      "1152/1152 [==============================] - 1s 509us/step - loss: 1.9114 - acc: 0.2700 - val_loss: 1.8558 - val_acc: 0.2917\n",
      "Epoch 30/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 1.9357 - acc: 0.2648 - val_loss: 1.8461 - val_acc: 0.3125\n",
      "Epoch 31/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 1.9244 - acc: 0.2587 - val_loss: 1.8204 - val_acc: 0.2917\n",
      "Epoch 32/1000\n",
      "1152/1152 [==============================] - 1s 498us/step - loss: 1.8902 - acc: 0.2717 - val_loss: 1.8421 - val_acc: 0.3194\n",
      "Epoch 33/1000\n",
      "1152/1152 [==============================] - 0s 407us/step - loss: 1.9008 - acc: 0.2622 - val_loss: 1.8194 - val_acc: 0.3125\n",
      "Epoch 34/1000\n",
      "1152/1152 [==============================] - 0s 415us/step - loss: 1.9087 - acc: 0.2812 - val_loss: 1.8142 - val_acc: 0.3125\n",
      "Epoch 35/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.8805 - acc: 0.2734 - val_loss: 1.8246 - val_acc: 0.3056\n",
      "Epoch 36/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 1.8751 - acc: 0.2682 - val_loss: 1.8279 - val_acc: 0.2847\n",
      "Epoch 37/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 1.8705 - acc: 0.2760 - val_loss: 1.8438 - val_acc: 0.3125\n",
      "Epoch 38/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 1.8791 - acc: 0.2734 - val_loss: 1.8339 - val_acc: 0.2812\n",
      "Epoch 39/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.8528 - acc: 0.2734 - val_loss: 1.8045 - val_acc: 0.3125\n",
      "Epoch 40/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 1.8590 - acc: 0.2760 - val_loss: 1.8207 - val_acc: 0.3229\n",
      "Epoch 41/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 1.8486 - acc: 0.3047 - val_loss: 1.7980 - val_acc: 0.3507\n",
      "Epoch 42/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 1.8095 - acc: 0.3108 - val_loss: 1.7776 - val_acc: 0.3403\n",
      "Epoch 43/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 1.8078 - acc: 0.3108 - val_loss: 1.7804 - val_acc: 0.3403\n",
      "Epoch 44/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.8054 - acc: 0.307 - 0s 365us/step - loss: 1.8070 - acc: 0.3030 - val_loss: 1.7941 - val_acc: 0.3333\n",
      "Epoch 45/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.8083 - acc: 0.3125 - val_loss: 1.7839 - val_acc: 0.3264\n",
      "Epoch 46/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 1.8142 - acc: 0.3125 - val_loss: 1.7890 - val_acc: 0.3194\n",
      "Epoch 47/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 1.7770 - acc: 0.3247 - val_loss: 1.7700 - val_acc: 0.3472\n",
      "Epoch 48/1000\n",
      "1152/1152 [==============================] - 0s 307us/step - loss: 1.8027 - acc: 0.3212 - val_loss: 1.7746 - val_acc: 0.3194\n",
      "Epoch 49/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 1.7890 - acc: 0.3377 - val_loss: 1.7752 - val_acc: 0.3299\n",
      "Epoch 50/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.7799 - acc: 0.3368 - val_loss: 1.7811 - val_acc: 0.3333\n",
      "Epoch 51/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 1.7750 - acc: 0.3177 - val_loss: 1.7795 - val_acc: 0.3264\n",
      "Epoch 52/1000\n",
      "1152/1152 [==============================] - 0s 301us/step - loss: 1.7731 - acc: 0.3281 - val_loss: 1.7526 - val_acc: 0.3438\n",
      "Epoch 53/1000\n",
      "1152/1152 [==============================] - 0s 305us/step - loss: 1.7786 - acc: 0.3168 - val_loss: 1.7619 - val_acc: 0.3542\n",
      "Epoch 54/1000\n",
      "1152/1152 [==============================] - 0s 301us/step - loss: 1.7537 - acc: 0.3394 - val_loss: 1.7566 - val_acc: 0.3438\n",
      "Epoch 55/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.7457 - acc: 0.3307 - val_loss: 1.7492 - val_acc: 0.3438\n",
      "Epoch 56/1000\n",
      "1152/1152 [==============================] - 0s 305us/step - loss: 1.7479 - acc: 0.3481 - val_loss: 1.7577 - val_acc: 0.3021\n",
      "Epoch 57/1000\n",
      "1152/1152 [==============================] - 0s 428us/step - loss: 1.7518 - acc: 0.3385 - val_loss: 1.7440 - val_acc: 0.3611\n",
      "Epoch 58/1000\n",
      "1152/1152 [==============================] - 0s 399us/step - loss: 1.7309 - acc: 0.3455 - val_loss: 1.7361 - val_acc: 0.3611\n",
      "Epoch 59/1000\n",
      "1152/1152 [==============================] - 1s 540us/step - loss: 1.7473 - acc: 0.3351 - val_loss: 1.7168 - val_acc: 0.3507\n",
      "Epoch 60/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 1.7286 - acc: 0.3385 - val_loss: 1.7263 - val_acc: 0.3403\n",
      "Epoch 61/1000\n",
      "1152/1152 [==============================] - 1s 442us/step - loss: 1.7381 - acc: 0.3420 - val_loss: 1.7565 - val_acc: 0.3403\n",
      "Epoch 62/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.7045 - acc: 0.3568 - val_loss: 1.7086 - val_acc: 0.3368\n",
      "Epoch 63/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.7140 - acc: 0.359 - 0s 313us/step - loss: 1.7095 - acc: 0.3637 - val_loss: 1.7008 - val_acc: 0.3854\n",
      "Epoch 64/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 1.7143 - acc: 0.3611 - val_loss: 1.7351 - val_acc: 0.3264\n",
      "Epoch 65/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 1.7120 - acc: 0.3646 - val_loss: 1.6902 - val_acc: 0.3819\n",
      "Epoch 66/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 1.7091 - acc: 0.3576 - val_loss: 1.7161 - val_acc: 0.3646\n",
      "Epoch 67/1000\n",
      "1152/1152 [==============================] - 1s 480us/step - loss: 1.6853 - acc: 0.3681 - val_loss: 1.6781 - val_acc: 0.3819\n",
      "Epoch 68/1000\n",
      "1152/1152 [==============================] - 1s 490us/step - loss: 1.6684 - acc: 0.3655 - val_loss: 1.6943 - val_acc: 0.3646\n",
      "Epoch 69/1000\n",
      "1152/1152 [==============================] - 1s 460us/step - loss: 1.6918 - acc: 0.3559 - val_loss: 1.7150 - val_acc: 0.3681\n",
      "Epoch 70/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 1.6697 - acc: 0.3698 - val_loss: 1.6677 - val_acc: 0.3785\n",
      "Epoch 71/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 1.6557 - acc: 0.3672 - val_loss: 1.6837 - val_acc: 0.3611\n",
      "Epoch 72/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 1.6535 - acc: 0.3924 - val_loss: 1.7440 - val_acc: 0.3160\n",
      "Epoch 73/1000\n",
      "1152/1152 [==============================] - 1s 443us/step - loss: 1.6694 - acc: 0.3663 - val_loss: 1.7080 - val_acc: 0.3472\n",
      "Epoch 74/1000\n",
      "1152/1152 [==============================] - 0s 294us/step - loss: 1.6614 - acc: 0.3672 - val_loss: 1.6918 - val_acc: 0.3750\n",
      "Epoch 75/1000\n",
      "1152/1152 [==============================] - 0s 296us/step - loss: 1.6366 - acc: 0.3854 - val_loss: 1.6854 - val_acc: 0.3924\n",
      "Epoch 76/1000\n",
      "1152/1152 [==============================] - 0s 301us/step - loss: 1.6612 - acc: 0.3785 - val_loss: 1.6722 - val_acc: 0.4097\n",
      "Epoch 77/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 1.6520 - acc: 0.3750 - val_loss: 1.6847 - val_acc: 0.3958\n",
      "Epoch 78/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.6341 - acc: 0.3750 - val_loss: 1.6524 - val_acc: 0.4271\n",
      "Epoch 79/1000\n",
      "1152/1152 [==============================] - 0s 294us/step - loss: 1.6180 - acc: 0.3993 - val_loss: 1.6425 - val_acc: 0.3819\n",
      "Epoch 80/1000\n",
      "1152/1152 [==============================] - 0s 287us/step - loss: 1.6053 - acc: 0.4071 - val_loss: 1.6476 - val_acc: 0.3889\n",
      "Epoch 81/1000\n",
      "1152/1152 [==============================] - 0s 414us/step - loss: 1.6439 - acc: 0.3906 - val_loss: 1.6516 - val_acc: 0.4271\n",
      "Epoch 82/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 1.6251 - acc: 0.3880 - val_loss: 1.6613 - val_acc: 0.3889\n",
      "Epoch 83/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.6077 - acc: 0.4106 - val_loss: 1.6724 - val_acc: 0.3958\n",
      "Epoch 84/1000\n",
      "1152/1152 [==============================] - 0s 299us/step - loss: 1.6084 - acc: 0.3924 - val_loss: 1.6428 - val_acc: 0.4271\n",
      "Epoch 85/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.6240 - acc: 0.4002 - val_loss: 1.6376 - val_acc: 0.4097\n",
      "Epoch 86/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 1.6181 - acc: 0.4002 - val_loss: 1.6194 - val_acc: 0.3993\n",
      "Epoch 87/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.6022 - acc: 0.4036 - val_loss: 1.6443 - val_acc: 0.4028\n",
      "Epoch 88/1000\n",
      "1152/1152 [==============================] - 0s 299us/step - loss: 1.5937 - acc: 0.3915 - val_loss: 1.6344 - val_acc: 0.4097\n",
      "Epoch 89/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.6192 - acc: 0.3776 - val_loss: 1.6372 - val_acc: 0.4062\n",
      "Epoch 90/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.5903 - acc: 0.4132 - val_loss: 1.6248 - val_acc: 0.4167\n",
      "Epoch 91/1000\n",
      "1152/1152 [==============================] - 0s 425us/step - loss: 1.5987 - acc: 0.4132 - val_loss: 1.6132 - val_acc: 0.4271\n",
      "Epoch 92/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 1.5865 - acc: 0.3984 - val_loss: 1.6000 - val_acc: 0.4514\n",
      "Epoch 93/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.5896 - acc: 0.3967 - val_loss: 1.5842 - val_acc: 0.4688\n",
      "Epoch 94/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.5688 - acc: 0.4097 - val_loss: 1.5862 - val_acc: 0.4444\n",
      "Epoch 95/1000\n",
      "1152/1152 [==============================] - 0s 295us/step - loss: 1.5630 - acc: 0.4158 - val_loss: 1.5759 - val_acc: 0.4340\n",
      "Epoch 96/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.5692 - acc: 0.4080 - val_loss: 1.5816 - val_acc: 0.4444\n",
      "Epoch 97/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 1.5650 - acc: 0.4297 - val_loss: 1.5961 - val_acc: 0.4340\n",
      "Epoch 98/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 1.5443 - acc: 0.4253 - val_loss: 1.5636 - val_acc: 0.4618\n",
      "Epoch 99/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.5514 - acc: 0.4375 - val_loss: 1.5711 - val_acc: 0.4340\n",
      "Epoch 100/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.5614 - acc: 0.4201 - val_loss: 1.5799 - val_acc: 0.4410\n",
      "Epoch 101/1000\n",
      "1152/1152 [==============================] - 0s 294us/step - loss: 1.5432 - acc: 0.4410 - val_loss: 1.5695 - val_acc: 0.4757\n",
      "Epoch 102/1000\n",
      "1152/1152 [==============================] - 0s 296us/step - loss: 1.5556 - acc: 0.4080 - val_loss: 1.5756 - val_acc: 0.4236\n",
      "Epoch 103/1000\n",
      "1152/1152 [==============================] - 0s 296us/step - loss: 1.5524 - acc: 0.4288 - val_loss: 1.5919 - val_acc: 0.4167\n",
      "Epoch 104/1000\n",
      "1152/1152 [==============================] - 0s 290us/step - loss: 1.5317 - acc: 0.4245 - val_loss: 1.5702 - val_acc: 0.4618\n",
      "Epoch 105/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 1.5236 - acc: 0.4436 - val_loss: 1.5691 - val_acc: 0.4375\n",
      "Epoch 106/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.5446 - acc: 0.4410 - val_loss: 1.5762 - val_acc: 0.4132\n",
      "Epoch 107/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.5263 - acc: 0.4384 - val_loss: 1.5649 - val_acc: 0.4375\n",
      "Epoch 108/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.5247 - acc: 0.4410 - val_loss: 1.5692 - val_acc: 0.4618\n",
      "Epoch 109/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.5167 - acc: 0.4384 - val_loss: 1.5480 - val_acc: 0.4757\n",
      "Epoch 110/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 1.5087 - acc: 0.4392 - val_loss: 1.5334 - val_acc: 0.4826\n",
      "Epoch 111/1000\n",
      "1152/1152 [==============================] - 0s 301us/step - loss: 1.4971 - acc: 0.4401 - val_loss: 1.5322 - val_acc: 0.4688\n",
      "Epoch 112/1000\n",
      "1152/1152 [==============================] - 0s 293us/step - loss: 1.5280 - acc: 0.4358 - val_loss: 1.5449 - val_acc: 0.4583\n",
      "Epoch 113/1000\n",
      "1152/1152 [==============================] - 0s 296us/step - loss: 1.5111 - acc: 0.4609 - val_loss: 1.5577 - val_acc: 0.4514\n",
      "Epoch 114/1000\n",
      "1152/1152 [==============================] - 0s 425us/step - loss: 1.5029 - acc: 0.4453 - val_loss: 1.5429 - val_acc: 0.4722\n",
      "Epoch 115/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.4927 - acc: 0.4670 - val_loss: 1.5408 - val_acc: 0.4549\n",
      "Epoch 116/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 1.4933 - acc: 0.4358 - val_loss: 1.5565 - val_acc: 0.4653\n",
      "Epoch 117/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 1.4747 - acc: 0.4618 - val_loss: 1.5206 - val_acc: 0.4861\n",
      "Epoch 118/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.4962 - acc: 0.4514 - val_loss: 1.5498 - val_acc: 0.4653\n",
      "Epoch 119/1000\n",
      "1152/1152 [==============================] - 1s 441us/step - loss: 1.4537 - acc: 0.4661 - val_loss: 1.5563 - val_acc: 0.4306\n",
      "Epoch 120/1000\n",
      "1152/1152 [==============================] - 0s 433us/step - loss: 1.4727 - acc: 0.4696 - val_loss: 1.5592 - val_acc: 0.4549\n",
      "Epoch 121/1000\n",
      "1152/1152 [==============================] - 1s 437us/step - loss: 1.4801 - acc: 0.4592 - val_loss: 1.5370 - val_acc: 0.4722\n",
      "Epoch 122/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 1.5095 - acc: 0.4488 - val_loss: 1.5332 - val_acc: 0.4583\n",
      "Epoch 123/1000\n",
      "1152/1152 [==============================] - 0s 419us/step - loss: 1.4861 - acc: 0.4523 - val_loss: 1.5350 - val_acc: 0.4688\n",
      "Epoch 124/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 1.4534 - acc: 0.4722 - val_loss: 1.5254 - val_acc: 0.4618\n",
      "Epoch 125/1000\n",
      "1152/1152 [==============================] - 0s 426us/step - loss: 1.4500 - acc: 0.4653 - val_loss: 1.5392 - val_acc: 0.4549\n",
      "Epoch 126/1000\n",
      "1152/1152 [==============================] - 1s 472us/step - loss: 1.4605 - acc: 0.4835 - val_loss: 1.5108 - val_acc: 0.4583\n",
      "Epoch 127/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 1.4820 - acc: 0.4644 - val_loss: 1.5375 - val_acc: 0.4514\n",
      "Epoch 128/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 1.4295 - acc: 0.4774 - val_loss: 1.5021 - val_acc: 0.4757\n",
      "Epoch 129/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.4507 - acc: 0.4757 - val_loss: 1.4922 - val_acc: 0.4896\n",
      "Epoch 130/1000\n",
      "1152/1152 [==============================] - 0s 288us/step - loss: 1.4744 - acc: 0.4670 - val_loss: 1.5165 - val_acc: 0.4618\n",
      "Epoch 131/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.4260 - acc: 0.4878 - val_loss: 1.5417 - val_acc: 0.4271\n",
      "Epoch 132/1000\n",
      "1152/1152 [==============================] - 0s 299us/step - loss: 1.4475 - acc: 0.4688 - val_loss: 1.4851 - val_acc: 0.4931\n",
      "Epoch 133/1000\n",
      "1152/1152 [==============================] - 0s 293us/step - loss: 1.4476 - acc: 0.4714 - val_loss: 1.5175 - val_acc: 0.4271\n",
      "Epoch 134/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.4347 - acc: 0.4722 - val_loss: 1.4915 - val_acc: 0.4583\n",
      "Epoch 135/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.4208 - acc: 0.4722 - val_loss: 1.4935 - val_acc: 0.4618\n",
      "Epoch 136/1000\n",
      "1152/1152 [==============================] - 0s 303us/step - loss: 1.4128 - acc: 0.4965 - val_loss: 1.5119 - val_acc: 0.4688\n",
      "Epoch 137/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.4294 - acc: 0.4870 - val_loss: 1.4840 - val_acc: 0.4792\n",
      "Epoch 138/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 1.4159 - acc: 0.4809 - val_loss: 1.4621 - val_acc: 0.4757\n",
      "Epoch 139/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 1.4114 - acc: 0.4905 - val_loss: 1.4948 - val_acc: 0.4479\n",
      "Epoch 140/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 1.4288 - acc: 0.4878 - val_loss: 1.4956 - val_acc: 0.4618\n",
      "Epoch 141/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 1.4163 - acc: 0.4896 - val_loss: 1.4831 - val_acc: 0.4757\n",
      "Epoch 142/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.4093 - acc: 0.5035 - val_loss: 1.4681 - val_acc: 0.4688\n",
      "Epoch 143/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 1.4191 - acc: 0.4896 - val_loss: 1.4534 - val_acc: 0.4931\n",
      "Epoch 144/1000\n",
      "1152/1152 [==============================] - 0s 311us/step - loss: 1.4066 - acc: 0.4809 - val_loss: 1.4822 - val_acc: 0.4792\n",
      "Epoch 145/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 1.4088 - acc: 0.5017 - val_loss: 1.4886 - val_acc: 0.4792\n",
      "Epoch 146/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 1.3869 - acc: 0.5130 - val_loss: 1.4876 - val_acc: 0.4722\n",
      "Epoch 147/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.3832 - acc: 0.4844 - val_loss: 1.4608 - val_acc: 0.4931\n",
      "Epoch 148/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 1.3811 - acc: 0.5156 - val_loss: 1.4491 - val_acc: 0.5069\n",
      "Epoch 149/1000\n",
      "1152/1152 [==============================] - 0s 292us/step - loss: 1.3730 - acc: 0.5078 - val_loss: 1.4473 - val_acc: 0.4896\n",
      "Epoch 150/1000\n",
      "1152/1152 [==============================] - 0s 298us/step - loss: 1.3809 - acc: 0.4974 - val_loss: 1.4713 - val_acc: 0.4549\n",
      "Epoch 151/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 1.3756 - acc: 0.5104 - val_loss: 1.4710 - val_acc: 0.4792\n",
      "Epoch 152/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.4017 - acc: 0.4852 - val_loss: 1.4591 - val_acc: 0.4861\n",
      "Epoch 153/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 1.3952 - acc: 0.4861 - val_loss: 1.4527 - val_acc: 0.4826\n",
      "Epoch 154/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 1.3514 - acc: 0.5217 - val_loss: 1.4567 - val_acc: 0.4931\n",
      "Epoch 155/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.3540 - acc: 0.5043 - val_loss: 1.4258 - val_acc: 0.5104\n",
      "Epoch 156/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.3859 - acc: 0.5000 - val_loss: 1.4433 - val_acc: 0.5104\n",
      "Epoch 157/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 1.3632 - acc: 0.5156 - val_loss: 1.4668 - val_acc: 0.5000\n",
      "Epoch 158/1000\n",
      "1152/1152 [==============================] - 0s 291us/step - loss: 1.3445 - acc: 0.5252 - val_loss: 1.4818 - val_acc: 0.4792\n",
      "Epoch 159/1000\n",
      "1152/1152 [==============================] - 0s 302us/step - loss: 1.3576 - acc: 0.5174 - val_loss: 1.4477 - val_acc: 0.4757\n",
      "Epoch 160/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.3424 - acc: 0.5035 - val_loss: 1.4639 - val_acc: 0.4792\n",
      "Epoch 161/1000\n",
      "1152/1152 [==============================] - 0s 291us/step - loss: 1.3371 - acc: 0.5156 - val_loss: 1.4551 - val_acc: 0.4757\n",
      "Epoch 162/1000\n",
      "1152/1152 [==============================] - 0s 298us/step - loss: 1.3456 - acc: 0.5122 - val_loss: 1.4183 - val_acc: 0.5174\n",
      "Epoch 163/1000\n",
      "1152/1152 [==============================] - 0s 302us/step - loss: 1.3257 - acc: 0.5174 - val_loss: 1.4470 - val_acc: 0.4722\n",
      "Epoch 164/1000\n",
      "1152/1152 [==============================] - 0s 295us/step - loss: 1.3518 - acc: 0.5113 - val_loss: 1.4547 - val_acc: 0.4931\n",
      "Epoch 165/1000\n",
      "1152/1152 [==============================] - 0s 312us/step - loss: 1.3464 - acc: 0.5130 - val_loss: 1.4493 - val_acc: 0.4861\n",
      "Epoch 166/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 1.3280 - acc: 0.5234 - val_loss: 1.4333 - val_acc: 0.4931\n",
      "Epoch 167/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.3316 - acc: 0.5330 - val_loss: 1.4477 - val_acc: 0.4965\n",
      "Epoch 168/1000\n",
      "1152/1152 [==============================] - 0s 292us/step - loss: 1.3458 - acc: 0.5052 - val_loss: 1.4638 - val_acc: 0.4861\n",
      "Epoch 169/1000\n",
      "1152/1152 [==============================] - 0s 290us/step - loss: 1.3185 - acc: 0.5391 - val_loss: 1.4189 - val_acc: 0.5069\n",
      "Epoch 170/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.3314 - acc: 0.5330 - val_loss: 1.4313 - val_acc: 0.5174\n",
      "Epoch 171/1000\n",
      "1152/1152 [==============================] - 0s 297us/step - loss: 1.3124 - acc: 0.5295 - val_loss: 1.4035 - val_acc: 0.5035\n",
      "Epoch 172/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.3134 - acc: 0.5252 - val_loss: 1.3944 - val_acc: 0.5035\n",
      "Epoch 173/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 1.3269 - acc: 0.5200 - val_loss: 1.4147 - val_acc: 0.5069\n",
      "Epoch 174/1000\n",
      "1152/1152 [==============================] - 0s 299us/step - loss: 1.3085 - acc: 0.5312 - val_loss: 1.4140 - val_acc: 0.5139\n",
      "Epoch 175/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.3254 - acc: 0.5356 - val_loss: 1.3976 - val_acc: 0.5035\n",
      "Epoch 176/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.2770 - acc: 0.5503 - val_loss: 1.4002 - val_acc: 0.5069\n",
      "Epoch 177/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.2928 - acc: 0.5295 - val_loss: 1.4333 - val_acc: 0.4618\n",
      "Epoch 178/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.2875 - acc: 0.5278 - val_loss: 1.4087 - val_acc: 0.5069\n",
      "Epoch 179/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.3067 - acc: 0.5339 - val_loss: 1.3980 - val_acc: 0.5035\n",
      "Epoch 180/1000\n",
      "1152/1152 [==============================] - 0s 296us/step - loss: 1.2957 - acc: 0.5304 - val_loss: 1.4118 - val_acc: 0.5069\n",
      "Epoch 181/1000\n",
      "1152/1152 [==============================] - 0s 293us/step - loss: 1.2806 - acc: 0.5356 - val_loss: 1.4093 - val_acc: 0.4965\n",
      "Epoch 182/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.2789 - acc: 0.5425 - val_loss: 1.3948 - val_acc: 0.4931\n",
      "Epoch 183/1000\n",
      "1152/1152 [==============================] - 0s 293us/step - loss: 1.2738 - acc: 0.5547 - val_loss: 1.4102 - val_acc: 0.5035\n",
      "Epoch 184/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.2969 - acc: 0.5339 - val_loss: 1.3992 - val_acc: 0.4896\n",
      "Epoch 185/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.2722 - acc: 0.5512 - val_loss: 1.4335 - val_acc: 0.4861\n",
      "Epoch 186/1000\n",
      "1152/1152 [==============================] - 0s 294us/step - loss: 1.2667 - acc: 0.5521 - val_loss: 1.3928 - val_acc: 0.5035\n",
      "Epoch 187/1000\n",
      "1152/1152 [==============================] - 0s 289us/step - loss: 1.2472 - acc: 0.5477 - val_loss: 1.3957 - val_acc: 0.5104\n",
      "Epoch 188/1000\n",
      "1152/1152 [==============================] - 0s 299us/step - loss: 1.2636 - acc: 0.5573 - val_loss: 1.3732 - val_acc: 0.4965\n",
      "Epoch 189/1000\n",
      "1152/1152 [==============================] - 0s 296us/step - loss: 1.2673 - acc: 0.5530 - val_loss: 1.4178 - val_acc: 0.4792\n",
      "Epoch 190/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.2311 - acc: 0.5608 - val_loss: 1.3889 - val_acc: 0.4931\n",
      "Epoch 191/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.2563 - acc: 0.5634 - val_loss: 1.3882 - val_acc: 0.5069\n",
      "Epoch 192/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.2748 - acc: 0.5477 - val_loss: 1.4098 - val_acc: 0.4931\n",
      "Epoch 193/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.2630 - acc: 0.5451 - val_loss: 1.4225 - val_acc: 0.4757\n",
      "Epoch 194/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.2505 - acc: 0.5503 - val_loss: 1.3871 - val_acc: 0.5174\n",
      "Epoch 195/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.2382 - acc: 0.5521 - val_loss: 1.4090 - val_acc: 0.4861\n",
      "Epoch 196/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.2548 - acc: 0.5434 - val_loss: 1.3738 - val_acc: 0.5069\n",
      "Epoch 197/1000\n",
      "1152/1152 [==============================] - 0s 295us/step - loss: 1.2279 - acc: 0.5547 - val_loss: 1.4093 - val_acc: 0.5000\n",
      "Epoch 198/1000\n",
      "1152/1152 [==============================] - 0s 304us/step - loss: 1.2417 - acc: 0.5425 - val_loss: 1.3836 - val_acc: 0.5104\n",
      "Epoch 199/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.2282 - acc: 0.5686 - val_loss: 1.4071 - val_acc: 0.4965\n",
      "Epoch 200/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.2317 - acc: 0.5486 - val_loss: 1.3942 - val_acc: 0.5208\n",
      "Epoch 201/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.2324 - acc: 0.5625 - val_loss: 1.3969 - val_acc: 0.5139\n",
      "Epoch 202/1000\n",
      "1152/1152 [==============================] - 0s 305us/step - loss: 1.2363 - acc: 0.5625 - val_loss: 1.3931 - val_acc: 0.5208\n",
      "Epoch 203/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.2171 - acc: 0.5677 - val_loss: 1.3760 - val_acc: 0.5069\n",
      "Epoch 204/1000\n",
      "1152/1152 [==============================] - 0s 307us/step - loss: 1.2294 - acc: 0.5625 - val_loss: 1.4027 - val_acc: 0.4965\n",
      "Epoch 205/1000\n",
      "1152/1152 [==============================] - 0s 305us/step - loss: 1.2335 - acc: 0.5677 - val_loss: 1.3966 - val_acc: 0.5069\n",
      "Epoch 206/1000\n",
      "1152/1152 [==============================] - 0s 291us/step - loss: 1.2114 - acc: 0.5677 - val_loss: 1.3742 - val_acc: 0.5035\n",
      "Epoch 207/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.1778 - acc: 0.5807 - val_loss: 1.3675 - val_acc: 0.5382\n",
      "Epoch 208/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.2079 - acc: 0.5747 - val_loss: 1.3610 - val_acc: 0.4896\n",
      "Epoch 209/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.2242 - acc: 0.5538 - val_loss: 1.3876 - val_acc: 0.5174\n",
      "Epoch 210/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 1.2136 - acc: 0.5512 - val_loss: 1.3708 - val_acc: 0.5000\n",
      "Epoch 211/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 1.2289 - acc: 0.5677 - val_loss: 1.3557 - val_acc: 0.5000\n",
      "Epoch 212/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 1.1997 - acc: 0.5703 - val_loss: 1.3521 - val_acc: 0.5243\n",
      "Epoch 213/1000\n",
      "1152/1152 [==============================] - 0s 306us/step - loss: 1.1859 - acc: 0.5799 - val_loss: 1.3946 - val_acc: 0.4792\n",
      "Epoch 214/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.1920 - acc: 0.5668 - val_loss: 1.3743 - val_acc: 0.5347\n",
      "Epoch 215/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 1.2024 - acc: 0.5755 - val_loss: 1.3909 - val_acc: 0.5104\n",
      "Epoch 216/1000\n",
      "1152/1152 [==============================] - 0s 301us/step - loss: 1.1996 - acc: 0.5859 - val_loss: 1.3966 - val_acc: 0.4896\n",
      "Epoch 217/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.2118 - acc: 0.5851 - val_loss: 1.3810 - val_acc: 0.5139\n",
      "Epoch 218/1000\n",
      "1152/1152 [==============================] - 0s 316us/step - loss: 1.1992 - acc: 0.5712 - val_loss: 1.3611 - val_acc: 0.5000\n",
      "Epoch 219/1000\n",
      "1152/1152 [==============================] - 0s 307us/step - loss: 1.1861 - acc: 0.5842 - val_loss: 1.3895 - val_acc: 0.5069\n",
      "Epoch 220/1000\n",
      "1152/1152 [==============================] - 0s 307us/step - loss: 1.1910 - acc: 0.5790 - val_loss: 1.3481 - val_acc: 0.4931\n",
      "Epoch 221/1000\n",
      "1152/1152 [==============================] - 0s 313us/step - loss: 1.2005 - acc: 0.5747 - val_loss: 1.3578 - val_acc: 0.5139\n",
      "Epoch 222/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 1.1639 - acc: 0.5981 - val_loss: 1.3686 - val_acc: 0.4965\n",
      "Epoch 223/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.1827 - acc: 0.5825 - val_loss: 1.3768 - val_acc: 0.4896\n",
      "Epoch 224/1000\n",
      "1152/1152 [==============================] - 0s 300us/step - loss: 1.1760 - acc: 0.5903 - val_loss: 1.3869 - val_acc: 0.4688\n",
      "Epoch 225/1000\n",
      "1152/1152 [==============================] - 0s 292us/step - loss: 1.1691 - acc: 0.5790 - val_loss: 1.3585 - val_acc: 0.5208\n",
      "Epoch 226/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.1931 - acc: 0.5747 - val_loss: 1.3519 - val_acc: 0.4965\n",
      "Epoch 227/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 1.1692 - acc: 0.5781 - val_loss: 1.3653 - val_acc: 0.4861\n",
      "Epoch 228/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.1707 - acc: 0.5651 - val_loss: 1.3485 - val_acc: 0.5035\n",
      "Epoch 229/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 1.1480 - acc: 0.5911 - val_loss: 1.3553 - val_acc: 0.5000\n",
      "Epoch 230/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 1.1437 - acc: 0.5833 - val_loss: 1.3649 - val_acc: 0.5312\n",
      "Epoch 231/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 1.1689 - acc: 0.6059 - val_loss: 1.3284 - val_acc: 0.5278\n",
      "Epoch 232/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.1666 - acc: 0.5842 - val_loss: 1.3720 - val_acc: 0.5104\n",
      "Epoch 233/1000\n",
      "1152/1152 [==============================] - 0s 295us/step - loss: 1.1567 - acc: 0.5955 - val_loss: 1.3715 - val_acc: 0.5104\n",
      "Epoch 234/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 1.1359 - acc: 0.5946 - val_loss: 1.3554 - val_acc: 0.4931\n",
      "Epoch 235/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 1.1716 - acc: 0.5842 - val_loss: 1.3516 - val_acc: 0.5000\n",
      "Epoch 236/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.1584 - acc: 0.5816 - val_loss: 1.3557 - val_acc: 0.4965\n",
      "Epoch 237/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.1282 - acc: 0.5955 - val_loss: 1.3283 - val_acc: 0.5382\n",
      "Epoch 238/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 1.1389 - acc: 0.5894 - val_loss: 1.3538 - val_acc: 0.5000\n",
      "Epoch 239/1000\n",
      "1152/1152 [==============================] - 0s 310us/step - loss: 1.1209 - acc: 0.6068 - val_loss: 1.3353 - val_acc: 0.5208\n",
      "Epoch 240/1000\n",
      "1152/1152 [==============================] - 0s 292us/step - loss: 1.1174 - acc: 0.6007 - val_loss: 1.3375 - val_acc: 0.5243\n",
      "Epoch 241/1000\n",
      "1152/1152 [==============================] - 0s 293us/step - loss: 1.1479 - acc: 0.5807 - val_loss: 1.3506 - val_acc: 0.5069\n",
      "Epoch 242/1000\n",
      "1152/1152 [==============================] - 0s 308us/step - loss: 1.1302 - acc: 0.6033 - val_loss: 1.3548 - val_acc: 0.5208\n",
      "Epoch 243/1000\n",
      "1152/1152 [==============================] - 0s 294us/step - loss: 1.1248 - acc: 0.6042 - val_loss: 1.3604 - val_acc: 0.5104\n",
      "Epoch 244/1000\n",
      "1152/1152 [==============================] - 0s 304us/step - loss: 1.1306 - acc: 0.5946 - val_loss: 1.3233 - val_acc: 0.5104\n",
      "Epoch 245/1000\n",
      "1152/1152 [==============================] - 0s 424us/step - loss: 1.1060 - acc: 0.6181 - val_loss: 1.3318 - val_acc: 0.5174\n",
      "Epoch 246/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 1.1172 - acc: 0.5990 - val_loss: 1.3450 - val_acc: 0.5278\n",
      "Epoch 247/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 1.1204 - acc: 0.6050 - val_loss: 1.3138 - val_acc: 0.5104\n",
      "Epoch 248/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0996 - acc: 0.6042 - val_loss: 1.3228 - val_acc: 0.5035\n",
      "Epoch 249/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 1.1084 - acc: 0.6068 - val_loss: 1.3403 - val_acc: 0.5104\n",
      "Epoch 250/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.1160 - acc: 0.6007 - val_loss: 1.3426 - val_acc: 0.5174\n",
      "Epoch 251/1000\n",
      "1152/1152 [==============================] - 0s 315us/step - loss: 1.1314 - acc: 0.5955 - val_loss: 1.3277 - val_acc: 0.5382\n",
      "Epoch 252/1000\n",
      "1152/1152 [==============================] - 0s 309us/step - loss: 1.1259 - acc: 0.6016 - val_loss: 1.3385 - val_acc: 0.5347\n",
      "Epoch 253/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 1.1063 - acc: 0.6076 - val_loss: 1.3389 - val_acc: 0.5417\n",
      "Epoch 254/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.0947 - acc: 0.6172 - val_loss: 1.3249 - val_acc: 0.5069\n",
      "Epoch 255/1000\n",
      "1152/1152 [==============================] - 0s 417us/step - loss: 1.0894 - acc: 0.6207 - val_loss: 1.3303 - val_acc: 0.5174\n",
      "Epoch 256/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 1.1120 - acc: 0.6120 - val_loss: 1.3231 - val_acc: 0.5347\n",
      "Epoch 257/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 1.0849 - acc: 0.6241 - val_loss: 1.3473 - val_acc: 0.5035\n",
      "Epoch 258/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0854 - acc: 0.6241 - val_loss: 1.3378 - val_acc: 0.5243\n",
      "Epoch 259/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.0989 - acc: 0.6024 - val_loss: 1.3068 - val_acc: 0.5382\n",
      "Epoch 260/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 1.0880 - acc: 0.6068 - val_loss: 1.3295 - val_acc: 0.5000\n",
      "Epoch 261/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 1.0924 - acc: 0.6146 - val_loss: 1.3409 - val_acc: 0.5139\n",
      "Epoch 262/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.0895 - acc: 0.6311 - val_loss: 1.3193 - val_acc: 0.5347\n",
      "Epoch 263/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 1.0956 - acc: 0.6146 - val_loss: 1.3058 - val_acc: 0.5139\n",
      "Epoch 264/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 1.0949 - acc: 0.6172 - val_loss: 1.3114 - val_acc: 0.5382\n",
      "Epoch 265/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 1.0789 - acc: 0.6267 - val_loss: 1.3147 - val_acc: 0.5521\n",
      "Epoch 266/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 1.0620 - acc: 0.6441 - val_loss: 1.3171 - val_acc: 0.5174\n",
      "Epoch 267/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.0743 - acc: 0.6207 - val_loss: 1.2839 - val_acc: 0.5243\n",
      "Epoch 268/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 1.0482 - acc: 0.6189 - val_loss: 1.3263 - val_acc: 0.5243\n",
      "Epoch 269/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.0639 - acc: 0.6267 - val_loss: 1.3168 - val_acc: 0.5278\n",
      "Epoch 270/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 1.0720 - acc: 0.6172 - val_loss: 1.2976 - val_acc: 0.5139\n",
      "Epoch 271/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 1.0925 - acc: 0.6293 - val_loss: 1.3350 - val_acc: 0.5139\n",
      "Epoch 272/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.0778 - acc: 0.6128 - val_loss: 1.3055 - val_acc: 0.5312\n",
      "Epoch 273/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 1.0593 - acc: 0.6293 - val_loss: 1.3101 - val_acc: 0.5451\n",
      "Epoch 274/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 1.0481 - acc: 0.6302 - val_loss: 1.3325 - val_acc: 0.5312\n",
      "Epoch 275/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.0463 - acc: 0.6224 - val_loss: 1.3192 - val_acc: 0.5312\n",
      "Epoch 276/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 1.0355 - acc: 0.6441 - val_loss: 1.3367 - val_acc: 0.5208\n",
      "Epoch 277/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 1.0505 - acc: 0.6233 - val_loss: 1.3354 - val_acc: 0.5174\n",
      "Epoch 278/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 1.0474 - acc: 0.6372 - val_loss: 1.2888 - val_acc: 0.5347\n",
      "Epoch 279/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 1.0467 - acc: 0.6380 - val_loss: 1.3195 - val_acc: 0.5104\n",
      "Epoch 280/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0536 - acc: 0.6328 - val_loss: 1.3238 - val_acc: 0.5243\n",
      "Epoch 281/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 1.0631 - acc: 0.6354 - val_loss: 1.3029 - val_acc: 0.5451\n",
      "Epoch 282/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 1.0383 - acc: 0.6267 - val_loss: 1.3090 - val_acc: 0.5382\n",
      "Epoch 283/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 1.0306 - acc: 0.6259 - val_loss: 1.2957 - val_acc: 0.5556\n",
      "Epoch 284/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0127 - acc: 0.6484 - val_loss: 1.3114 - val_acc: 0.5417\n",
      "Epoch 285/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 1.0462 - acc: 0.6519 - val_loss: 1.3017 - val_acc: 0.5174\n",
      "Epoch 286/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 1.0181 - acc: 0.6450 - val_loss: 1.2916 - val_acc: 0.5174\n",
      "Epoch 287/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 1.0456 - acc: 0.6302 - val_loss: 1.2919 - val_acc: 0.5625\n",
      "Epoch 288/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 1.0142 - acc: 0.6441 - val_loss: 1.3540 - val_acc: 0.5035\n",
      "Epoch 289/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 1.0196 - acc: 0.6259 - val_loss: 1.3027 - val_acc: 0.5382\n",
      "Epoch 290/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 1.0123 - acc: 0.6458 - val_loss: 1.3558 - val_acc: 0.5035\n",
      "Epoch 291/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 0.9988 - acc: 0.6615 - val_loss: 1.3230 - val_acc: 0.5035\n",
      "Epoch 292/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 1.0339 - acc: 0.6354 - val_loss: 1.2872 - val_acc: 0.5417\n",
      "Epoch 293/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.0026 - acc: 0.6441 - val_loss: 1.3081 - val_acc: 0.5347\n",
      "Epoch 294/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 1.0083 - acc: 0.638 - 0s 351us/step - loss: 1.0130 - acc: 0.6389 - val_loss: 1.3009 - val_acc: 0.5451\n",
      "Epoch 295/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 1.0125 - acc: 0.6493 - val_loss: 1.3108 - val_acc: 0.5347\n",
      "Epoch 296/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.9960 - acc: 0.6545 - val_loss: 1.2946 - val_acc: 0.5278\n",
      "Epoch 297/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 1.0072 - acc: 0.6606 - val_loss: 1.2949 - val_acc: 0.5347\n",
      "Epoch 298/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.9951 - acc: 0.6441 - val_loss: 1.2811 - val_acc: 0.5382\n",
      "Epoch 299/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.9828 - acc: 0.6589 - val_loss: 1.3056 - val_acc: 0.5521\n",
      "Epoch 300/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.9830 - acc: 0.6641 - val_loss: 1.2914 - val_acc: 0.5208\n",
      "Epoch 301/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 1.0278 - acc: 0.6380 - val_loss: 1.3056 - val_acc: 0.5625\n",
      "Epoch 302/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 1.0018 - acc: 0.6441 - val_loss: 1.3043 - val_acc: 0.5278\n",
      "Epoch 303/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 0.9898 - acc: 0.6580 - val_loss: 1.3089 - val_acc: 0.5174\n",
      "Epoch 304/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.9756 - acc: 0.6580 - val_loss: 1.2755 - val_acc: 0.5556\n",
      "Epoch 305/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.9747 - acc: 0.6580 - val_loss: 1.2945 - val_acc: 0.5382\n",
      "Epoch 306/1000\n",
      "1152/1152 [==============================] - 1s 510us/step - loss: 0.9720 - acc: 0.6684 - val_loss: 1.2801 - val_acc: 0.5486\n",
      "Epoch 307/1000\n",
      "1152/1152 [==============================] - 1s 503us/step - loss: 0.9643 - acc: 0.6667 - val_loss: 1.2850 - val_acc: 0.5312\n",
      "Epoch 308/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.9948 - acc: 0.6450 - val_loss: 1.2690 - val_acc: 0.5278\n",
      "Epoch 309/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.9514 - acc: 0.6780 - val_loss: 1.2775 - val_acc: 0.5486\n",
      "Epoch 310/1000\n",
      "1152/1152 [==============================] - 1s 488us/step - loss: 0.9809 - acc: 0.6597 - val_loss: 1.2833 - val_acc: 0.5382\n",
      "Epoch 311/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.9759 - acc: 0.6727 - val_loss: 1.3131 - val_acc: 0.5556\n",
      "Epoch 312/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.9670 - acc: 0.6632 - val_loss: 1.2837 - val_acc: 0.5625\n",
      "Epoch 313/1000\n",
      "1152/1152 [==============================] - 1s 506us/step - loss: 0.9675 - acc: 0.6684 - val_loss: 1.2760 - val_acc: 0.5451\n",
      "Epoch 314/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.9693 - acc: 0.6571 - val_loss: 1.2659 - val_acc: 0.5556\n",
      "Epoch 315/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.9720 - acc: 0.6623 - val_loss: 1.3017 - val_acc: 0.5417\n",
      "Epoch 316/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.9750 - acc: 0.6675 - val_loss: 1.2934 - val_acc: 0.5521\n",
      "Epoch 317/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.9632 - acc: 0.6649 - val_loss: 1.2730 - val_acc: 0.5382\n",
      "Epoch 318/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.9189 - acc: 0.6840 - val_loss: 1.2661 - val_acc: 0.5451\n",
      "Epoch 319/1000\n",
      "1152/1152 [==============================] - 1s 491us/step - loss: 0.9489 - acc: 0.6658 - val_loss: 1.2963 - val_acc: 0.5312\n",
      "Epoch 320/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.9543 - acc: 0.6753 - val_loss: 1.2601 - val_acc: 0.5521\n",
      "Epoch 321/1000\n",
      "1152/1152 [==============================] - 1s 500us/step - loss: 0.9353 - acc: 0.6753 - val_loss: 1.2735 - val_acc: 0.5590\n",
      "Epoch 322/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.9403 - acc: 0.6875 - val_loss: 1.2547 - val_acc: 0.5660\n",
      "Epoch 323/1000\n",
      "1152/1152 [==============================] - 1s 444us/step - loss: 0.9226 - acc: 0.6710 - val_loss: 1.2942 - val_acc: 0.5625\n",
      "Epoch 324/1000\n",
      "1152/1152 [==============================] - 0s 400us/step - loss: 0.9343 - acc: 0.6597 - val_loss: 1.3132 - val_acc: 0.5278\n",
      "Epoch 325/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 0.9506 - acc: 0.6736 - val_loss: 1.2700 - val_acc: 0.5451\n",
      "Epoch 326/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.9222 - acc: 0.6762 - val_loss: 1.2790 - val_acc: 0.5243\n",
      "Epoch 327/1000\n",
      "1152/1152 [==============================] - 0s 425us/step - loss: 0.9420 - acc: 0.6658 - val_loss: 1.2731 - val_acc: 0.5521\n",
      "Epoch 328/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.9401 - acc: 0.6615 - val_loss: 1.2523 - val_acc: 0.5625\n",
      "Epoch 329/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.9128 - acc: 0.6788 - val_loss: 1.2948 - val_acc: 0.5312\n",
      "Epoch 330/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.9427 - acc: 0.6675 - val_loss: 1.2784 - val_acc: 0.5590\n",
      "Epoch 331/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.9223 - acc: 0.6797 - val_loss: 1.2786 - val_acc: 0.5729\n",
      "Epoch 332/1000\n",
      "1152/1152 [==============================] - 0s 417us/step - loss: 0.9482 - acc: 0.6571 - val_loss: 1.2593 - val_acc: 0.5417\n",
      "Epoch 333/1000\n",
      "1152/1152 [==============================] - 1s 575us/step - loss: 0.9237 - acc: 0.6693 - val_loss: 1.2836 - val_acc: 0.5590\n",
      "Epoch 334/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.8998 - acc: 0.6866 - val_loss: 1.2824 - val_acc: 0.5312\n",
      "Epoch 335/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 0.9058 - acc: 0.6814 - val_loss: 1.2842 - val_acc: 0.5347\n",
      "Epoch 336/1000\n",
      "1152/1152 [==============================] - 1s 447us/step - loss: 0.9143 - acc: 0.6753 - val_loss: 1.2770 - val_acc: 0.5312\n",
      "Epoch 337/1000\n",
      "1152/1152 [==============================] - 1s 462us/step - loss: 0.9261 - acc: 0.6832 - val_loss: 1.2870 - val_acc: 0.5521\n",
      "Epoch 338/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 0.9011 - acc: 0.6814 - val_loss: 1.2963 - val_acc: 0.5347\n",
      "Epoch 339/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.9266 - acc: 0.6840 - val_loss: 1.2740 - val_acc: 0.5625\n",
      "Epoch 340/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.9010 - acc: 0.6884 - val_loss: 1.2868 - val_acc: 0.5625\n",
      "Epoch 341/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.8954 - acc: 0.6806 - val_loss: 1.2798 - val_acc: 0.5729\n",
      "Epoch 342/1000\n",
      "1152/1152 [==============================] - 0s 406us/step - loss: 0.9380 - acc: 0.6658 - val_loss: 1.2735 - val_acc: 0.5451\n",
      "Epoch 343/1000\n",
      "1152/1152 [==============================] - 1s 542us/step - loss: 0.8849 - acc: 0.7101 - val_loss: 1.2534 - val_acc: 0.5764\n",
      "Epoch 344/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.8998 - acc: 0.6936 - val_loss: 1.2206 - val_acc: 0.5521\n",
      "Epoch 345/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.9156 - acc: 0.6727 - val_loss: 1.2711 - val_acc: 0.5278\n",
      "Epoch 346/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.9103 - acc: 0.6849 - val_loss: 1.2723 - val_acc: 0.5660\n",
      "Epoch 347/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.8866 - acc: 0.7040 - val_loss: 1.2872 - val_acc: 0.5451\n",
      "Epoch 348/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.8720 - acc: 0.7083 - val_loss: 1.2452 - val_acc: 0.5486\n",
      "Epoch 349/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.8917 - acc: 0.7057 - val_loss: 1.2519 - val_acc: 0.5660\n",
      "Epoch 350/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8722 - acc: 0.7101 - val_loss: 1.2644 - val_acc: 0.5382\n",
      "Epoch 351/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.9019 - acc: 0.6918 - val_loss: 1.2354 - val_acc: 0.5521\n",
      "Epoch 352/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.8791 - acc: 0.6953 - val_loss: 1.2463 - val_acc: 0.5799\n",
      "Epoch 353/1000\n",
      "1152/1152 [==============================] - 0s 378us/step - loss: 0.8800 - acc: 0.6979 - val_loss: 1.2677 - val_acc: 0.5382\n",
      "Epoch 354/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.8865 - acc: 0.6788 - val_loss: 1.2677 - val_acc: 0.5590\n",
      "Epoch 355/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.8728 - acc: 0.7057 - val_loss: 1.2532 - val_acc: 0.5799\n",
      "Epoch 356/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.8436 - acc: 0.7161 - val_loss: 1.2520 - val_acc: 0.5556\n",
      "Epoch 357/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.8797 - acc: 0.6832 - val_loss: 1.2537 - val_acc: 0.5521\n",
      "Epoch 358/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.8785 - acc: 0.7066 - val_loss: 1.2518 - val_acc: 0.5556\n",
      "Epoch 359/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.8769 - acc: 0.6910 - val_loss: 1.2553 - val_acc: 0.5521\n",
      "Epoch 360/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.8663 - acc: 0.6970 - val_loss: 1.2610 - val_acc: 0.5486\n",
      "Epoch 361/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.8609 - acc: 0.7101 - val_loss: 1.2834 - val_acc: 0.5208\n",
      "Epoch 362/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.8586 - acc: 0.7066 - val_loss: 1.2671 - val_acc: 0.5625\n",
      "Epoch 363/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.8451 - acc: 0.7101 - val_loss: 1.2565 - val_acc: 0.5764\n",
      "Epoch 364/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.8545 - acc: 0.7040 - val_loss: 1.2406 - val_acc: 0.5486\n",
      "Epoch 365/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.8653 - acc: 0.7101 - val_loss: 1.2490 - val_acc: 0.5590\n",
      "Epoch 366/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.8571 - acc: 0.7161 - val_loss: 1.2927 - val_acc: 0.5417\n",
      "Epoch 367/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.8336 - acc: 0.7170 - val_loss: 1.2364 - val_acc: 0.5417\n",
      "Epoch 368/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.8400 - acc: 0.7144 - val_loss: 1.2372 - val_acc: 0.5590\n",
      "Epoch 369/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.8601 - acc: 0.7127 - val_loss: 1.2828 - val_acc: 0.5521\n",
      "Epoch 370/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.8421 - acc: 0.7040 - val_loss: 1.2373 - val_acc: 0.5660\n",
      "Epoch 371/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.8468 - acc: 0.7274 - val_loss: 1.2635 - val_acc: 0.5694\n",
      "Epoch 372/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.8362 - acc: 0.7153 - val_loss: 1.2505 - val_acc: 0.5590\n",
      "Epoch 373/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.8400 - acc: 0.7075 - val_loss: 1.2539 - val_acc: 0.5590\n",
      "Epoch 374/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.8596 - acc: 0.7075 - val_loss: 1.2935 - val_acc: 0.5556\n",
      "Epoch 375/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.8311 - acc: 0.7222 - val_loss: 1.2360 - val_acc: 0.5694\n",
      "Epoch 376/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.8248 - acc: 0.7109 - val_loss: 1.2778 - val_acc: 0.5417\n",
      "Epoch 377/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.8339 - acc: 0.7214 - val_loss: 1.2545 - val_acc: 0.5694\n",
      "Epoch 378/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.8397 - acc: 0.7101 - val_loss: 1.2361 - val_acc: 0.5451\n",
      "Epoch 379/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.8554 - acc: 0.7135 - val_loss: 1.2536 - val_acc: 0.5764\n",
      "Epoch 380/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 0.8232 - acc: 0.7144 - val_loss: 1.3032 - val_acc: 0.5417\n",
      "Epoch 381/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.8258 - acc: 0.7144 - val_loss: 1.2772 - val_acc: 0.5764\n",
      "Epoch 382/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.8327 - acc: 0.7222 - val_loss: 1.2400 - val_acc: 0.5625\n",
      "Epoch 383/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.8206 - acc: 0.7170 - val_loss: 1.2636 - val_acc: 0.5590\n",
      "Epoch 384/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.8202 - acc: 0.7170 - val_loss: 1.2569 - val_acc: 0.5451\n",
      "Epoch 385/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.8295 - acc: 0.7135 - val_loss: 1.2276 - val_acc: 0.5903\n",
      "Epoch 386/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.8123 - acc: 0.7179 - val_loss: 1.2483 - val_acc: 0.5799\n",
      "Epoch 387/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.8151 - acc: 0.7274 - val_loss: 1.2541 - val_acc: 0.5764\n",
      "Epoch 388/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.8116 - acc: 0.7153 - val_loss: 1.2354 - val_acc: 0.5486\n",
      "Epoch 389/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.8261 - acc: 0.7109 - val_loss: 1.2780 - val_acc: 0.5694\n",
      "Epoch 390/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.8046 - acc: 0.7326 - val_loss: 1.2196 - val_acc: 0.5799\n",
      "Epoch 391/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.7927 - acc: 0.7214 - val_loss: 1.2420 - val_acc: 0.5451\n",
      "Epoch 392/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.7804 - acc: 0.7214 - val_loss: 1.2458 - val_acc: 0.5729\n",
      "Epoch 393/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 0.8235 - acc: 0.7066 - val_loss: 1.2307 - val_acc: 0.5729\n",
      "Epoch 394/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.8075 - acc: 0.7300 - val_loss: 1.2405 - val_acc: 0.5660\n",
      "Epoch 395/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.7937 - acc: 0.7396 - val_loss: 1.2438 - val_acc: 0.5694\n",
      "Epoch 396/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.8028 - acc: 0.7179 - val_loss: 1.2424 - val_acc: 0.5694\n",
      "Epoch 397/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.7841 - acc: 0.7326 - val_loss: 1.2647 - val_acc: 0.5486\n",
      "Epoch 398/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.8016 - acc: 0.7300 - val_loss: 1.1983 - val_acc: 0.5868\n",
      "Epoch 399/1000\n",
      "1152/1152 [==============================] - 0s 427us/step - loss: 0.7859 - acc: 0.7361 - val_loss: 1.2048 - val_acc: 0.5729\n",
      "Epoch 400/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.7748 - acc: 0.7300 - val_loss: 1.2365 - val_acc: 0.5833\n",
      "Epoch 401/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.7873 - acc: 0.7370 - val_loss: 1.2374 - val_acc: 0.5625\n",
      "Epoch 402/1000\n",
      "1152/1152 [==============================] - 1s 541us/step - loss: 0.8103 - acc: 0.7161 - val_loss: 1.2825 - val_acc: 0.5451\n",
      "Epoch 403/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.7987 - acc: 0.7274 - val_loss: 1.2328 - val_acc: 0.5729\n",
      "Epoch 404/1000\n",
      "1152/1152 [==============================] - 1s 449us/step - loss: 0.7899 - acc: 0.7222 - val_loss: 1.2582 - val_acc: 0.5694\n",
      "Epoch 405/1000\n",
      "1152/1152 [==============================] - 1s 479us/step - loss: 0.7645 - acc: 0.7344 - val_loss: 1.2469 - val_acc: 0.5521\n",
      "Epoch 406/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.7629 - acc: 0.7335 - val_loss: 1.2300 - val_acc: 0.5660\n",
      "Epoch 407/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.7889 - acc: 0.7309 - val_loss: 1.2461 - val_acc: 0.5625\n",
      "Epoch 408/1000\n",
      "1152/1152 [==============================] - 0s 404us/step - loss: 0.7621 - acc: 0.7422 - val_loss: 1.2213 - val_acc: 0.5694\n",
      "Epoch 409/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.7723 - acc: 0.7378 - val_loss: 1.2691 - val_acc: 0.5556\n",
      "Epoch 410/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.7919 - acc: 0.7231 - val_loss: 1.2321 - val_acc: 0.5799\n",
      "Epoch 411/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 0.8052 - acc: 0.7214 - val_loss: 1.2544 - val_acc: 0.5660\n",
      "Epoch 412/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.7741 - acc: 0.7387 - val_loss: 1.2230 - val_acc: 0.5694\n",
      "Epoch 413/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.7827 - acc: 0.7309 - val_loss: 1.2477 - val_acc: 0.5590\n",
      "Epoch 414/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.7428 - acc: 0.7543 - val_loss: 1.2318 - val_acc: 0.5521\n",
      "Epoch 415/1000\n",
      "1152/1152 [==============================] - 0s 383us/step - loss: 0.7615 - acc: 0.7344 - val_loss: 1.2341 - val_acc: 0.5868\n",
      "Epoch 416/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.7554 - acc: 0.7214 - val_loss: 1.2553 - val_acc: 0.5729\n",
      "Epoch 417/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.7506 - acc: 0.7448 - val_loss: 1.2254 - val_acc: 0.5938\n",
      "Epoch 418/1000\n",
      "1152/1152 [==============================] - 1s 467us/step - loss: 0.7502 - acc: 0.7543 - val_loss: 1.2520 - val_acc: 0.5660\n",
      "Epoch 419/1000\n",
      "1152/1152 [==============================] - 1s 469us/step - loss: 0.7453 - acc: 0.7370 - val_loss: 1.2480 - val_acc: 0.5903\n",
      "Epoch 420/1000\n",
      "1152/1152 [==============================] - 1s 531us/step - loss: 0.7573 - acc: 0.7465 - val_loss: 1.2445 - val_acc: 0.5764\n",
      "Epoch 421/1000\n",
      "1152/1152 [==============================] - 1s 498us/step - loss: 0.7392 - acc: 0.7474 - val_loss: 1.2119 - val_acc: 0.5868\n",
      "Epoch 422/1000\n",
      "1152/1152 [==============================] - 0s 394us/step - loss: 0.7513 - acc: 0.7465 - val_loss: 1.2405 - val_acc: 0.5694\n",
      "Epoch 423/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.7447 - acc: 0.7457 - val_loss: 1.2703 - val_acc: 0.5625\n",
      "Epoch 424/1000\n",
      "1152/1152 [==============================] - 0s 428us/step - loss: 0.7565 - acc: 0.7283 - val_loss: 1.2017 - val_acc: 0.5868\n",
      "Epoch 425/1000\n",
      "1152/1152 [==============================] - 1s 448us/step - loss: 0.7527 - acc: 0.7396 - val_loss: 1.2138 - val_acc: 0.5764\n",
      "Epoch 426/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.7516 - acc: 0.7465 - val_loss: 1.2416 - val_acc: 0.5590\n",
      "Epoch 427/1000\n",
      "1152/1152 [==============================] - 1s 478us/step - loss: 0.7462 - acc: 0.7283 - val_loss: 1.2162 - val_acc: 0.5660\n",
      "Epoch 428/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.7568 - acc: 0.7361 - val_loss: 1.2161 - val_acc: 0.5868\n",
      "Epoch 429/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.7621 - acc: 0.7422 - val_loss: 1.2112 - val_acc: 0.6042\n",
      "Epoch 430/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.7136 - acc: 0.7578 - val_loss: 1.2456 - val_acc: 0.5694\n",
      "Epoch 431/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.7356 - acc: 0.7595 - val_loss: 1.1951 - val_acc: 0.5729\n",
      "Epoch 432/1000\n",
      "1152/1152 [==============================] - 1s 541us/step - loss: 0.7523 - acc: 0.7378 - val_loss: 1.2161 - val_acc: 0.5764\n",
      "Epoch 433/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.7359 - acc: 0.7300 - val_loss: 1.2433 - val_acc: 0.5729\n",
      "Epoch 434/1000\n",
      "1152/1152 [==============================] - 0s 393us/step - loss: 0.7369 - acc: 0.7500 - val_loss: 1.2305 - val_acc: 0.5729\n",
      "Epoch 435/1000\n",
      "1152/1152 [==============================] - 1s 455us/step - loss: 0.7371 - acc: 0.7630 - val_loss: 1.2144 - val_acc: 0.5625\n",
      "Epoch 436/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.7287 - acc: 0.7500 - val_loss: 1.2244 - val_acc: 0.5764\n",
      "Epoch 437/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.7090 - acc: 0.7682 - val_loss: 1.2260 - val_acc: 0.5625\n",
      "Epoch 438/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.7306 - acc: 0.7526 - val_loss: 1.2044 - val_acc: 0.5972\n",
      "Epoch 439/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.7338 - acc: 0.7352 - val_loss: 1.2180 - val_acc: 0.5660\n",
      "Epoch 440/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.7086 - acc: 0.7578 - val_loss: 1.2029 - val_acc: 0.5868\n",
      "Epoch 441/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6982 - acc: 0.7665 - val_loss: 1.2255 - val_acc: 0.5694\n",
      "Epoch 442/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.7214 - acc: 0.7509 - val_loss: 1.2180 - val_acc: 0.5799\n",
      "Epoch 443/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.7254 - acc: 0.7517 - val_loss: 1.2080 - val_acc: 0.5660\n",
      "Epoch 444/1000\n",
      "1152/1152 [==============================] - 1s 459us/step - loss: 0.7055 - acc: 0.7630 - val_loss: 1.2031 - val_acc: 0.5903\n",
      "Epoch 445/1000\n",
      "1152/1152 [==============================] - 1s 472us/step - loss: 0.7019 - acc: 0.7700 - val_loss: 1.2171 - val_acc: 0.5833\n",
      "Epoch 446/1000\n",
      "1152/1152 [==============================] - 0s 394us/step - loss: 0.7167 - acc: 0.7500 - val_loss: 1.2007 - val_acc: 0.5729\n",
      "Epoch 447/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.7187 - acc: 0.7561 - val_loss: 1.2083 - val_acc: 0.5833\n",
      "Epoch 448/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.7145 - acc: 0.7483 - val_loss: 1.2056 - val_acc: 0.5590\n",
      "Epoch 449/1000\n",
      "1152/1152 [==============================] - 1s 500us/step - loss: 0.7129 - acc: 0.7639 - val_loss: 1.2209 - val_acc: 0.5625\n",
      "Epoch 450/1000\n",
      "1152/1152 [==============================] - 1s 459us/step - loss: 0.7084 - acc: 0.7465 - val_loss: 1.2374 - val_acc: 0.5590\n",
      "Epoch 451/1000\n",
      "1152/1152 [==============================] - 1s 457us/step - loss: 0.7177 - acc: 0.7578 - val_loss: 1.2246 - val_acc: 0.5625\n",
      "Epoch 452/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 0.7042 - acc: 0.7604 - val_loss: 1.2073 - val_acc: 0.5556\n",
      "Epoch 453/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6917 - acc: 0.7517 - val_loss: 1.2442 - val_acc: 0.5799\n",
      "Epoch 454/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.7018 - acc: 0.7587 - val_loss: 1.2259 - val_acc: 0.5799\n",
      "Epoch 455/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6824 - acc: 0.7656 - val_loss: 1.2286 - val_acc: 0.5625\n",
      "Epoch 456/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.7049 - acc: 0.7639 - val_loss: 1.2157 - val_acc: 0.5729\n",
      "Epoch 457/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.7053 - acc: 0.7682 - val_loss: 1.2299 - val_acc: 0.5764\n",
      "Epoch 458/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6757 - acc: 0.7743 - val_loss: 1.1983 - val_acc: 0.6007\n",
      "Epoch 459/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.6928 - acc: 0.7682 - val_loss: 1.2368 - val_acc: 0.5764\n",
      "Epoch 460/1000\n",
      "1152/1152 [==============================] - 1s 694us/step - loss: 0.6974 - acc: 0.7587 - val_loss: 1.2274 - val_acc: 0.5799\n",
      "Epoch 461/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.6695 - acc: 0.7795 - val_loss: 1.2220 - val_acc: 0.5938\n",
      "Epoch 462/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.6765 - acc: 0.7743 - val_loss: 1.2393 - val_acc: 0.5833\n",
      "Epoch 463/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.7176 - acc: 0.7474 - val_loss: 1.2056 - val_acc: 0.5799\n",
      "Epoch 464/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 0.7010 - acc: 0.7726 - val_loss: 1.2377 - val_acc: 0.5764\n",
      "Epoch 465/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.6993 - acc: 0.7595 - val_loss: 1.2272 - val_acc: 0.5590\n",
      "Epoch 466/1000\n",
      "1152/1152 [==============================] - 1s 438us/step - loss: 0.6823 - acc: 0.7752 - val_loss: 1.2341 - val_acc: 0.5764\n",
      "Epoch 467/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.6823 - acc: 0.7665 - val_loss: 1.2146 - val_acc: 0.5868\n",
      "Epoch 468/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.6700 - acc: 0.7700 - val_loss: 1.2259 - val_acc: 0.5764\n",
      "Epoch 469/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6740 - acc: 0.7700 - val_loss: 1.2331 - val_acc: 0.5625\n",
      "Epoch 470/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.6523 - acc: 0.7795 - val_loss: 1.2529 - val_acc: 0.5764\n",
      "Epoch 471/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.6663 - acc: 0.7812 - val_loss: 1.2349 - val_acc: 0.5833\n",
      "Epoch 472/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.6789 - acc: 0.7665 - val_loss: 1.2229 - val_acc: 0.6007\n",
      "Epoch 473/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.6598 - acc: 0.7830 - val_loss: 1.2136 - val_acc: 0.5764\n",
      "Epoch 474/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6701 - acc: 0.7821 - val_loss: 1.2210 - val_acc: 0.5799\n",
      "Epoch 475/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.6712 - acc: 0.7795 - val_loss: 1.2052 - val_acc: 0.6007\n",
      "Epoch 476/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.6789 - acc: 0.7630 - val_loss: 1.2052 - val_acc: 0.5833\n",
      "Epoch 477/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.6564 - acc: 0.7847 - val_loss: 1.2942 - val_acc: 0.5521\n",
      "Epoch 478/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.6795 - acc: 0.7682 - val_loss: 1.2249 - val_acc: 0.5729\n",
      "Epoch 479/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.6556 - acc: 0.7691 - val_loss: 1.2184 - val_acc: 0.5625\n",
      "Epoch 480/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.6414 - acc: 0.7795 - val_loss: 1.2242 - val_acc: 0.5868\n",
      "Epoch 481/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6481 - acc: 0.7812 - val_loss: 1.2178 - val_acc: 0.5521\n",
      "Epoch 482/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6575 - acc: 0.7726 - val_loss: 1.2388 - val_acc: 0.5660\n",
      "Epoch 483/1000\n",
      "1152/1152 [==============================] - 0s 401us/step - loss: 0.6565 - acc: 0.7795 - val_loss: 1.2503 - val_acc: 0.5625\n",
      "Epoch 484/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.6660 - acc: 0.7630 - val_loss: 1.2257 - val_acc: 0.5556\n",
      "Epoch 485/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.6611 - acc: 0.7804 - val_loss: 1.2429 - val_acc: 0.5625\n",
      "Epoch 486/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.6632 - acc: 0.7865 - val_loss: 1.2370 - val_acc: 0.5729\n",
      "Epoch 487/1000\n",
      "1152/1152 [==============================] - 1s 464us/step - loss: 0.6400 - acc: 0.7830 - val_loss: 1.2531 - val_acc: 0.5903\n",
      "Epoch 488/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.6389 - acc: 0.7726 - val_loss: 1.2116 - val_acc: 0.5868\n",
      "Epoch 489/1000\n",
      "1152/1152 [==============================] - 0s 318us/step - loss: 0.6553 - acc: 0.7847 - val_loss: 1.2350 - val_acc: 0.5694\n",
      "Epoch 490/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.6664 - acc: 0.7769 - val_loss: 1.2293 - val_acc: 0.5833\n",
      "Epoch 491/1000\n",
      "1152/1152 [==============================] - 1s 481us/step - loss: 0.6379 - acc: 0.7752 - val_loss: 1.2102 - val_acc: 0.5764\n",
      "Epoch 492/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.6452 - acc: 0.7726 - val_loss: 1.2435 - val_acc: 0.5833\n",
      "Epoch 493/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.6638 - acc: 0.7812 - val_loss: 1.2253 - val_acc: 0.5903\n",
      "Epoch 494/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.6453 - acc: 0.7795 - val_loss: 1.2287 - val_acc: 0.6076\n",
      "Epoch 495/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6382 - acc: 0.7865 - val_loss: 1.2596 - val_acc: 0.5729\n",
      "Epoch 496/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.6516 - acc: 0.7786 - val_loss: 1.2539 - val_acc: 0.5590\n",
      "Epoch 497/1000\n",
      "1152/1152 [==============================] - 0s 391us/step - loss: 0.6429 - acc: 0.7882 - val_loss: 1.2254 - val_acc: 0.5660\n",
      "Epoch 498/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.6328 - acc: 0.7882 - val_loss: 1.2392 - val_acc: 0.5590\n",
      "Epoch 499/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.6373 - acc: 0.7908 - val_loss: 1.2165 - val_acc: 0.5868\n",
      "Epoch 500/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6420 - acc: 0.7769 - val_loss: 1.2174 - val_acc: 0.5694\n",
      "Epoch 501/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.6184 - acc: 0.7969 - val_loss: 1.2657 - val_acc: 0.5764\n",
      "Epoch 502/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.6219 - acc: 0.7847 - val_loss: 1.2451 - val_acc: 0.5972\n",
      "Epoch 503/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.6155 - acc: 0.8047 - val_loss: 1.2177 - val_acc: 0.5729\n",
      "Epoch 504/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.6604 - acc: 0.7769 - val_loss: 1.2721 - val_acc: 0.5590\n",
      "Epoch 505/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.6182 - acc: 0.7943 - val_loss: 1.2486 - val_acc: 0.5729\n",
      "Epoch 506/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.6533 - acc: 0.7691 - val_loss: 1.2640 - val_acc: 0.5625\n",
      "Epoch 507/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.6343 - acc: 0.7821 - val_loss: 1.2341 - val_acc: 0.5833\n",
      "Epoch 508/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.6251 - acc: 0.7969 - val_loss: 1.2315 - val_acc: 0.5868\n",
      "Epoch 509/1000\n",
      "1152/1152 [==============================] - 0s 314us/step - loss: 0.6227 - acc: 0.7882 - val_loss: 1.2347 - val_acc: 0.5694\n",
      "Epoch 510/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.6238 - acc: 0.8056 - val_loss: 1.2353 - val_acc: 0.5799\n",
      "Epoch 511/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 0.6225 - acc: 0.8003 - val_loss: 1.2307 - val_acc: 0.5799\n",
      "Epoch 512/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.6367 - acc: 0.7821 - val_loss: 1.2393 - val_acc: 0.5903\n",
      "Epoch 513/1000\n",
      "1152/1152 [==============================] - 0s 321us/step - loss: 0.6050 - acc: 0.7882 - val_loss: 1.2093 - val_acc: 0.5799\n",
      "Epoch 514/1000\n",
      "1152/1152 [==============================] - 0s 317us/step - loss: 0.6136 - acc: 0.7943 - val_loss: 1.2263 - val_acc: 0.6007\n",
      "Epoch 515/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5992 - acc: 0.8003 - val_loss: 1.2236 - val_acc: 0.5764\n",
      "Epoch 516/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.6126 - acc: 0.7847 - val_loss: 1.2433 - val_acc: 0.5625\n",
      "Epoch 517/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.6255 - acc: 0.7674 - val_loss: 1.2346 - val_acc: 0.5729\n",
      "Epoch 518/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.6297 - acc: 0.7995 - val_loss: 1.2478 - val_acc: 0.5833\n",
      "Epoch 519/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.6165 - acc: 0.7969 - val_loss: 1.2233 - val_acc: 0.5799\n",
      "Epoch 520/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.6217 - acc: 0.7951 - val_loss: 1.2371 - val_acc: 0.5868\n",
      "Epoch 521/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.6162 - acc: 0.7821 - val_loss: 1.2143 - val_acc: 0.5903\n",
      "Epoch 522/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.6029 - acc: 0.7943 - val_loss: 1.2297 - val_acc: 0.5660\n",
      "Epoch 523/1000\n",
      "1152/1152 [==============================] - 1s 455us/step - loss: 0.6173 - acc: 0.7899 - val_loss: 1.2410 - val_acc: 0.6076\n",
      "Epoch 524/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.6067 - acc: 0.7960 - val_loss: 1.2569 - val_acc: 0.5903\n",
      "Epoch 525/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.5901 - acc: 0.8047 - val_loss: 1.2430 - val_acc: 0.5729\n",
      "Epoch 526/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.6045 - acc: 0.7977 - val_loss: 1.2236 - val_acc: 0.5938\n",
      "Epoch 527/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.5860 - acc: 0.8030 - val_loss: 1.2292 - val_acc: 0.5868\n",
      "Epoch 528/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.5772 - acc: 0.8090 - val_loss: 1.2681 - val_acc: 0.5556\n",
      "Epoch 529/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.5961 - acc: 0.7969 - val_loss: 1.2645 - val_acc: 0.5660\n",
      "Epoch 530/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.5811 - acc: 0.8160 - val_loss: 1.2402 - val_acc: 0.6007\n",
      "Epoch 531/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.5976 - acc: 0.7977 - val_loss: 1.2395 - val_acc: 0.5799\n",
      "Epoch 532/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.5811 - acc: 0.8099 - val_loss: 1.2488 - val_acc: 0.5729\n",
      "Epoch 533/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5942 - acc: 0.8021 - val_loss: 1.2313 - val_acc: 0.5903\n",
      "Epoch 534/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 0.5929 - acc: 0.7934 - val_loss: 1.2743 - val_acc: 0.5625\n",
      "Epoch 535/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.5858 - acc: 0.7986 - val_loss: 1.2021 - val_acc: 0.5486\n",
      "Epoch 536/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.5814 - acc: 0.8056 - val_loss: 1.2462 - val_acc: 0.5764\n",
      "Epoch 537/1000\n",
      "1152/1152 [==============================] - 0s 401us/step - loss: 0.5827 - acc: 0.8047 - val_loss: 1.2675 - val_acc: 0.5868\n",
      "Epoch 538/1000\n",
      "1152/1152 [==============================] - 1s 485us/step - loss: 0.5759 - acc: 0.7977 - val_loss: 1.2467 - val_acc: 0.5694\n",
      "Epoch 539/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.5839 - acc: 0.7882 - val_loss: 1.2380 - val_acc: 0.5833\n",
      "Epoch 540/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.5696 - acc: 0.8125 - val_loss: 1.2614 - val_acc: 0.5764\n",
      "Epoch 541/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.5799 - acc: 0.8003 - val_loss: 1.2351 - val_acc: 0.5764\n",
      "Epoch 542/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5656 - acc: 0.8134 - val_loss: 1.2048 - val_acc: 0.5694\n",
      "Epoch 543/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.5680 - acc: 0.8177 - val_loss: 1.2713 - val_acc: 0.5833\n",
      "Epoch 544/1000\n",
      "1152/1152 [==============================] - 1s 435us/step - loss: 0.5994 - acc: 0.7925 - val_loss: 1.2311 - val_acc: 0.5694\n",
      "Epoch 545/1000\n",
      "1152/1152 [==============================] - 1s 442us/step - loss: 0.5736 - acc: 0.8099 - val_loss: 1.1927 - val_acc: 0.5694\n",
      "Epoch 546/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.5796 - acc: 0.7960 - val_loss: 1.2714 - val_acc: 0.5521\n",
      "Epoch 547/1000\n",
      "1152/1152 [==============================] - 0s 418us/step - loss: 0.5583 - acc: 0.8160 - val_loss: 1.2261 - val_acc: 0.5868\n",
      "Epoch 548/1000\n",
      "1152/1152 [==============================] - 0s 396us/step - loss: 0.5600 - acc: 0.8056 - val_loss: 1.1897 - val_acc: 0.6076\n",
      "Epoch 549/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5662 - acc: 0.7995 - val_loss: 1.2329 - val_acc: 0.5868\n",
      "Epoch 550/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5664 - acc: 0.8168 - val_loss: 1.2321 - val_acc: 0.5903\n",
      "Epoch 551/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.5594 - acc: 0.8177 - val_loss: 1.2509 - val_acc: 0.5729\n",
      "Epoch 552/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.5529 - acc: 0.8177 - val_loss: 1.2394 - val_acc: 0.5764\n",
      "Epoch 553/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.5503 - acc: 0.8194 - val_loss: 1.2382 - val_acc: 0.5764\n",
      "Epoch 554/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5560 - acc: 0.8082 - val_loss: 1.2093 - val_acc: 0.6076\n",
      "Epoch 555/1000\n",
      "1152/1152 [==============================] - 0s 415us/step - loss: 0.5544 - acc: 0.8047 - val_loss: 1.2483 - val_acc: 0.5903\n",
      "Epoch 556/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.5698 - acc: 0.8082 - val_loss: 1.2291 - val_acc: 0.5868\n",
      "Epoch 557/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.5401 - acc: 0.8194 - val_loss: 1.2293 - val_acc: 0.5903\n",
      "Epoch 558/1000\n",
      "1152/1152 [==============================] - 0s 405us/step - loss: 0.5453 - acc: 0.8281 - val_loss: 1.2358 - val_acc: 0.5868\n",
      "Epoch 559/1000\n",
      "1152/1152 [==============================] - 0s 341us/step - loss: 0.5487 - acc: 0.8168 - val_loss: 1.2571 - val_acc: 0.5799\n",
      "Epoch 560/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.5514 - acc: 0.8255 - val_loss: 1.2544 - val_acc: 0.5903\n",
      "Epoch 561/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.5474 - acc: 0.8177 - val_loss: 1.2189 - val_acc: 0.5938\n",
      "Epoch 562/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.5750 - acc: 0.8064 - val_loss: 1.2506 - val_acc: 0.5764\n",
      "Epoch 563/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.5617 - acc: 0.8134 - val_loss: 1.2589 - val_acc: 0.5660\n",
      "Epoch 564/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.5484 - acc: 0.8142 - val_loss: 1.2545 - val_acc: 0.5799\n",
      "Epoch 565/1000\n",
      "1152/1152 [==============================] - 0s 320us/step - loss: 0.5364 - acc: 0.8220 - val_loss: 1.2433 - val_acc: 0.5799\n",
      "Epoch 566/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.5442 - acc: 0.8108 - val_loss: 1.2341 - val_acc: 0.5694\n",
      "Epoch 567/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.5658 - acc: 0.8099 - val_loss: 1.2361 - val_acc: 0.5764\n",
      "Epoch 568/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.5691 - acc: 0.8160 - val_loss: 1.2394 - val_acc: 0.5972\n",
      "Epoch 569/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.5585 - acc: 0.8125 - val_loss: 1.2502 - val_acc: 0.5660\n",
      "Epoch 570/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.5470 - acc: 0.8125 - val_loss: 1.2394 - val_acc: 0.5694\n",
      "Epoch 571/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5463 - acc: 0.8229 - val_loss: 1.2536 - val_acc: 0.5833\n",
      "Epoch 572/1000\n",
      "1152/1152 [==============================] - 1s 467us/step - loss: 0.5302 - acc: 0.8316 - val_loss: 1.2228 - val_acc: 0.5938\n",
      "Epoch 573/1000\n",
      "1152/1152 [==============================] - 1s 448us/step - loss: 0.5250 - acc: 0.8325 - val_loss: 1.2520 - val_acc: 0.6042\n",
      "Epoch 574/1000\n",
      "1152/1152 [==============================] - 1s 475us/step - loss: 0.5519 - acc: 0.8047 - val_loss: 1.2785 - val_acc: 0.5868\n",
      "Epoch 575/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.5361 - acc: 0.8134 - val_loss: 1.2901 - val_acc: 0.5729\n",
      "Epoch 576/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.5434 - acc: 0.8160 - val_loss: 1.2631 - val_acc: 0.5972\n",
      "Epoch 577/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.5586 - acc: 0.8134 - val_loss: 1.2152 - val_acc: 0.5660\n",
      "Epoch 578/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.5137 - acc: 0.8307 - val_loss: 1.2814 - val_acc: 0.5729\n",
      "Epoch 579/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.5633 - acc: 0.8108 - val_loss: 1.2584 - val_acc: 0.5660\n",
      "Epoch 580/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.5377 - acc: 0.8142 - val_loss: 1.2424 - val_acc: 0.5938\n",
      "Epoch 581/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.5433 - acc: 0.8151 - val_loss: 1.2639 - val_acc: 0.5729\n",
      "Epoch 582/1000\n",
      "1152/1152 [==============================] - 1s 464us/step - loss: 0.5258 - acc: 0.8307 - val_loss: 1.2582 - val_acc: 0.5729\n",
      "Epoch 583/1000\n",
      "1152/1152 [==============================] - 1s 503us/step - loss: 0.5102 - acc: 0.8342 - val_loss: 1.2357 - val_acc: 0.5903\n",
      "Epoch 584/1000\n",
      "1152/1152 [==============================] - 0s 427us/step - loss: 0.5217 - acc: 0.8229 - val_loss: 1.2498 - val_acc: 0.5833\n",
      "Epoch 585/1000\n",
      "1152/1152 [==============================] - 0s 400us/step - loss: 0.5549 - acc: 0.8064 - val_loss: 1.2427 - val_acc: 0.5799\n",
      "Epoch 586/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 0.5055 - acc: 0.8342 - val_loss: 1.2723 - val_acc: 0.5868\n",
      "Epoch 587/1000\n",
      "1152/1152 [==============================] - 0s 413us/step - loss: 0.5432 - acc: 0.8082 - val_loss: 1.2322 - val_acc: 0.6007\n",
      "Epoch 588/1000\n",
      "1152/1152 [==============================] - 1s 473us/step - loss: 0.5199 - acc: 0.8212 - val_loss: 1.2330 - val_acc: 0.5938\n",
      "Epoch 589/1000\n",
      "1152/1152 [==============================] - 1s 445us/step - loss: 0.5233 - acc: 0.8316 - val_loss: 1.2291 - val_acc: 0.6007\n",
      "Epoch 590/1000\n",
      "1152/1152 [==============================] - 1s 436us/step - loss: 0.5224 - acc: 0.8177 - val_loss: 1.2681 - val_acc: 0.5764\n",
      "Epoch 591/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.5202 - acc: 0.8342 - val_loss: 1.2657 - val_acc: 0.5729\n",
      "Epoch 592/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.4978 - acc: 0.8229 - val_loss: 1.2512 - val_acc: 0.6076\n",
      "Epoch 593/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.5269 - acc: 0.8290 - val_loss: 1.2528 - val_acc: 0.6042\n",
      "Epoch 594/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.5011 - acc: 0.8186 - val_loss: 1.2493 - val_acc: 0.6007\n",
      "Epoch 595/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.5163 - acc: 0.8281 - val_loss: 1.2565 - val_acc: 0.5764\n",
      "Epoch 596/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.5267 - acc: 0.8273 - val_loss: 1.2389 - val_acc: 0.5833\n",
      "Epoch 597/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.5142 - acc: 0.8325 - val_loss: 1.2257 - val_acc: 0.6007\n",
      "Epoch 598/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.5056 - acc: 0.8316 - val_loss: 1.2733 - val_acc: 0.5660\n",
      "Epoch 599/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.5012 - acc: 0.8255 - val_loss: 1.2515 - val_acc: 0.5938\n",
      "Epoch 600/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.5078 - acc: 0.8316 - val_loss: 1.2308 - val_acc: 0.5764\n",
      "Epoch 601/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.5170 - acc: 0.8290 - val_loss: 1.2540 - val_acc: 0.5694\n",
      "Epoch 602/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 0.4972 - acc: 0.8377 - val_loss: 1.2589 - val_acc: 0.5833\n",
      "Epoch 603/1000\n",
      "1152/1152 [==============================] - 1s 497us/step - loss: 0.5133 - acc: 0.8160 - val_loss: 1.2304 - val_acc: 0.5799\n",
      "Epoch 604/1000\n",
      "1152/1152 [==============================] - 0s 421us/step - loss: 0.4836 - acc: 0.8481 - val_loss: 1.2573 - val_acc: 0.6042\n",
      "Epoch 605/1000\n",
      "1152/1152 [==============================] - 1s 458us/step - loss: 0.5050 - acc: 0.8238 - val_loss: 1.2511 - val_acc: 0.5972\n",
      "Epoch 606/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 0.5198 - acc: 0.8247 - val_loss: 1.2318 - val_acc: 0.5938\n",
      "Epoch 607/1000\n",
      "1152/1152 [==============================] - 1s 513us/step - loss: 0.5105 - acc: 0.8394 - val_loss: 1.2490 - val_acc: 0.5903\n",
      "Epoch 608/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.4899 - acc: 0.8307 - val_loss: 1.2954 - val_acc: 0.5799\n",
      "Epoch 609/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.5062 - acc: 0.8307 - val_loss: 1.2153 - val_acc: 0.6250\n",
      "Epoch 610/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.5109 - acc: 0.8325 - val_loss: 1.2733 - val_acc: 0.5799\n",
      "Epoch 611/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.4897 - acc: 0.8429 - val_loss: 1.2624 - val_acc: 0.6042\n",
      "Epoch 612/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.4856 - acc: 0.8507 - val_loss: 1.2702 - val_acc: 0.5938\n",
      "Epoch 613/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.5059 - acc: 0.8368 - val_loss: 1.2743 - val_acc: 0.5868\n",
      "Epoch 614/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.4945 - acc: 0.8351 - val_loss: 1.3047 - val_acc: 0.5764\n",
      "Epoch 615/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.4905 - acc: 0.8351 - val_loss: 1.2398 - val_acc: 0.5868\n",
      "Epoch 616/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.5001 - acc: 0.8333 - val_loss: 1.2356 - val_acc: 0.5833\n",
      "Epoch 617/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.5027 - acc: 0.8385 - val_loss: 1.2045 - val_acc: 0.5868\n",
      "Epoch 618/1000\n",
      "1152/1152 [==============================] - 0s 417us/step - loss: 0.5006 - acc: 0.8281 - val_loss: 1.2071 - val_acc: 0.6111\n",
      "Epoch 619/1000\n",
      "1152/1152 [==============================] - 1s 448us/step - loss: 0.5076 - acc: 0.8264 - val_loss: 1.3241 - val_acc: 0.5660\n",
      "Epoch 620/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4787 - acc: 0.8507 - val_loss: 1.2591 - val_acc: 0.6076\n",
      "Epoch 621/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.5253 - acc: 0.8142 - val_loss: 1.2628 - val_acc: 0.5833\n",
      "Epoch 622/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.4950 - acc: 0.8316 - val_loss: 1.2666 - val_acc: 0.5972\n",
      "Epoch 623/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.4949 - acc: 0.8351 - val_loss: 1.2316 - val_acc: 0.6007\n",
      "Epoch 624/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4824 - acc: 0.8377 - val_loss: 1.2625 - val_acc: 0.5799\n",
      "Epoch 625/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.4867 - acc: 0.8368 - val_loss: 1.2371 - val_acc: 0.5972\n",
      "Epoch 626/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.5045 - acc: 0.8307 - val_loss: 1.2860 - val_acc: 0.5903\n",
      "Epoch 627/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.4644 - acc: 0.8472 - val_loss: 1.2720 - val_acc: 0.5868\n",
      "Epoch 628/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.4639 - acc: 0.8594 - val_loss: 1.2939 - val_acc: 0.5799\n",
      "Epoch 629/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.4978 - acc: 0.8273 - val_loss: 1.2851 - val_acc: 0.6007\n",
      "Epoch 630/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.4756 - acc: 0.8542 - val_loss: 1.2731 - val_acc: 0.5903\n",
      "Epoch 631/1000\n",
      "1152/1152 [==============================] - 1s 501us/step - loss: 0.4791 - acc: 0.8316 - val_loss: 1.2601 - val_acc: 0.5903\n",
      "Epoch 632/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.4705 - acc: 0.8559 - val_loss: 1.2259 - val_acc: 0.5938\n",
      "Epoch 633/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.4839 - acc: 0.8351 - val_loss: 1.2534 - val_acc: 0.5903\n",
      "Epoch 634/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.4733 - acc: 0.8420 - val_loss: 1.3049 - val_acc: 0.5694\n",
      "Epoch 635/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.4747 - acc: 0.8351 - val_loss: 1.2523 - val_acc: 0.5972\n",
      "Epoch 636/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.4822 - acc: 0.8438 - val_loss: 1.2747 - val_acc: 0.5938\n",
      "Epoch 637/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.4734 - acc: 0.8455 - val_loss: 1.2578 - val_acc: 0.6007\n",
      "Epoch 638/1000\n",
      "1152/1152 [==============================] - 1s 624us/step - loss: 0.4683 - acc: 0.8438 - val_loss: 1.2663 - val_acc: 0.5903\n",
      "Epoch 639/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 0.4690 - acc: 0.8464 - val_loss: 1.2628 - val_acc: 0.6007\n",
      "Epoch 640/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.4921 - acc: 0.8359 - val_loss: 1.2849 - val_acc: 0.5764\n",
      "Epoch 641/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.4761 - acc: 0.8438 - val_loss: 1.2851 - val_acc: 0.5972\n",
      "Epoch 642/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.4844 - acc: 0.8446 - val_loss: 1.2674 - val_acc: 0.5903\n",
      "Epoch 643/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.4813 - acc: 0.8377 - val_loss: 1.2479 - val_acc: 0.5972\n",
      "Epoch 644/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.4734 - acc: 0.8446 - val_loss: 1.2908 - val_acc: 0.5903\n",
      "Epoch 645/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.4470 - acc: 0.8524 - val_loss: 1.2995 - val_acc: 0.5972\n",
      "Epoch 646/1000\n",
      "1152/1152 [==============================] - 1s 445us/step - loss: 0.4503 - acc: 0.8602 - val_loss: 1.2597 - val_acc: 0.6042\n",
      "Epoch 647/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.4591 - acc: 0.8507 - val_loss: 1.2627 - val_acc: 0.6076\n",
      "Epoch 648/1000\n",
      "1152/1152 [==============================] - 1s 438us/step - loss: 0.4586 - acc: 0.8550 - val_loss: 1.2771 - val_acc: 0.5799\n",
      "Epoch 649/1000\n",
      "1152/1152 [==============================] - 1s 452us/step - loss: 0.4401 - acc: 0.8646 - val_loss: 1.3017 - val_acc: 0.5833\n",
      "Epoch 650/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.4680 - acc: 0.8385 - val_loss: 1.2676 - val_acc: 0.6042\n",
      "Epoch 651/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.4531 - acc: 0.8594 - val_loss: 1.3195 - val_acc: 0.5764\n",
      "Epoch 652/1000\n",
      "1152/1152 [==============================] - 0s 431us/step - loss: 0.4707 - acc: 0.8342 - val_loss: 1.2508 - val_acc: 0.6007\n",
      "Epoch 653/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.4670 - acc: 0.8507 - val_loss: 1.2360 - val_acc: 0.6076\n",
      "Epoch 654/1000\n",
      "1152/1152 [==============================] - 1s 444us/step - loss: 0.4362 - acc: 0.8516 - val_loss: 1.2827 - val_acc: 0.6007\n",
      "Epoch 655/1000\n",
      "1152/1152 [==============================] - 0s 423us/step - loss: 0.4725 - acc: 0.8394 - val_loss: 1.2394 - val_acc: 0.6042\n",
      "Epoch 656/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.4535 - acc: 0.8481 - val_loss: 1.2393 - val_acc: 0.5868\n",
      "Epoch 657/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.4254 - acc: 0.8663 - val_loss: 1.2650 - val_acc: 0.6042\n",
      "Epoch 658/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.4500 - acc: 0.8672 - val_loss: 1.2326 - val_acc: 0.6215\n",
      "Epoch 659/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.4424 - acc: 0.8637 - val_loss: 1.2648 - val_acc: 0.6076\n",
      "Epoch 660/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.4656 - acc: 0.8394 - val_loss: 1.2607 - val_acc: 0.5972\n",
      "Epoch 661/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.4495 - acc: 0.8542 - val_loss: 1.2606 - val_acc: 0.5972\n",
      "Epoch 662/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.4476 - acc: 0.8559 - val_loss: 1.2518 - val_acc: 0.6007\n",
      "Epoch 663/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.4318 - acc: 0.8490 - val_loss: 1.2817 - val_acc: 0.5868\n",
      "Epoch 664/1000\n",
      "1152/1152 [==============================] - ETA: 0s - loss: 0.4266 - acc: 0.852 - 1s 555us/step - loss: 0.4284 - acc: 0.8507 - val_loss: 1.2300 - val_acc: 0.6042\n",
      "Epoch 665/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.4476 - acc: 0.8576 - val_loss: 1.2763 - val_acc: 0.5833\n",
      "Epoch 666/1000\n",
      "1152/1152 [==============================] - 1s 520us/step - loss: 0.4261 - acc: 0.8594 - val_loss: 1.2553 - val_acc: 0.6007\n",
      "Epoch 667/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.4616 - acc: 0.8342 - val_loss: 1.2839 - val_acc: 0.5833\n",
      "Epoch 668/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4414 - acc: 0.8542 - val_loss: 1.3490 - val_acc: 0.5729\n",
      "Epoch 669/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.4342 - acc: 0.8576 - val_loss: 1.2905 - val_acc: 0.6111\n",
      "Epoch 670/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.4474 - acc: 0.8542 - val_loss: 1.3166 - val_acc: 0.5972\n",
      "Epoch 671/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.4454 - acc: 0.8550 - val_loss: 1.3191 - val_acc: 0.5868\n",
      "Epoch 672/1000\n",
      "1152/1152 [==============================] - 0s 343us/step - loss: 0.4419 - acc: 0.8550 - val_loss: 1.2547 - val_acc: 0.5833\n",
      "Epoch 673/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.4401 - acc: 0.8602 - val_loss: 1.2776 - val_acc: 0.6076\n",
      "Epoch 674/1000\n",
      "1152/1152 [==============================] - 0s 336us/step - loss: 0.4554 - acc: 0.8385 - val_loss: 1.3169 - val_acc: 0.5868\n",
      "Epoch 675/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4289 - acc: 0.8559 - val_loss: 1.2504 - val_acc: 0.5903\n",
      "Epoch 676/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4376 - acc: 0.8498 - val_loss: 1.3055 - val_acc: 0.6076\n",
      "Epoch 677/1000\n",
      "1152/1152 [==============================] - 0s 328us/step - loss: 0.4310 - acc: 0.8663 - val_loss: 1.2571 - val_acc: 0.6007\n",
      "Epoch 678/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.4310 - acc: 0.8568 - val_loss: 1.2466 - val_acc: 0.5972\n",
      "Epoch 679/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.4604 - acc: 0.8455 - val_loss: 1.2839 - val_acc: 0.6007\n",
      "Epoch 680/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.4465 - acc: 0.8490 - val_loss: 1.3387 - val_acc: 0.5938\n",
      "Epoch 681/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.4314 - acc: 0.8602 - val_loss: 1.3031 - val_acc: 0.5868\n",
      "Epoch 682/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.4363 - acc: 0.8550 - val_loss: 1.2769 - val_acc: 0.5972\n",
      "Epoch 683/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.4113 - acc: 0.8733 - val_loss: 1.2731 - val_acc: 0.5833\n",
      "Epoch 684/1000\n",
      "1152/1152 [==============================] - 0s 323us/step - loss: 0.4470 - acc: 0.8559 - val_loss: 1.2765 - val_acc: 0.5903\n",
      "Epoch 685/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.4559 - acc: 0.8446 - val_loss: 1.2675 - val_acc: 0.6146\n",
      "Epoch 686/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.4232 - acc: 0.8707 - val_loss: 1.2521 - val_acc: 0.6146\n",
      "Epoch 687/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.4081 - acc: 0.8663 - val_loss: 1.3107 - val_acc: 0.5938\n",
      "Epoch 688/1000\n",
      "1152/1152 [==============================] - 0s 334us/step - loss: 0.4238 - acc: 0.8628 - val_loss: 1.2561 - val_acc: 0.6076\n",
      "Epoch 689/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.4244 - acc: 0.8602 - val_loss: 1.2672 - val_acc: 0.5938\n",
      "Epoch 690/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.4076 - acc: 0.8655 - val_loss: 1.2512 - val_acc: 0.6111\n",
      "Epoch 691/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.4177 - acc: 0.8628 - val_loss: 1.2704 - val_acc: 0.5972\n",
      "Epoch 692/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4139 - acc: 0.8707 - val_loss: 1.2926 - val_acc: 0.5903\n",
      "Epoch 693/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.4175 - acc: 0.8559 - val_loss: 1.2661 - val_acc: 0.5938\n",
      "Epoch 694/1000\n",
      "1152/1152 [==============================] - 0s 338us/step - loss: 0.4391 - acc: 0.8516 - val_loss: 1.2851 - val_acc: 0.6146\n",
      "Epoch 695/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4323 - acc: 0.8490 - val_loss: 1.3008 - val_acc: 0.6076\n",
      "Epoch 696/1000\n",
      "1152/1152 [==============================] - 0s 324us/step - loss: 0.4084 - acc: 0.8602 - val_loss: 1.2499 - val_acc: 0.6146\n",
      "Epoch 697/1000\n",
      "1152/1152 [==============================] - 0s 326us/step - loss: 0.4169 - acc: 0.8594 - val_loss: 1.2882 - val_acc: 0.5903\n",
      "Epoch 698/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.4315 - acc: 0.8533 - val_loss: 1.2741 - val_acc: 0.6181\n",
      "Epoch 699/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4127 - acc: 0.8750 - val_loss: 1.2775 - val_acc: 0.5903\n",
      "Epoch 700/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.4206 - acc: 0.8559 - val_loss: 1.2608 - val_acc: 0.6007\n",
      "Epoch 701/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.4509 - acc: 0.8464 - val_loss: 1.2873 - val_acc: 0.6042\n",
      "Epoch 702/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.4119 - acc: 0.8568 - val_loss: 1.2533 - val_acc: 0.6146\n",
      "Epoch 703/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.4326 - acc: 0.8611 - val_loss: 1.2612 - val_acc: 0.6076\n",
      "Epoch 704/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.4094 - acc: 0.8637 - val_loss: 1.3211 - val_acc: 0.6007\n",
      "Epoch 705/1000\n",
      "1152/1152 [==============================] - 0s 319us/step - loss: 0.4161 - acc: 0.8542 - val_loss: 1.3084 - val_acc: 0.5868\n",
      "Epoch 706/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.4231 - acc: 0.8628 - val_loss: 1.2621 - val_acc: 0.6042\n",
      "Epoch 707/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.4130 - acc: 0.8663 - val_loss: 1.2734 - val_acc: 0.6111\n",
      "Epoch 708/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.4116 - acc: 0.8689 - val_loss: 1.3007 - val_acc: 0.5833\n",
      "Epoch 709/1000\n",
      "1152/1152 [==============================] - 0s 411us/step - loss: 0.3934 - acc: 0.8767 - val_loss: 1.2673 - val_acc: 0.5972\n",
      "Epoch 710/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.4172 - acc: 0.8576 - val_loss: 1.3047 - val_acc: 0.5799\n",
      "Epoch 711/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 0.3884 - acc: 0.8802 - val_loss: 1.3181 - val_acc: 0.5938\n",
      "Epoch 712/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.3896 - acc: 0.8724 - val_loss: 1.3358 - val_acc: 0.6111\n",
      "Epoch 713/1000\n",
      "1152/1152 [==============================] - 0s 335us/step - loss: 0.3968 - acc: 0.8689 - val_loss: 1.2916 - val_acc: 0.6146\n",
      "Epoch 714/1000\n",
      "1152/1152 [==============================] - 1s 460us/step - loss: 0.4033 - acc: 0.8637 - val_loss: 1.2746 - val_acc: 0.6111\n",
      "Epoch 715/1000\n",
      "1152/1152 [==============================] - 1s 483us/step - loss: 0.3893 - acc: 0.8672 - val_loss: 1.2924 - val_acc: 0.5972\n",
      "Epoch 716/1000\n",
      "1152/1152 [==============================] - 1s 570us/step - loss: 0.4019 - acc: 0.8741 - val_loss: 1.2739 - val_acc: 0.5938\n",
      "Epoch 717/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 0.3866 - acc: 0.8767 - val_loss: 1.3090 - val_acc: 0.6076\n",
      "Epoch 718/1000\n",
      "1152/1152 [==============================] - 0s 325us/step - loss: 0.3987 - acc: 0.8828 - val_loss: 1.3031 - val_acc: 0.5972\n",
      "Epoch 719/1000\n",
      "1152/1152 [==============================] - 0s 332us/step - loss: 0.4005 - acc: 0.8637 - val_loss: 1.3009 - val_acc: 0.6215\n",
      "Epoch 720/1000\n",
      "1152/1152 [==============================] - 0s 322us/step - loss: 0.4058 - acc: 0.8698 - val_loss: 1.2822 - val_acc: 0.6250\n",
      "Epoch 721/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.4156 - acc: 0.8585 - val_loss: 1.3076 - val_acc: 0.6042\n",
      "Epoch 722/1000\n",
      "1152/1152 [==============================] - 0s 327us/step - loss: 0.3837 - acc: 0.8646 - val_loss: 1.2777 - val_acc: 0.6042\n",
      "Epoch 723/1000\n",
      "1152/1152 [==============================] - 0s 333us/step - loss: 0.3925 - acc: 0.8785 - val_loss: 1.2888 - val_acc: 0.6042\n",
      "Epoch 724/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.3839 - acc: 0.8759 - val_loss: 1.2956 - val_acc: 0.6146\n",
      "Epoch 725/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.3819 - acc: 0.8689 - val_loss: 1.2729 - val_acc: 0.6181\n",
      "Epoch 726/1000\n",
      "1152/1152 [==============================] - 0s 396us/step - loss: 0.3914 - acc: 0.8819 - val_loss: 1.3051 - val_acc: 0.6215\n",
      "Epoch 727/1000\n",
      "1152/1152 [==============================] - 0s 387us/step - loss: 0.3867 - acc: 0.8602 - val_loss: 1.2784 - val_acc: 0.5868\n",
      "Epoch 728/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 0.3921 - acc: 0.8637 - val_loss: 1.2952 - val_acc: 0.6076\n",
      "Epoch 729/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.3993 - acc: 0.8663 - val_loss: 1.3186 - val_acc: 0.5833\n",
      "Epoch 730/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3796 - acc: 0.8741 - val_loss: 1.3011 - val_acc: 0.5972\n",
      "Epoch 731/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.3695 - acc: 0.8819 - val_loss: 1.2876 - val_acc: 0.6076\n",
      "Epoch 732/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.3956 - acc: 0.8646 - val_loss: 1.2714 - val_acc: 0.6007\n",
      "Epoch 733/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.3785 - acc: 0.8863 - val_loss: 1.3003 - val_acc: 0.6007\n",
      "Epoch 734/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.3929 - acc: 0.8655 - val_loss: 1.3375 - val_acc: 0.6076\n",
      "Epoch 735/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3949 - acc: 0.8724 - val_loss: 1.2969 - val_acc: 0.6215\n",
      "Epoch 736/1000\n",
      "1152/1152 [==============================] - 1s 460us/step - loss: 0.3818 - acc: 0.8776 - val_loss: 1.3112 - val_acc: 0.6042\n",
      "Epoch 737/1000\n",
      "1152/1152 [==============================] - 0s 331us/step - loss: 0.3792 - acc: 0.8620 - val_loss: 1.2702 - val_acc: 0.6146\n",
      "Epoch 738/1000\n",
      "1152/1152 [==============================] - 0s 337us/step - loss: 0.4011 - acc: 0.8776 - val_loss: 1.3004 - val_acc: 0.5903\n",
      "Epoch 739/1000\n",
      "1152/1152 [==============================] - 0s 329us/step - loss: 0.3686 - acc: 0.8776 - val_loss: 1.2756 - val_acc: 0.6076\n",
      "Epoch 740/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.3820 - acc: 0.8724 - val_loss: 1.3430 - val_acc: 0.5903\n",
      "Epoch 741/1000\n",
      "1152/1152 [==============================] - 0s 330us/step - loss: 0.3750 - acc: 0.8767 - val_loss: 1.3145 - val_acc: 0.6007\n",
      "Epoch 742/1000\n",
      "1152/1152 [==============================] - 1s 442us/step - loss: 0.3768 - acc: 0.8845 - val_loss: 1.2974 - val_acc: 0.6076\n",
      "Epoch 743/1000\n",
      "1152/1152 [==============================] - 0s 346us/step - loss: 0.4000 - acc: 0.8672 - val_loss: 1.3298 - val_acc: 0.6181\n",
      "Epoch 744/1000\n",
      "1152/1152 [==============================] - 0s 339us/step - loss: 0.3711 - acc: 0.8898 - val_loss: 1.3219 - val_acc: 0.6007\n",
      "Epoch 745/1000\n",
      "1152/1152 [==============================] - 0s 395us/step - loss: 0.3739 - acc: 0.8767 - val_loss: 1.2985 - val_acc: 0.6076\n",
      "Epoch 746/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.3837 - acc: 0.8594 - val_loss: 1.3298 - val_acc: 0.5938\n",
      "Epoch 747/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.3488 - acc: 0.8759 - val_loss: 1.2911 - val_acc: 0.6042\n",
      "Epoch 748/1000\n",
      "1152/1152 [==============================] - 0s 382us/step - loss: 0.3500 - acc: 0.8845 - val_loss: 1.3399 - val_acc: 0.5972\n",
      "Epoch 749/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.3674 - acc: 0.8759 - val_loss: 1.2899 - val_acc: 0.6007\n",
      "Epoch 750/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 0.3631 - acc: 0.8802 - val_loss: 1.3240 - val_acc: 0.5903\n",
      "Epoch 751/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3963 - acc: 0.8724 - val_loss: 1.3351 - val_acc: 0.5833\n",
      "Epoch 752/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.3840 - acc: 0.8663 - val_loss: 1.3314 - val_acc: 0.5833\n",
      "Epoch 753/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3732 - acc: 0.8741 - val_loss: 1.3249 - val_acc: 0.6007\n",
      "Epoch 754/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.3565 - acc: 0.8924 - val_loss: 1.3089 - val_acc: 0.6076\n",
      "Epoch 755/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.3737 - acc: 0.8707 - val_loss: 1.2999 - val_acc: 0.6111\n",
      "Epoch 756/1000\n",
      "1152/1152 [==============================] - 0s 342us/step - loss: 0.3701 - acc: 0.8698 - val_loss: 1.3028 - val_acc: 0.6181\n",
      "Epoch 757/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.3245 - acc: 0.8958 - val_loss: 1.3457 - val_acc: 0.6007\n",
      "Epoch 758/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.3534 - acc: 0.8854 - val_loss: 1.3000 - val_acc: 0.6215\n",
      "Epoch 759/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.3627 - acc: 0.8845 - val_loss: 1.2968 - val_acc: 0.6076\n",
      "Epoch 760/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.3811 - acc: 0.8776 - val_loss: 1.3014 - val_acc: 0.6215\n",
      "Epoch 761/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.3445 - acc: 0.8898 - val_loss: 1.3404 - val_acc: 0.6007\n",
      "Epoch 762/1000\n",
      "1152/1152 [==============================] - 0s 386us/step - loss: 0.3690 - acc: 0.8741 - val_loss: 1.2901 - val_acc: 0.6042\n",
      "Epoch 763/1000\n",
      "1152/1152 [==============================] - 0s 407us/step - loss: 0.3488 - acc: 0.8906 - val_loss: 1.3409 - val_acc: 0.6076\n",
      "Epoch 764/1000\n",
      "1152/1152 [==============================] - 1s 485us/step - loss: 0.3548 - acc: 0.8845 - val_loss: 1.3330 - val_acc: 0.6042\n",
      "Epoch 765/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.3499 - acc: 0.8854 - val_loss: 1.3634 - val_acc: 0.6007\n",
      "Epoch 766/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.3520 - acc: 0.8898 - val_loss: 1.3307 - val_acc: 0.6076\n",
      "Epoch 767/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.3557 - acc: 0.8837 - val_loss: 1.3666 - val_acc: 0.6007\n",
      "Epoch 768/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.3397 - acc: 0.8915 - val_loss: 1.3604 - val_acc: 0.6042\n",
      "Epoch 769/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.3495 - acc: 0.8906 - val_loss: 1.3185 - val_acc: 0.6111\n",
      "Epoch 770/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.3606 - acc: 0.8759 - val_loss: 1.3294 - val_acc: 0.6007\n",
      "Epoch 771/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.3568 - acc: 0.8889 - val_loss: 1.3472 - val_acc: 0.5903\n",
      "Epoch 772/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.3687 - acc: 0.8880 - val_loss: 1.3081 - val_acc: 0.6111\n",
      "Epoch 773/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.3412 - acc: 0.8906 - val_loss: 1.3185 - val_acc: 0.6215\n",
      "Epoch 774/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.3532 - acc: 0.8863 - val_loss: 1.3082 - val_acc: 0.6146\n",
      "Epoch 775/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.3476 - acc: 0.8811 - val_loss: 1.3269 - val_acc: 0.6215\n",
      "Epoch 776/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.3752 - acc: 0.8819 - val_loss: 1.3223 - val_acc: 0.5972\n",
      "Epoch 777/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.3638 - acc: 0.8715 - val_loss: 1.3186 - val_acc: 0.5972\n",
      "Epoch 778/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3602 - acc: 0.8872 - val_loss: 1.3001 - val_acc: 0.6076\n",
      "Epoch 779/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.3564 - acc: 0.8932 - val_loss: 1.3565 - val_acc: 0.5972\n",
      "Epoch 780/1000\n",
      "1152/1152 [==============================] - 0s 417us/step - loss: 0.3375 - acc: 0.8880 - val_loss: 1.3078 - val_acc: 0.5938\n",
      "Epoch 781/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 0.3477 - acc: 0.8915 - val_loss: 1.3543 - val_acc: 0.5903\n",
      "Epoch 782/1000\n",
      "1152/1152 [==============================] - 0s 352us/step - loss: 0.3540 - acc: 0.8889 - val_loss: 1.3264 - val_acc: 0.6007\n",
      "Epoch 783/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.3283 - acc: 0.9002 - val_loss: 1.3141 - val_acc: 0.6042\n",
      "Epoch 784/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 0.3629 - acc: 0.8767 - val_loss: 1.3549 - val_acc: 0.5972\n",
      "Epoch 785/1000\n",
      "1152/1152 [==============================] - 0s 397us/step - loss: 0.3398 - acc: 0.8828 - val_loss: 1.3456 - val_acc: 0.5868\n",
      "Epoch 786/1000\n",
      "1152/1152 [==============================] - 0s 340us/step - loss: 0.3650 - acc: 0.8776 - val_loss: 1.3328 - val_acc: 0.6042\n",
      "Epoch 787/1000\n",
      "1152/1152 [==============================] - 1s 446us/step - loss: 0.3551 - acc: 0.8837 - val_loss: 1.3458 - val_acc: 0.6042\n",
      "Epoch 788/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.3697 - acc: 0.8785 - val_loss: 1.3470 - val_acc: 0.5938\n",
      "Epoch 789/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.3393 - acc: 0.8958 - val_loss: 1.3165 - val_acc: 0.6181\n",
      "Epoch 790/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.3654 - acc: 0.8819 - val_loss: 1.3186 - val_acc: 0.6146\n",
      "Epoch 791/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3350 - acc: 0.8863 - val_loss: 1.3114 - val_acc: 0.6007\n",
      "Epoch 792/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.3481 - acc: 0.8898 - val_loss: 1.3036 - val_acc: 0.6181\n",
      "Epoch 793/1000\n",
      "1152/1152 [==============================] - 0s 372us/step - loss: 0.3505 - acc: 0.8889 - val_loss: 1.4277 - val_acc: 0.5833\n",
      "Epoch 794/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.3241 - acc: 0.8950 - val_loss: 1.3697 - val_acc: 0.6007\n",
      "Epoch 795/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3288 - acc: 0.9002 - val_loss: 1.3231 - val_acc: 0.6076\n",
      "Epoch 796/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3453 - acc: 0.8828 - val_loss: 1.3485 - val_acc: 0.6181\n",
      "Epoch 797/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.3436 - acc: 0.8863 - val_loss: 1.3541 - val_acc: 0.6111\n",
      "Epoch 798/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.3438 - acc: 0.8845 - val_loss: 1.3154 - val_acc: 0.6111\n",
      "Epoch 799/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 0.3597 - acc: 0.8750 - val_loss: 1.3491 - val_acc: 0.6007\n",
      "Epoch 800/1000\n",
      "1152/1152 [==============================] - 0s 418us/step - loss: 0.3446 - acc: 0.8845 - val_loss: 1.3061 - val_acc: 0.5938\n",
      "Epoch 801/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 0.3452 - acc: 0.8828 - val_loss: 1.3186 - val_acc: 0.6042\n",
      "Epoch 802/1000\n",
      "1152/1152 [==============================] - 0s 424us/step - loss: 0.3435 - acc: 0.8880 - val_loss: 1.3046 - val_acc: 0.6250\n",
      "Epoch 803/1000\n",
      "1152/1152 [==============================] - 0s 398us/step - loss: 0.3469 - acc: 0.8906 - val_loss: 1.3908 - val_acc: 0.5972\n",
      "Epoch 804/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.3501 - acc: 0.8837 - val_loss: 1.3859 - val_acc: 0.5938\n",
      "Epoch 805/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.3454 - acc: 0.8872 - val_loss: 1.3356 - val_acc: 0.6285\n",
      "Epoch 806/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.3200 - acc: 0.8915 - val_loss: 1.3152 - val_acc: 0.6181\n",
      "Epoch 807/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.3439 - acc: 0.8863 - val_loss: 1.3469 - val_acc: 0.5938\n",
      "Epoch 808/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.3365 - acc: 0.8915 - val_loss: 1.3291 - val_acc: 0.6076\n",
      "Epoch 809/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.3210 - acc: 0.8976 - val_loss: 1.3707 - val_acc: 0.6042\n",
      "Epoch 810/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3382 - acc: 0.8898 - val_loss: 1.3441 - val_acc: 0.6111\n",
      "Epoch 811/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.3183 - acc: 0.8950 - val_loss: 1.3860 - val_acc: 0.6146\n",
      "Epoch 812/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.3140 - acc: 0.8958 - val_loss: 1.3218 - val_acc: 0.6076\n",
      "Epoch 813/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.3040 - acc: 0.9071 - val_loss: 1.3726 - val_acc: 0.6076\n",
      "Epoch 814/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3197 - acc: 0.9028 - val_loss: 1.3570 - val_acc: 0.6146\n",
      "Epoch 815/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3656 - acc: 0.8698 - val_loss: 1.3117 - val_acc: 0.6111\n",
      "Epoch 816/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3371 - acc: 0.8915 - val_loss: 1.3425 - val_acc: 0.6111\n",
      "Epoch 817/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.3336 - acc: 0.8880 - val_loss: 1.3334 - val_acc: 0.6042\n",
      "Epoch 818/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.3112 - acc: 0.9054 - val_loss: 1.3040 - val_acc: 0.6285\n",
      "Epoch 819/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.2920 - acc: 0.9158 - val_loss: 1.3545 - val_acc: 0.6076\n",
      "Epoch 820/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.3158 - acc: 0.8993 - val_loss: 1.3342 - val_acc: 0.6181\n",
      "Epoch 821/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.3263 - acc: 0.8906 - val_loss: 1.3443 - val_acc: 0.6250\n",
      "Epoch 822/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3169 - acc: 0.9071 - val_loss: 1.3527 - val_acc: 0.6215\n",
      "Epoch 823/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.3273 - acc: 0.9019 - val_loss: 1.3209 - val_acc: 0.6215\n",
      "Epoch 824/1000\n",
      "1152/1152 [==============================] - 0s 344us/step - loss: 0.3181 - acc: 0.8941 - val_loss: 1.3536 - val_acc: 0.6146\n",
      "Epoch 825/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.3357 - acc: 0.8785 - val_loss: 1.3345 - val_acc: 0.6111\n",
      "Epoch 826/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.3196 - acc: 0.8958 - val_loss: 1.3978 - val_acc: 0.6042\n",
      "Epoch 827/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.3218 - acc: 0.9045 - val_loss: 1.3312 - val_acc: 0.6250\n",
      "Epoch 828/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.3114 - acc: 0.9045 - val_loss: 1.3902 - val_acc: 0.6076\n",
      "Epoch 829/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3057 - acc: 0.8984 - val_loss: 1.3560 - val_acc: 0.6146\n",
      "Epoch 830/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.3061 - acc: 0.9010 - val_loss: 1.3991 - val_acc: 0.6042\n",
      "Epoch 831/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.3263 - acc: 0.9002 - val_loss: 1.3477 - val_acc: 0.6146\n",
      "Epoch 832/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.3176 - acc: 0.9045 - val_loss: 1.3563 - val_acc: 0.6146\n",
      "Epoch 833/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3014 - acc: 0.9028 - val_loss: 1.3639 - val_acc: 0.6111\n",
      "Epoch 834/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3168 - acc: 0.8958 - val_loss: 1.3617 - val_acc: 0.6076\n",
      "Epoch 835/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.2931 - acc: 0.9106 - val_loss: 1.3327 - val_acc: 0.6285\n",
      "Epoch 836/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.3345 - acc: 0.8880 - val_loss: 1.3424 - val_acc: 0.6111\n",
      "Epoch 837/1000\n",
      "1152/1152 [==============================] - 0s 390us/step - loss: 0.3218 - acc: 0.9019 - val_loss: 1.3705 - val_acc: 0.6285\n",
      "Epoch 838/1000\n",
      "1152/1152 [==============================] - 0s 394us/step - loss: 0.3246 - acc: 0.9010 - val_loss: 1.3468 - val_acc: 0.6111\n",
      "Epoch 839/1000\n",
      "1152/1152 [==============================] - 0s 409us/step - loss: 0.3107 - acc: 0.9106 - val_loss: 1.3639 - val_acc: 0.6215\n",
      "Epoch 840/1000\n",
      "1152/1152 [==============================] - 0s 399us/step - loss: 0.3046 - acc: 0.8976 - val_loss: 1.3695 - val_acc: 0.6111\n",
      "Epoch 841/1000\n",
      "1152/1152 [==============================] - 0s 422us/step - loss: 0.3163 - acc: 0.8993 - val_loss: 1.3383 - val_acc: 0.6007\n",
      "Epoch 842/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.3184 - acc: 0.8872 - val_loss: 1.4025 - val_acc: 0.6042\n",
      "Epoch 843/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3131 - acc: 0.9036 - val_loss: 1.3416 - val_acc: 0.6215\n",
      "Epoch 844/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.3138 - acc: 0.8941 - val_loss: 1.3297 - val_acc: 0.6042\n",
      "Epoch 845/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.3025 - acc: 0.9045 - val_loss: 1.3280 - val_acc: 0.6181\n",
      "Epoch 846/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3091 - acc: 0.9123 - val_loss: 1.3383 - val_acc: 0.6111\n",
      "Epoch 847/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.3055 - acc: 0.9106 - val_loss: 1.3598 - val_acc: 0.6250\n",
      "Epoch 848/1000\n",
      "1152/1152 [==============================] - 0s 376us/step - loss: 0.3134 - acc: 0.8993 - val_loss: 1.3461 - val_acc: 0.6111\n",
      "Epoch 849/1000\n",
      "1152/1152 [==============================] - 0s 375us/step - loss: 0.3074 - acc: 0.8924 - val_loss: 1.3893 - val_acc: 0.5938\n",
      "Epoch 850/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.3228 - acc: 0.8906 - val_loss: 1.4066 - val_acc: 0.6042\n",
      "Epoch 851/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.3033 - acc: 0.9010 - val_loss: 1.3848 - val_acc: 0.6111\n",
      "Epoch 852/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.2940 - acc: 0.9036 - val_loss: 1.3105 - val_acc: 0.6319\n",
      "Epoch 853/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.3008 - acc: 0.9115 - val_loss: 1.3707 - val_acc: 0.6250\n",
      "Epoch 854/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.3071 - acc: 0.8898 - val_loss: 1.4027 - val_acc: 0.6111\n",
      "Epoch 855/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.3036 - acc: 0.9062 - val_loss: 1.4140 - val_acc: 0.5938\n",
      "Epoch 856/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.3033 - acc: 0.8915 - val_loss: 1.4061 - val_acc: 0.6042\n",
      "Epoch 857/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.2989 - acc: 0.9010 - val_loss: 1.3468 - val_acc: 0.6181\n",
      "Epoch 858/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.3007 - acc: 0.9002 - val_loss: 1.3337 - val_acc: 0.5972\n",
      "Epoch 859/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.3282 - acc: 0.8880 - val_loss: 1.3222 - val_acc: 0.6007\n",
      "Epoch 860/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.2968 - acc: 0.9036 - val_loss: 1.3319 - val_acc: 0.6181\n",
      "Epoch 861/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.2915 - acc: 0.9097 - val_loss: 1.3724 - val_acc: 0.6146\n",
      "Epoch 862/1000\n",
      "1152/1152 [==============================] - 0s 363us/step - loss: 0.3098 - acc: 0.9089 - val_loss: 1.3424 - val_acc: 0.6146\n",
      "Epoch 863/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.2886 - acc: 0.9080 - val_loss: 1.3740 - val_acc: 0.6076\n",
      "Epoch 864/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.3040 - acc: 0.9036 - val_loss: 1.3683 - val_acc: 0.6215\n",
      "Epoch 865/1000\n",
      "1152/1152 [==============================] - 0s 347us/step - loss: 0.2750 - acc: 0.9132 - val_loss: 1.4035 - val_acc: 0.6181\n",
      "Epoch 866/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.2883 - acc: 0.9062 - val_loss: 1.3846 - val_acc: 0.6042\n",
      "Epoch 867/1000\n",
      "1152/1152 [==============================] - 0s 362us/step - loss: 0.2747 - acc: 0.9210 - val_loss: 1.3364 - val_acc: 0.6181\n",
      "Epoch 868/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.3032 - acc: 0.8958 - val_loss: 1.3516 - val_acc: 0.6215\n",
      "Epoch 869/1000\n",
      "1152/1152 [==============================] - 0s 396us/step - loss: 0.3002 - acc: 0.9045 - val_loss: 1.3411 - val_acc: 0.6319\n",
      "Epoch 870/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.2768 - acc: 0.9045 - val_loss: 1.3883 - val_acc: 0.6215\n",
      "Epoch 871/1000\n",
      "1152/1152 [==============================] - 0s 380us/step - loss: 0.2852 - acc: 0.9045 - val_loss: 1.3956 - val_acc: 0.6111\n",
      "Epoch 872/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.2691 - acc: 0.9115 - val_loss: 1.4312 - val_acc: 0.6076\n",
      "Epoch 873/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.2826 - acc: 0.9045 - val_loss: 1.3877 - val_acc: 0.6042\n",
      "Epoch 874/1000\n",
      "1152/1152 [==============================] - 0s 391us/step - loss: 0.2963 - acc: 0.9054 - val_loss: 1.3565 - val_acc: 0.6181\n",
      "Epoch 875/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.2761 - acc: 0.9115 - val_loss: 1.3370 - val_acc: 0.6111\n",
      "Epoch 876/1000\n",
      "1152/1152 [==============================] - 0s 393us/step - loss: 0.2727 - acc: 0.9167 - val_loss: 1.3422 - val_acc: 0.6250\n",
      "Epoch 877/1000\n",
      "1152/1152 [==============================] - 0s 359us/step - loss: 0.2976 - acc: 0.9002 - val_loss: 1.3370 - val_acc: 0.6146\n",
      "Epoch 878/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.2932 - acc: 0.8984 - val_loss: 1.3534 - val_acc: 0.6250\n",
      "Epoch 879/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.2624 - acc: 0.9115 - val_loss: 1.3512 - val_acc: 0.6146\n",
      "Epoch 880/1000\n",
      "1152/1152 [==============================] - 0s 351us/step - loss: 0.2703 - acc: 0.9123 - val_loss: 1.3529 - val_acc: 0.6215\n",
      "Epoch 881/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.2943 - acc: 0.8976 - val_loss: 1.3891 - val_acc: 0.6111\n",
      "Epoch 882/1000\n",
      "1152/1152 [==============================] - 0s 371us/step - loss: 0.2806 - acc: 0.9167 - val_loss: 1.4071 - val_acc: 0.6076\n",
      "Epoch 883/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.2885 - acc: 0.9071 - val_loss: 1.3195 - val_acc: 0.6319\n",
      "Epoch 884/1000\n",
      "1152/1152 [==============================] - 0s 367us/step - loss: 0.2806 - acc: 0.9175 - val_loss: 1.3944 - val_acc: 0.6146\n",
      "Epoch 885/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.2738 - acc: 0.9210 - val_loss: 1.3820 - val_acc: 0.6146\n",
      "Epoch 886/1000\n",
      "1152/1152 [==============================] - 0s 348us/step - loss: 0.2634 - acc: 0.9219 - val_loss: 1.4060 - val_acc: 0.6007\n",
      "Epoch 887/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.2832 - acc: 0.9106 - val_loss: 1.3811 - val_acc: 0.6076\n",
      "Epoch 888/1000\n",
      "1152/1152 [==============================] - 0s 410us/step - loss: 0.2787 - acc: 0.9149 - val_loss: 1.4231 - val_acc: 0.6007\n",
      "Epoch 889/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.2687 - acc: 0.9080 - val_loss: 1.3804 - val_acc: 0.6181\n",
      "Epoch 890/1000\n",
      "1152/1152 [==============================] - 0s 368us/step - loss: 0.2775 - acc: 0.9062 - val_loss: 1.3904 - val_acc: 0.6007\n",
      "Epoch 891/1000\n",
      "1152/1152 [==============================] - 0s 369us/step - loss: 0.2823 - acc: 0.9071 - val_loss: 1.3808 - val_acc: 0.6076\n",
      "Epoch 892/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.2541 - acc: 0.9184 - val_loss: 1.3688 - val_acc: 0.6215\n",
      "Epoch 893/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.2812 - acc: 0.9141 - val_loss: 1.3532 - val_acc: 0.6215\n",
      "Epoch 894/1000\n",
      "1152/1152 [==============================] - 0s 373us/step - loss: 0.2609 - acc: 0.9253 - val_loss: 1.3788 - val_acc: 0.6285\n",
      "Epoch 895/1000\n",
      "1152/1152 [==============================] - 0s 377us/step - loss: 0.2829 - acc: 0.9132 - val_loss: 1.3950 - val_acc: 0.6285\n",
      "Epoch 896/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.2601 - acc: 0.9175 - val_loss: 1.3702 - val_acc: 0.6111\n",
      "Epoch 897/1000\n",
      "1152/1152 [==============================] - 0s 356us/step - loss: 0.2659 - acc: 0.9219 - val_loss: 1.4577 - val_acc: 0.5972\n",
      "Epoch 898/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.2752 - acc: 0.9071 - val_loss: 1.4551 - val_acc: 0.6042\n",
      "Epoch 899/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.2739 - acc: 0.9193 - val_loss: 1.4275 - val_acc: 0.6076\n",
      "Epoch 900/1000\n",
      "1152/1152 [==============================] - 0s 370us/step - loss: 0.2684 - acc: 0.9115 - val_loss: 1.4059 - val_acc: 0.6007\n",
      "Epoch 901/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.2601 - acc: 0.9089 - val_loss: 1.4564 - val_acc: 0.6042\n",
      "Epoch 902/1000\n",
      "1152/1152 [==============================] - 0s 364us/step - loss: 0.2818 - acc: 0.9106 - val_loss: 1.3955 - val_acc: 0.6042\n",
      "Epoch 903/1000\n",
      "1152/1152 [==============================] - 0s 366us/step - loss: 0.2796 - acc: 0.9028 - val_loss: 1.4634 - val_acc: 0.5938\n",
      "Epoch 904/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.2870 - acc: 0.9062 - val_loss: 1.3915 - val_acc: 0.6111\n",
      "Epoch 905/1000\n",
      "1152/1152 [==============================] - 0s 355us/step - loss: 0.2548 - acc: 0.9201 - val_loss: 1.4187 - val_acc: 0.6146\n",
      "Epoch 906/1000\n",
      "1152/1152 [==============================] - 0s 365us/step - loss: 0.2672 - acc: 0.9132 - val_loss: 1.4220 - val_acc: 0.6007\n",
      "Epoch 907/1000\n",
      "1152/1152 [==============================] - 0s 379us/step - loss: 0.2556 - acc: 0.9132 - val_loss: 1.4106 - val_acc: 0.6146\n",
      "Epoch 908/1000\n",
      "1152/1152 [==============================] - 1s 442us/step - loss: 0.2460 - acc: 0.9314 - val_loss: 1.4396 - val_acc: 0.6007\n",
      "Epoch 909/1000\n",
      "1152/1152 [==============================] - 1s 448us/step - loss: 0.2675 - acc: 0.9115 - val_loss: 1.4469 - val_acc: 0.6215\n",
      "Epoch 910/1000\n",
      "1152/1152 [==============================] - 0s 424us/step - loss: 0.2595 - acc: 0.9193 - val_loss: 1.3781 - val_acc: 0.6354\n",
      "Epoch 911/1000\n",
      "1152/1152 [==============================] - 1s 500us/step - loss: 0.2817 - acc: 0.9158 - val_loss: 1.4305 - val_acc: 0.6146\n",
      "Epoch 912/1000\n",
      "1152/1152 [==============================] - 1s 526us/step - loss: 0.2769 - acc: 0.9132 - val_loss: 1.4051 - val_acc: 0.6215\n",
      "Epoch 913/1000\n",
      "1152/1152 [==============================] - 1s 491us/step - loss: 0.2610 - acc: 0.9158 - val_loss: 1.3615 - val_acc: 0.6146\n",
      "Epoch 914/1000\n",
      "1152/1152 [==============================] - 1s 550us/step - loss: 0.2759 - acc: 0.9158 - val_loss: 1.4226 - val_acc: 0.6076\n",
      "Epoch 915/1000\n",
      "1152/1152 [==============================] - 0s 403us/step - loss: 0.2618 - acc: 0.9123 - val_loss: 1.3919 - val_acc: 0.6146\n",
      "Epoch 916/1000\n",
      "1152/1152 [==============================] - 0s 402us/step - loss: 0.2526 - acc: 0.9149 - val_loss: 1.3875 - val_acc: 0.6181\n",
      "Epoch 917/1000\n",
      "1152/1152 [==============================] - 0s 391us/step - loss: 0.2782 - acc: 0.9115 - val_loss: 1.4361 - val_acc: 0.6215\n",
      "Epoch 918/1000\n",
      "1152/1152 [==============================] - 0s 395us/step - loss: 0.2647 - acc: 0.9236 - val_loss: 1.4283 - val_acc: 0.6250\n",
      "Epoch 919/1000\n",
      "1152/1152 [==============================] - 0s 393us/step - loss: 0.2404 - acc: 0.9245 - val_loss: 1.4117 - val_acc: 0.6111\n",
      "Epoch 920/1000\n",
      "1152/1152 [==============================] - 1s 484us/step - loss: 0.2717 - acc: 0.9201 - val_loss: 1.3906 - val_acc: 0.6285\n",
      "Epoch 921/1000\n",
      "1152/1152 [==============================] - 1s 455us/step - loss: 0.2504 - acc: 0.9184 - val_loss: 1.3888 - val_acc: 0.6042\n",
      "Epoch 922/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.2505 - acc: 0.9253 - val_loss: 1.4323 - val_acc: 0.6146\n",
      "Epoch 923/1000\n",
      "1152/1152 [==============================] - 1s 480us/step - loss: 0.2583 - acc: 0.9193 - val_loss: 1.4045 - val_acc: 0.6146\n",
      "Epoch 924/1000\n",
      "1152/1152 [==============================] - 1s 523us/step - loss: 0.2775 - acc: 0.9062 - val_loss: 1.4242 - val_acc: 0.6076\n",
      "Epoch 925/1000\n",
      "1152/1152 [==============================] - 1s 545us/step - loss: 0.2587 - acc: 0.9175 - val_loss: 1.3895 - val_acc: 0.6181\n",
      "Epoch 926/1000\n",
      "1152/1152 [==============================] - 1s 478us/step - loss: 0.2661 - acc: 0.9132 - val_loss: 1.3914 - val_acc: 0.6146\n",
      "Epoch 927/1000\n",
      "1152/1152 [==============================] - 1s 546us/step - loss: 0.2533 - acc: 0.9219 - val_loss: 1.3944 - val_acc: 0.6146\n",
      "Epoch 928/1000\n",
      "1152/1152 [==============================] - 1s 518us/step - loss: 0.2556 - acc: 0.9158 - val_loss: 1.4320 - val_acc: 0.6285\n",
      "Epoch 929/1000\n",
      "1152/1152 [==============================] - 1s 536us/step - loss: 0.2576 - acc: 0.9201 - val_loss: 1.4219 - val_acc: 0.6215\n",
      "Epoch 930/1000\n",
      "1152/1152 [==============================] - 1s 487us/step - loss: 0.2593 - acc: 0.9245 - val_loss: 1.3966 - val_acc: 0.6076\n",
      "Epoch 931/1000\n",
      "1152/1152 [==============================] - 1s 483us/step - loss: 0.2554 - acc: 0.9184 - val_loss: 1.3772 - val_acc: 0.6354\n",
      "Epoch 932/1000\n",
      "1152/1152 [==============================] - 1s 558us/step - loss: 0.2486 - acc: 0.9219 - val_loss: 1.4117 - val_acc: 0.6181\n",
      "Epoch 933/1000\n",
      "1152/1152 [==============================] - 1s 481us/step - loss: 0.2801 - acc: 0.9080 - val_loss: 1.4065 - val_acc: 0.6181\n",
      "Epoch 934/1000\n",
      "1152/1152 [==============================] - 1s 498us/step - loss: 0.2821 - acc: 0.9132 - val_loss: 1.4111 - val_acc: 0.5938\n",
      "Epoch 935/1000\n",
      "1152/1152 [==============================] - 0s 389us/step - loss: 0.2404 - acc: 0.9288 - val_loss: 1.4114 - val_acc: 0.6250\n",
      "Epoch 936/1000\n",
      "1152/1152 [==============================] - 1s 539us/step - loss: 0.2657 - acc: 0.9036 - val_loss: 1.4950 - val_acc: 0.6042\n",
      "Epoch 937/1000\n",
      "1152/1152 [==============================] - 0s 388us/step - loss: 0.2420 - acc: 0.9280 - val_loss: 1.4533 - val_acc: 0.6181\n",
      "Epoch 938/1000\n",
      "1152/1152 [==============================] - 1s 507us/step - loss: 0.2561 - acc: 0.9141 - val_loss: 1.4125 - val_acc: 0.6285\n",
      "Epoch 939/1000\n",
      "1152/1152 [==============================] - 0s 409us/step - loss: 0.2651 - acc: 0.9184 - val_loss: 1.4247 - val_acc: 0.6181\n",
      "Epoch 940/1000\n",
      "1152/1152 [==============================] - 1s 538us/step - loss: 0.2501 - acc: 0.9193 - val_loss: 1.3955 - val_acc: 0.6319\n",
      "Epoch 941/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 0.2648 - acc: 0.9184 - val_loss: 1.4300 - val_acc: 0.6146\n",
      "Epoch 942/1000\n",
      "1152/1152 [==============================] - 0s 422us/step - loss: 0.2483 - acc: 0.9193 - val_loss: 1.4236 - val_acc: 0.6111\n",
      "Epoch 943/1000\n",
      "1152/1152 [==============================] - 1s 547us/step - loss: 0.2528 - acc: 0.9227 - val_loss: 1.4304 - val_acc: 0.6215\n",
      "Epoch 944/1000\n",
      "1152/1152 [==============================] - 1s 477us/step - loss: 0.2811 - acc: 0.9097 - val_loss: 1.4739 - val_acc: 0.6042\n",
      "Epoch 945/1000\n",
      "1152/1152 [==============================] - 0s 424us/step - loss: 0.2495 - acc: 0.9123 - val_loss: 1.4081 - val_acc: 0.6007\n",
      "Epoch 946/1000\n",
      "1152/1152 [==============================] - 0s 391us/step - loss: 0.2619 - acc: 0.9149 - val_loss: 1.4101 - val_acc: 0.6007\n",
      "Epoch 947/1000\n",
      "1152/1152 [==============================] - 1s 540us/step - loss: 0.2396 - acc: 0.9271 - val_loss: 1.4475 - val_acc: 0.6111\n",
      "Epoch 948/1000\n",
      "1152/1152 [==============================] - 0s 419us/step - loss: 0.2538 - acc: 0.9149 - val_loss: 1.4219 - val_acc: 0.6076\n",
      "Epoch 949/1000\n",
      "1152/1152 [==============================] - 0s 408us/step - loss: 0.2433 - acc: 0.9193 - val_loss: 1.4463 - val_acc: 0.6111\n",
      "Epoch 950/1000\n",
      "1152/1152 [==============================] - 1s 587us/step - loss: 0.2429 - acc: 0.9227 - val_loss: 1.4255 - val_acc: 0.6042\n",
      "Epoch 951/1000\n",
      "1152/1152 [==============================] - 0s 422us/step - loss: 0.2640 - acc: 0.9184 - val_loss: 1.4486 - val_acc: 0.6181\n",
      "Epoch 952/1000\n",
      "1152/1152 [==============================] - 1s 505us/step - loss: 0.2697 - acc: 0.9089 - val_loss: 1.4854 - val_acc: 0.6111\n",
      "Epoch 953/1000\n",
      "1152/1152 [==============================] - 1s 480us/step - loss: 0.2442 - acc: 0.9306 - val_loss: 1.4212 - val_acc: 0.6354\n",
      "Epoch 954/1000\n",
      "1152/1152 [==============================] - 0s 411us/step - loss: 0.2555 - acc: 0.9167 - val_loss: 1.4582 - val_acc: 0.6215\n",
      "Epoch 955/1000\n",
      "1152/1152 [==============================] - 0s 421us/step - loss: 0.2422 - acc: 0.9219 - val_loss: 1.4124 - val_acc: 0.6215\n",
      "Epoch 956/1000\n",
      "1152/1152 [==============================] - 0s 428us/step - loss: 0.2325 - acc: 0.9297 - val_loss: 1.5143 - val_acc: 0.6111\n",
      "Epoch 957/1000\n",
      "1152/1152 [==============================] - 0s 422us/step - loss: 0.2500 - acc: 0.9219 - val_loss: 1.4261 - val_acc: 0.6250\n",
      "Epoch 958/1000\n",
      "1152/1152 [==============================] - 1s 528us/step - loss: 0.2388 - acc: 0.9288 - val_loss: 1.4539 - val_acc: 0.6215\n",
      "Epoch 959/1000\n",
      "1152/1152 [==============================] - 0s 426us/step - loss: 0.2433 - acc: 0.9201 - val_loss: 1.4115 - val_acc: 0.6181\n",
      "Epoch 960/1000\n",
      "1152/1152 [==============================] - 0s 429us/step - loss: 0.2343 - acc: 0.9262 - val_loss: 1.4294 - val_acc: 0.5938\n",
      "Epoch 961/1000\n",
      "1152/1152 [==============================] - 1s 495us/step - loss: 0.2405 - acc: 0.9210 - val_loss: 1.4312 - val_acc: 0.6076\n",
      "Epoch 962/1000\n",
      "1152/1152 [==============================] - 0s 434us/step - loss: 0.2213 - acc: 0.9323 - val_loss: 1.4983 - val_acc: 0.6215\n",
      "Epoch 963/1000\n",
      "1152/1152 [==============================] - 1s 493us/step - loss: 0.2540 - acc: 0.9167 - val_loss: 1.4518 - val_acc: 0.6146\n",
      "Epoch 964/1000\n",
      "1152/1152 [==============================] - 1s 478us/step - loss: 0.2462 - acc: 0.9210 - val_loss: 1.4088 - val_acc: 0.6250\n",
      "Epoch 965/1000\n",
      "1152/1152 [==============================] - 1s 435us/step - loss: 0.2423 - acc: 0.9210 - val_loss: 1.4167 - val_acc: 0.6146\n",
      "Epoch 966/1000\n",
      "1152/1152 [==============================] - 1s 466us/step - loss: 0.2624 - acc: 0.9201 - val_loss: 1.4823 - val_acc: 0.6111\n",
      "Epoch 967/1000\n",
      "1152/1152 [==============================] - 1s 437us/step - loss: 0.2277 - acc: 0.9314 - val_loss: 1.4690 - val_acc: 0.6007\n",
      "Epoch 968/1000\n",
      "1152/1152 [==============================] - 1s 475us/step - loss: 0.2364 - acc: 0.9280 - val_loss: 1.4858 - val_acc: 0.6146\n",
      "Epoch 969/1000\n",
      "1152/1152 [==============================] - 1s 487us/step - loss: 0.2391 - acc: 0.9280 - val_loss: 1.4732 - val_acc: 0.6111\n",
      "Epoch 970/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.2466 - acc: 0.9219 - val_loss: 1.4627 - val_acc: 0.6250\n",
      "Epoch 971/1000\n",
      "1152/1152 [==============================] - 1s 527us/step - loss: 0.2150 - acc: 0.9340 - val_loss: 1.4554 - val_acc: 0.6146\n",
      "Epoch 972/1000\n",
      "1152/1152 [==============================] - 1s 453us/step - loss: 0.2404 - acc: 0.9175 - val_loss: 1.4471 - val_acc: 0.6146\n",
      "Epoch 973/1000\n",
      "1152/1152 [==============================] - 1s 439us/step - loss: 0.2531 - acc: 0.9106 - val_loss: 1.4092 - val_acc: 0.6215\n",
      "Epoch 974/1000\n",
      "1152/1152 [==============================] - 1s 461us/step - loss: 0.2308 - acc: 0.9306 - val_loss: 1.4332 - val_acc: 0.6250\n",
      "Epoch 975/1000\n",
      "1152/1152 [==============================] - 0s 384us/step - loss: 0.2393 - acc: 0.9193 - val_loss: 1.4025 - val_acc: 0.6285\n",
      "Epoch 976/1000\n",
      "1152/1152 [==============================] - 0s 432us/step - loss: 0.2365 - acc: 0.9236 - val_loss: 1.3937 - val_acc: 0.6215\n",
      "Epoch 977/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.2279 - acc: 0.9280 - val_loss: 1.4542 - val_acc: 0.6146\n",
      "Epoch 978/1000\n",
      "1152/1152 [==============================] - 0s 396us/step - loss: 0.2289 - acc: 0.9227 - val_loss: 1.4202 - val_acc: 0.6215\n",
      "Epoch 979/1000\n",
      "1152/1152 [==============================] - 1s 474us/step - loss: 0.2297 - acc: 0.9349 - val_loss: 1.5073 - val_acc: 0.6181\n",
      "Epoch 980/1000\n",
      "1152/1152 [==============================] - 0s 401us/step - loss: 0.2303 - acc: 0.9271 - val_loss: 1.4865 - val_acc: 0.5938\n",
      "Epoch 981/1000\n",
      "1152/1152 [==============================] - 0s 381us/step - loss: 0.2218 - acc: 0.9349 - val_loss: 1.4266 - val_acc: 0.6146\n",
      "Epoch 982/1000\n",
      "1152/1152 [==============================] - 0s 415us/step - loss: 0.2251 - acc: 0.9323 - val_loss: 1.4133 - val_acc: 0.6146\n",
      "Epoch 983/1000\n",
      "1152/1152 [==============================] - 0s 354us/step - loss: 0.2140 - acc: 0.9332 - val_loss: 1.4452 - val_acc: 0.6319\n",
      "Epoch 984/1000\n",
      "1152/1152 [==============================] - 0s 357us/step - loss: 0.2417 - acc: 0.9115 - val_loss: 1.4343 - val_acc: 0.6111\n",
      "Epoch 985/1000\n",
      "1152/1152 [==============================] - 0s 349us/step - loss: 0.2205 - acc: 0.9340 - val_loss: 1.4626 - val_acc: 0.6215\n",
      "Epoch 986/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.2372 - acc: 0.9227 - val_loss: 1.5287 - val_acc: 0.6042\n",
      "Epoch 987/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.2298 - acc: 0.9210 - val_loss: 1.4319 - val_acc: 0.6215\n",
      "Epoch 988/1000\n",
      "1152/1152 [==============================] - 0s 345us/step - loss: 0.2528 - acc: 0.9132 - val_loss: 1.4128 - val_acc: 0.6250\n",
      "Epoch 989/1000\n",
      "1152/1152 [==============================] - 0s 353us/step - loss: 0.2459 - acc: 0.9236 - val_loss: 1.4444 - val_acc: 0.6181\n",
      "Epoch 990/1000\n",
      "1152/1152 [==============================] - 0s 350us/step - loss: 0.2293 - acc: 0.9253 - val_loss: 1.5070 - val_acc: 0.6146\n",
      "Epoch 991/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.2383 - acc: 0.9141 - val_loss: 1.4285 - val_acc: 0.6111\n",
      "Epoch 992/1000\n",
      "1152/1152 [==============================] - 0s 360us/step - loss: 0.2430 - acc: 0.9253 - val_loss: 1.4262 - val_acc: 0.6111\n",
      "Epoch 993/1000\n",
      "1152/1152 [==============================] - 0s 361us/step - loss: 0.2419 - acc: 0.9158 - val_loss: 1.4339 - val_acc: 0.6111\n",
      "Epoch 994/1000\n",
      "1152/1152 [==============================] - 0s 374us/step - loss: 0.2455 - acc: 0.9236 - val_loss: 1.4682 - val_acc: 0.6319\n",
      "Epoch 995/1000\n",
      "1152/1152 [==============================] - 1s 469us/step - loss: 0.2394 - acc: 0.9280 - val_loss: 1.4566 - val_acc: 0.6146\n",
      "Epoch 996/1000\n",
      "1152/1152 [==============================] - 0s 358us/step - loss: 0.2216 - acc: 0.9323 - val_loss: 1.4751 - val_acc: 0.6215\n",
      "Epoch 997/1000\n",
      "1152/1152 [==============================] - 1s 490us/step - loss: 0.2497 - acc: 0.9184 - val_loss: 1.4482 - val_acc: 0.6181\n",
      "Epoch 998/1000\n",
      "1152/1152 [==============================] - 1s 466us/step - loss: 0.2291 - acc: 0.9297 - val_loss: 1.4880 - val_acc: 0.6146\n",
      "Epoch 999/1000\n",
      "1152/1152 [==============================] - 0s 392us/step - loss: 0.2193 - acc: 0.9323 - val_loss: 1.4835 - val_acc: 0.6215\n",
      "Epoch 1000/1000\n",
      "1152/1152 [==============================] - 0s 412us/step - loss: 0.2380 - acc: 0.9193 - val_loss: 1.4663 - val_acc: 0.6250\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_testcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 5, 1, 6, 0, 3, 5, 2, 1, 5, 7, 4, 5, 6, 6, 7, 5, 1, 2, 2, 0,\n",
       "       0, 5, 5, 6, 4, 7, 6, 6, 2, 4, 7, 6, 7, 1, 6, 4, 5, 4, 1, 3, 1, 4,\n",
       "       2, 1, 1, 0, 5, 4, 7, 3, 7, 4, 0, 7, 2, 7, 4, 3, 7, 5, 2, 4, 5, 4,\n",
       "       1, 6, 3, 3, 2, 7, 1, 7, 6, 1, 4, 7, 3, 6, 2, 2, 5, 1, 2, 4, 5, 1,\n",
       "       1, 0, 0, 1, 4, 6, 0, 6, 7, 2, 2, 7, 6, 5, 4, 5, 7, 5, 2, 6, 4, 1,\n",
       "       2, 4, 7, 2, 6, 4, 5, 6, 2, 5, 6, 1, 7, 5, 3, 4, 7, 3, 5, 5, 2, 2,\n",
       "       1, 5, 4, 3, 3, 6, 4, 6, 2, 6, 3, 7, 4, 6, 4, 1, 2, 5, 5, 3, 1, 5,\n",
       "       5, 1, 2, 3, 7, 5, 2, 6, 4, 5, 2, 4, 4, 6, 3, 7, 1, 4, 4, 6, 4, 1,\n",
       "       6, 7, 2, 6, 2, 2, 2, 3, 0, 6, 7, 4, 4, 3, 6, 5, 3, 1, 7, 3, 7, 7,\n",
       "       6, 4, 2, 3, 7, 3, 1, 4, 6, 1, 2, 7, 3, 6, 7, 5, 6, 5, 2, 4, 7, 7,\n",
       "       3, 6, 6, 3, 3, 6, 3, 3, 3, 0, 3, 7, 0, 6, 7, 6, 6, 6, 4, 0, 1, 1,\n",
       "       7, 3, 7, 7, 0, 1, 1, 1, 4, 3, 2, 1, 4, 6, 3, 6, 1, 1, 6, 3, 1, 1,\n",
       "       4, 1, 6, 3, 7, 1, 7, 1, 3, 6, 6, 2, 6, 5, 5, 6, 4, 2, 6, 6, 3, 4,\n",
       "       0, 3], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 1, 1, 4, 1, 1, 5, 7, 1, 6, 7, 3, 5, 3, 6, 4, 5, 1, 6, 7, 0,\n",
       "       0, 3, 5, 3, 7, 7, 6, 4, 7, 4, 2, 5, 7, 1, 6, 4, 2, 4, 1, 0, 1, 4,\n",
       "       7, 1, 1, 0, 5, 3, 3, 7, 7, 4, 0, 1, 1, 7, 2, 0, 7, 5, 7, 7, 5, 2,\n",
       "       1, 6, 6, 3, 2, 7, 1, 2, 7, 1, 4, 7, 3, 6, 2, 2, 7, 1, 2, 4, 5, 1,\n",
       "       2, 1, 0, 1, 4, 4, 0, 6, 7, 2, 2, 7, 6, 5, 4, 5, 3, 7, 2, 6, 5, 0,\n",
       "       7, 4, 7, 1, 6, 4, 5, 6, 6, 5, 6, 1, 7, 5, 3, 4, 7, 2, 5, 5, 2, 2,\n",
       "       3, 5, 2, 3, 1, 1, 6, 6, 4, 4, 2, 2, 4, 2, 4, 1, 7, 5, 5, 3, 1, 5,\n",
       "       2, 0, 5, 3, 7, 5, 3, 4, 4, 3, 2, 0, 4, 1, 3, 7, 1, 5, 4, 6, 5, 1,\n",
       "       4, 7, 2, 3, 2, 5, 2, 3, 0, 0, 7, 4, 4, 3, 6, 5, 3, 1, 3, 5, 7, 7,\n",
       "       6, 1, 5, 3, 7, 3, 1, 4, 6, 3, 2, 7, 2, 6, 7, 5, 0, 5, 4, 4, 7, 7,\n",
       "       3, 7, 6, 5, 3, 6, 1, 3, 3, 0, 3, 7, 3, 6, 7, 4, 4, 6, 4, 0, 6, 1,\n",
       "       7, 0, 2, 2, 0, 1, 1, 3, 4, 3, 4, 1, 4, 6, 3, 7, 1, 1, 4, 1, 1, 1,\n",
       "       4, 0, 6, 7, 7, 1, 2, 0, 3, 7, 3, 4, 4, 2, 4, 2, 4, 2, 3, 6, 3, 6,\n",
       "       2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Ytest = y_test.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 1, 1, 4, 1, 1, 5, 7, 1, 6, 7, 3, 5, 3, 6, 4, 5, 1, 6, 7, 0,\n",
       "       0, 3, 5, 3, 7, 7, 6, 4, 7, 4, 2, 5, 7, 1, 6, 4, 2, 4, 1, 0, 1, 4,\n",
       "       7, 1, 1, 0, 5, 3, 3, 7, 7, 4, 0, 1, 1, 7, 2, 0, 7, 5, 7, 7, 5, 2,\n",
       "       1, 6, 6, 3, 2, 7, 1, 2, 7, 1, 4, 7, 3, 6, 2, 2, 7, 1, 2, 4, 5, 1,\n",
       "       2, 1, 0, 1, 4, 4, 0, 6, 7, 2, 2, 7, 6, 5, 4, 5, 3, 7, 2, 6, 5, 0,\n",
       "       7, 4, 7, 1, 6, 4, 5, 6, 6, 5, 6, 1, 7, 5, 3, 4, 7, 2, 5, 5, 2, 2,\n",
       "       3, 5, 2, 3, 1, 1, 6, 6, 4, 4, 2, 2, 4, 2, 4, 1, 7, 5, 5, 3, 1, 5,\n",
       "       2, 0, 5, 3, 7, 5, 3, 4, 4, 3, 2, 0, 4, 1, 3, 7, 1, 5, 4, 6, 5, 1,\n",
       "       4, 7, 2, 3, 2, 5, 2, 3, 0, 0, 7, 4, 4, 3, 6, 5, 3, 1, 3, 5, 7, 7,\n",
       "       6, 1, 5, 3, 7, 3, 1, 4, 6, 3, 2, 7, 2, 6, 7, 5, 0, 5, 4, 4, 7, 7,\n",
       "       3, 7, 6, 5, 3, 6, 1, 3, 3, 0, 3, 7, 3, 6, 7, 4, 4, 6, 4, 0, 6, 1,\n",
       "       7, 0, 2, 2, 0, 1, 1, 3, 4, 3, 4, 1, 4, 6, 3, 7, 1, 1, 4, 1, 1, 1,\n",
       "       4, 0, 6, 7, 7, 1, 2, 0, 3, 7, 3, 4, 4, 2, 4, 2, 4, 2, 3, 6, 3, 6,\n",
       "       2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.50      0.59        20\n",
      "           1       0.78      0.70      0.74        44\n",
      "           2       0.44      0.44      0.44        34\n",
      "           3       0.59      0.56      0.58        39\n",
      "           4       0.65      0.62      0.63        42\n",
      "           5       0.70      0.72      0.71        32\n",
      "           6       0.49      0.78      0.60        32\n",
      "           7       0.72      0.62      0.67        45\n",
      "\n",
      "    accuracy                           0.62       288\n",
      "   macro avg       0.64      0.62      0.62       288\n",
      "weighted avg       0.64      0.62      0.63       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  4  0  3  1  0  2  0]\n",
      " [ 2 31  2  4  1  1  2  1]\n",
      " [ 1  1 15  3  3  3  2  6]\n",
      " [ 1  3  1 22  2  2  5  3]\n",
      " [ 0  0  4  0 26  1 10  1]\n",
      " [ 0  0  3  2  3 23  1  0]\n",
      " [ 0  1  2  1  2  1 25  0]\n",
      " [ 0  0  7  2  2  2  4 28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:/another_model/Emotion_Voice_model_relu.h5 \n"
     ]
    }
   ],
   "source": [
    "#Save the model---/\n",
    "model_name = 'Emotion_Voice_model_relu.h5'\n",
    "save_dir = 'C:/another_model/'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('C:/another_model/Emotion_Voice_model_relu.h5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 0s 540us/step\n",
      "Restored model, accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "oss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
